{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech commands dataset version 2 already exists. Skipping download.\n",
      "Converting test set WAVs to numpy files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training set WAVs to numpy files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing Google Speech commands dataset version 2\n",
      "SpeechDownloader.PrepareGoogleSpeechCmd(), dt= 0.950458288192749\n",
      "gscInfo.keys()= dict_keys(['train', 'test', 'val', 'testREAL']), nCategs= 36\n",
      "load google_spcmd_test.gz .... \n",
      "xL.shape= (11005, 16000)\n",
      "yL.shape= (11005,)\n",
      "cL= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35]\n",
      "yDist=[408, 419, 405, 425, 406, 412, 396, 396, 402, 411, 402, 418, 399, 424, 405, 400, 445, 394, 406, 408, 165, 207, 185, 194, 220, 172, 155, 203, 191, 161, 195, 212, 193, 165, 206]\n",
      "xL.shape= (4890, 16000)\n",
      "yL.shape= (4890,)\n",
      "cL= [ 0  2  3  4  5  6  7  8  9 10 11]\n",
      "yDist=[816, 419, 405, 425, 406, 412, 396, 396, 402, 411, 402]\n",
      "fn= google_spcmd_test.gz, dt(sec)= 7.811\n",
      "load google_spcmd_train.gz .... \n",
      "xL.shape= (84800, 16000)\n",
      "yL.shape= (84800,)\n",
      "cL= [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35]\n",
      "yDist=[6, 3168, 3228, 3129, 2944, 3132, 3036, 3016, 3083, 2969, 3109, 3104, 3248, 3140, 3108, 2964, 2954, 3237, 3088, 3200, 3029, 1346, 1594, 1697, 1657, 1711, 1275, 1254, 1632, 1725, 1286, 1710, 1603, 1407, 1287, 1724]\n",
      "xL.shape= (9920, 16000)\n",
      "yL.shape= (9920,)\n",
      "cL= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35]\n",
      "yDist=[353, 394, 404, 350, 375, 347, 362, 359, 369, 349, 370, 384, 350, 343, 353, 367, 364, 372, 387, 345, 150, 212, 182, 180, 197, 131, 145, 217, 194, 127, 194, 203, 159, 139, 193]\n",
      "fn= google_spcmd_train.gz, dt(sec)= 50.204\n",
      "XL.shape=(4890, 125, 128)\n",
      "tf.signal.stft, 執行時間 dt= 0.7420110702514648\n",
      "XL.shape=(11005, 125, 128)\n",
      "tf.signal.stft, 執行時間 dt= 1.7152135372161865\n",
      "XL.shape=(9920, 125, 128)\n",
      "tf.signal.stft, 執行時間 dt= 1.5049757957458496\n",
      "XL.shape=(84800, 125, 128)\n",
      "tf.signal.stft, 執行時間 dt= 28.793631553649902\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 125, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 125, 128, 8)       2056      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 16)        8208      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                9252      \n",
      "=================================================================\n",
      "Total params: 281,916\n",
      "Trainable params: 281,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 84800 samples, validate on 9920 samples\n",
      "Epoch 1/100\n",
      "84000/84800 [============================>.] - ETA: 4:53 - loss: 3.7632 - accuracy: 0.02 - ETA: 2:30 - loss: 3.6710 - accuracy: 0.03 - ETA: 1:43 - loss: 3.6313 - accuracy: 0.03 - ETA: 1:19 - loss: 3.6005 - accuracy: 0.03 - ETA: 1:04 - loss: 3.5812 - accuracy: 0.04 - ETA: 55s - loss: 3.5642 - accuracy: 0.0488 - ETA: 48s - loss: 3.5498 - accuracy: 0.050 - ETA: 43s - loss: 3.5336 - accuracy: 0.053 - ETA: 39s - loss: 3.5268 - accuracy: 0.056 - ETA: 35s - loss: 3.5127 - accuracy: 0.059 - ETA: 33s - loss: 3.4962 - accuracy: 0.066 - ETA: 30s - loss: 3.4778 - accuracy: 0.071 - ETA: 28s - loss: 3.4643 - accuracy: 0.074 - ETA: 27s - loss: 3.4408 - accuracy: 0.079 - ETA: 25s - loss: 3.4180 - accuracy: 0.083 - ETA: 24s - loss: 3.3986 - accuracy: 0.089 - ETA: 23s - loss: 3.3845 - accuracy: 0.092 - ETA: 22s - loss: 3.3772 - accuracy: 0.096 - ETA: 21s - loss: 3.3666 - accuracy: 0.098 - ETA: 20s - loss: 3.3464 - accuracy: 0.104 - ETA: 19s - loss: 3.3249 - accuracy: 0.108 - ETA: 18s - loss: 3.3061 - accuracy: 0.112 - ETA: 17s - loss: 3.2881 - accuracy: 0.116 - ETA: 17s - loss: 3.2704 - accuracy: 0.121 - ETA: 16s - loss: 3.2492 - accuracy: 0.125 - ETA: 16s - loss: 3.2294 - accuracy: 0.129 - ETA: 15s - loss: 3.2134 - accuracy: 0.132 - ETA: 14s - loss: 3.1934 - accuracy: 0.137 - ETA: 14s - loss: 3.1764 - accuracy: 0.141 - ETA: 13s - loss: 3.1623 - accuracy: 0.144 - ETA: 13s - loss: 3.1528 - accuracy: 0.147 - ETA: 13s - loss: 3.1379 - accuracy: 0.150 - ETA: 12s - loss: 3.1212 - accuracy: 0.153 - ETA: 12s - loss: 3.1014 - accuracy: 0.157 - ETA: 11s - loss: 3.0854 - accuracy: 0.161 - ETA: 11s - loss: 3.0718 - accuracy: 0.164 - ETA: 11s - loss: 3.0578 - accuracy: 0.168 - ETA: 10s - loss: 3.0458 - accuracy: 0.170 - ETA: 10s - loss: 3.0357 - accuracy: 0.173 - ETA: 10s - loss: 3.0239 - accuracy: 0.175 - ETA: 9s - loss: 3.0095 - accuracy: 0.179 - ETA: 9s - loss: 2.9965 - accuracy: 0.18 - ETA: 9s - loss: 2.9827 - accuracy: 0.18 - ETA: 8s - loss: 2.9692 - accuracy: 0.18 - ETA: 8s - loss: 2.9554 - accuracy: 0.19 - ETA: 8s - loss: 2.9446 - accuracy: 0.19 - ETA: 8s - loss: 2.9320 - accuracy: 0.19 - ETA: 7s - loss: 2.9213 - accuracy: 0.19 - ETA: 7s - loss: 2.9098 - accuracy: 0.20 - ETA: 7s - loss: 2.9009 - accuracy: 0.20 - ETA: 7s - loss: 2.8914 - accuracy: 0.20 - ETA: 6s - loss: 2.8798 - accuracy: 0.20 - ETA: 6s - loss: 2.8684 - accuracy: 0.21 - ETA: 6s - loss: 2.8566 - accuracy: 0.21 - ETA: 6s - loss: 2.8466 - accuracy: 0.21 - ETA: 5s - loss: 2.8360 - accuracy: 0.21 - ETA: 5s - loss: 2.8254 - accuracy: 0.22 - ETA: 5s - loss: 2.8154 - accuracy: 0.22 - ETA: 5s - loss: 2.8072 - accuracy: 0.22 - ETA: 4s - loss: 2.7959 - accuracy: 0.22 - ETA: 4s - loss: 2.7858 - accuracy: 0.23 - ETA: 4s - loss: 2.7744 - accuracy: 0.23 - ETA: 4s - loss: 2.7647 - accuracy: 0.23 - ETA: 4s - loss: 2.7548 - accuracy: 0.23 - ETA: 3s - loss: 2.7448 - accuracy: 0.24 - ETA: 3s - loss: 2.7356 - accuracy: 0.24 - ETA: 3s - loss: 2.7263 - accuracy: 0.24 - ETA: 3s - loss: 2.7174 - accuracy: 0.24 - ETA: 3s - loss: 2.7086 - accuracy: 0.24 - ETA: 2s - loss: 2.7002 - accuracy: 0.25 - ETA: 2s - loss: 2.6910 - accuracy: 0.25 - ETA: 2s - loss: 2.6835 - accuracy: 0.25 - ETA: 2s - loss: 2.6752 - accuracy: 0.25 - ETA: 2s - loss: 2.6655 - accuracy: 0.25 - ETA: 1s - loss: 2.6575 - accuracy: 0.26 - ETA: 1s - loss: 2.6502 - accuracy: 0.26 - ETA: 1s - loss: 2.6415 - accuracy: 0.26 - ETA: 1s - loss: 2.6328 - accuracy: 0.26 - ETA: 1s - loss: 2.6249 - accuracy: 0.26 - ETA: 0s - loss: 2.6168 - accuracy: 0.27 - ETA: 0s - loss: 2.6104 - accuracy: 0.27 - ETA: 0s - loss: 2.6036 - accuracy: 0.27 - ETA: 0s - loss: 2.5964 - accuracy: 0.27 - ETA: 0s - loss: 2.5888 - accuracy: 0.2779\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.53175, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 17s 204us/sample - loss: 2.5820 - accuracy: 0.2797 - val_loss: 1.6586 - val_accuracy: 0.5318\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 1.7838 - accuracy: 0.485 - ETA: 11s - loss: 1.8529 - accuracy: 0.469 - ETA: 11s - loss: 1.8471 - accuracy: 0.473 - ETA: 11s - loss: 1.8409 - accuracy: 0.474 - ETA: 11s - loss: 1.8586 - accuracy: 0.466 - ETA: 11s - loss: 1.8645 - accuracy: 0.464 - ETA: 11s - loss: 1.8885 - accuracy: 0.458 - ETA: 11s - loss: 1.8842 - accuracy: 0.462 - ETA: 10s - loss: 1.8822 - accuracy: 0.461 - ETA: 10s - loss: 1.8761 - accuracy: 0.462 - ETA: 10s - loss: 1.8647 - accuracy: 0.464 - ETA: 10s - loss: 1.8548 - accuracy: 0.467 - ETA: 10s - loss: 1.8524 - accuracy: 0.466 - ETA: 10s - loss: 1.8582 - accuracy: 0.465 - ETA: 10s - loss: 1.8606 - accuracy: 0.463 - ETA: 9s - loss: 1.8551 - accuracy: 0.465 - ETA: 9s - loss: 1.8452 - accuracy: 0.46 - ETA: 9s - loss: 1.8447 - accuracy: 0.46 - ETA: 9s - loss: 1.8393 - accuracy: 0.46 - ETA: 9s - loss: 1.8340 - accuracy: 0.47 - ETA: 9s - loss: 1.8334 - accuracy: 0.47 - ETA: 9s - loss: 1.8344 - accuracy: 0.47 - ETA: 8s - loss: 1.8325 - accuracy: 0.47 - ETA: 8s - loss: 1.8277 - accuracy: 0.47 - ETA: 8s - loss: 1.8213 - accuracy: 0.47 - ETA: 8s - loss: 1.8135 - accuracy: 0.47 - ETA: 8s - loss: 1.8086 - accuracy: 0.47 - ETA: 8s - loss: 1.8026 - accuracy: 0.48 - ETA: 8s - loss: 1.7963 - accuracy: 0.48 - ETA: 7s - loss: 1.7905 - accuracy: 0.48 - ETA: 7s - loss: 1.7840 - accuracy: 0.48 - ETA: 7s - loss: 1.7784 - accuracy: 0.48 - ETA: 7s - loss: 1.7778 - accuracy: 0.48 - ETA: 7s - loss: 1.7779 - accuracy: 0.48 - ETA: 7s - loss: 1.7742 - accuracy: 0.48 - ETA: 7s - loss: 1.7689 - accuracy: 0.48 - ETA: 6s - loss: 1.7674 - accuracy: 0.48 - ETA: 6s - loss: 1.7617 - accuracy: 0.49 - ETA: 6s - loss: 1.7565 - accuracy: 0.49 - ETA: 6s - loss: 1.7506 - accuracy: 0.49 - ETA: 6s - loss: 1.7473 - accuracy: 0.49 - ETA: 6s - loss: 1.7468 - accuracy: 0.49 - ETA: 6s - loss: 1.7452 - accuracy: 0.49 - ETA: 5s - loss: 1.7428 - accuracy: 0.49 - ETA: 5s - loss: 1.7398 - accuracy: 0.49 - ETA: 5s - loss: 1.7390 - accuracy: 0.49 - ETA: 5s - loss: 1.7364 - accuracy: 0.49 - ETA: 5s - loss: 1.7332 - accuracy: 0.49 - ETA: 5s - loss: 1.7292 - accuracy: 0.49 - ETA: 4s - loss: 1.7271 - accuracy: 0.49 - ETA: 4s - loss: 1.7223 - accuracy: 0.50 - ETA: 4s - loss: 1.7197 - accuracy: 0.50 - ETA: 4s - loss: 1.7165 - accuracy: 0.50 - ETA: 4s - loss: 1.7117 - accuracy: 0.50 - ETA: 4s - loss: 1.7091 - accuracy: 0.50 - ETA: 4s - loss: 1.7042 - accuracy: 0.50 - ETA: 3s - loss: 1.7006 - accuracy: 0.50 - ETA: 3s - loss: 1.6999 - accuracy: 0.50 - ETA: 3s - loss: 1.6978 - accuracy: 0.50 - ETA: 3s - loss: 1.6955 - accuracy: 0.50 - ETA: 3s - loss: 1.6929 - accuracy: 0.50 - ETA: 3s - loss: 1.6890 - accuracy: 0.51 - ETA: 3s - loss: 1.6861 - accuracy: 0.51 - ETA: 2s - loss: 1.6820 - accuracy: 0.51 - ETA: 2s - loss: 1.6772 - accuracy: 0.51 - ETA: 2s - loss: 1.6733 - accuracy: 0.51 - ETA: 2s - loss: 1.6694 - accuracy: 0.51 - ETA: 2s - loss: 1.6654 - accuracy: 0.51 - ETA: 2s - loss: 1.6630 - accuracy: 0.51 - ETA: 2s - loss: 1.6616 - accuracy: 0.51 - ETA: 1s - loss: 1.6601 - accuracy: 0.51 - ETA: 1s - loss: 1.6566 - accuracy: 0.51 - ETA: 1s - loss: 1.6536 - accuracy: 0.51 - ETA: 1s - loss: 1.6504 - accuracy: 0.52 - ETA: 1s - loss: 1.6471 - accuracy: 0.52 - ETA: 1s - loss: 1.6445 - accuracy: 0.52 - ETA: 1s - loss: 1.6411 - accuracy: 0.52 - ETA: 0s - loss: 1.6376 - accuracy: 0.52 - ETA: 0s - loss: 1.6340 - accuracy: 0.52 - ETA: 0s - loss: 1.6313 - accuracy: 0.52 - ETA: 0s - loss: 1.6285 - accuracy: 0.52 - ETA: 0s - loss: 1.6263 - accuracy: 0.52 - ETA: 0s - loss: 1.6229 - accuracy: 0.52 - ETA: 0s - loss: 1.6194 - accuracy: 0.5288\n",
      "Epoch 00002: val_accuracy improved from 0.53175 to 0.71492, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 156us/sample - loss: 1.6173 - accuracy: 0.5295 - val_loss: 1.0454 - val_accuracy: 0.7149\n",
      "Epoch 3/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 1.4041 - accuracy: 0.594 - ETA: 11s - loss: 1.3355 - accuracy: 0.612 - ETA: 11s - loss: 1.2869 - accuracy: 0.618 - ETA: 11s - loss: 1.3112 - accuracy: 0.610 - ETA: 11s - loss: 1.3212 - accuracy: 0.607 - ETA: 11s - loss: 1.3174 - accuracy: 0.611 - ETA: 11s - loss: 1.3200 - accuracy: 0.608 - ETA: 10s - loss: 1.3285 - accuracy: 0.607 - ETA: 10s - loss: 1.3296 - accuracy: 0.608 - ETA: 10s - loss: 1.3316 - accuracy: 0.608 - ETA: 10s - loss: 1.3310 - accuracy: 0.609 - ETA: 10s - loss: 1.3285 - accuracy: 0.610 - ETA: 10s - loss: 1.3247 - accuracy: 0.611 - ETA: 10s - loss: 1.3167 - accuracy: 0.613 - ETA: 9s - loss: 1.3145 - accuracy: 0.614 - ETA: 9s - loss: 1.3179 - accuracy: 0.61 - ETA: 9s - loss: 1.3211 - accuracy: 0.61 - ETA: 9s - loss: 1.3148 - accuracy: 0.61 - ETA: 9s - loss: 1.3122 - accuracy: 0.61 - ETA: 9s - loss: 1.3085 - accuracy: 0.61 - ETA: 9s - loss: 1.3067 - accuracy: 0.61 - ETA: 8s - loss: 1.3114 - accuracy: 0.61 - ETA: 8s - loss: 1.3150 - accuracy: 0.61 - ETA: 8s - loss: 1.3124 - accuracy: 0.61 - ETA: 8s - loss: 1.3078 - accuracy: 0.61 - ETA: 8s - loss: 1.3039 - accuracy: 0.61 - ETA: 8s - loss: 1.3023 - accuracy: 0.61 - ETA: 8s - loss: 1.3011 - accuracy: 0.61 - ETA: 7s - loss: 1.2991 - accuracy: 0.61 - ETA: 7s - loss: 1.2958 - accuracy: 0.62 - ETA: 7s - loss: 1.2911 - accuracy: 0.62 - ETA: 7s - loss: 1.2879 - accuracy: 0.62 - ETA: 7s - loss: 1.2858 - accuracy: 0.62 - ETA: 7s - loss: 1.2835 - accuracy: 0.62 - ETA: 7s - loss: 1.2819 - accuracy: 0.62 - ETA: 6s - loss: 1.2841 - accuracy: 0.62 - ETA: 6s - loss: 1.2871 - accuracy: 0.62 - ETA: 6s - loss: 1.2854 - accuracy: 0.62 - ETA: 6s - loss: 1.2803 - accuracy: 0.62 - ETA: 6s - loss: 1.2765 - accuracy: 0.62 - ETA: 6s - loss: 1.2710 - accuracy: 0.62 - ETA: 6s - loss: 1.2674 - accuracy: 0.62 - ETA: 5s - loss: 1.2642 - accuracy: 0.62 - ETA: 5s - loss: 1.2620 - accuracy: 0.62 - ETA: 5s - loss: 1.2577 - accuracy: 0.63 - ETA: 5s - loss: 1.2545 - accuracy: 0.63 - ETA: 5s - loss: 1.2506 - accuracy: 0.63 - ETA: 5s - loss: 1.2494 - accuracy: 0.63 - ETA: 5s - loss: 1.2479 - accuracy: 0.63 - ETA: 4s - loss: 1.2461 - accuracy: 0.63 - ETA: 4s - loss: 1.2438 - accuracy: 0.63 - ETA: 4s - loss: 1.2448 - accuracy: 0.63 - ETA: 4s - loss: 1.2426 - accuracy: 0.63 - ETA: 4s - loss: 1.2429 - accuracy: 0.63 - ETA: 4s - loss: 1.2418 - accuracy: 0.63 - ETA: 4s - loss: 1.2433 - accuracy: 0.63 - ETA: 3s - loss: 1.2439 - accuracy: 0.63 - ETA: 3s - loss: 1.2436 - accuracy: 0.63 - ETA: 3s - loss: 1.2419 - accuracy: 0.63 - ETA: 3s - loss: 1.2392 - accuracy: 0.63 - ETA: 3s - loss: 1.2367 - accuracy: 0.63 - ETA: 3s - loss: 1.2345 - accuracy: 0.63 - ETA: 3s - loss: 1.2318 - accuracy: 0.63 - ETA: 2s - loss: 1.2300 - accuracy: 0.63 - ETA: 2s - loss: 1.2285 - accuracy: 0.63 - ETA: 2s - loss: 1.2270 - accuracy: 0.63 - ETA: 2s - loss: 1.2240 - accuracy: 0.64 - ETA: 2s - loss: 1.2222 - accuracy: 0.64 - ETA: 2s - loss: 1.2191 - accuracy: 0.64 - ETA: 2s - loss: 1.2170 - accuracy: 0.64 - ETA: 1s - loss: 1.2161 - accuracy: 0.64 - ETA: 1s - loss: 1.2170 - accuracy: 0.64 - ETA: 1s - loss: 1.2167 - accuracy: 0.64 - ETA: 1s - loss: 1.2155 - accuracy: 0.64 - ETA: 1s - loss: 1.2147 - accuracy: 0.64 - ETA: 1s - loss: 1.2136 - accuracy: 0.64 - ETA: 1s - loss: 1.2123 - accuracy: 0.64 - ETA: 0s - loss: 1.2103 - accuracy: 0.64 - ETA: 0s - loss: 1.2078 - accuracy: 0.64 - ETA: 0s - loss: 1.2061 - accuracy: 0.64 - ETA: 0s - loss: 1.2051 - accuracy: 0.64 - ETA: 0s - loss: 1.2039 - accuracy: 0.64 - ETA: 0s - loss: 1.2023 - accuracy: 0.64 - ETA: 0s - loss: 1.2012 - accuracy: 0.6474\n",
      "Epoch 00003: val_accuracy improved from 0.71492 to 0.77833, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 152us/sample - loss: 1.1998 - accuracy: 0.6480 - val_loss: 0.7684 - val_accuracy: 0.7783\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.9692 - accuracy: 0.712 - ETA: 11s - loss: 1.0259 - accuracy: 0.699 - ETA: 11s - loss: 1.0631 - accuracy: 0.682 - ETA: 11s - loss: 1.0423 - accuracy: 0.695 - ETA: 11s - loss: 1.0250 - accuracy: 0.696 - ETA: 11s - loss: 1.0229 - accuracy: 0.694 - ETA: 11s - loss: 1.0126 - accuracy: 0.698 - ETA: 10s - loss: 1.0131 - accuracy: 0.698 - ETA: 10s - loss: 1.0212 - accuracy: 0.695 - ETA: 10s - loss: 1.0171 - accuracy: 0.697 - ETA: 10s - loss: 1.0146 - accuracy: 0.699 - ETA: 10s - loss: 1.0134 - accuracy: 0.699 - ETA: 10s - loss: 1.0175 - accuracy: 0.697 - ETA: 10s - loss: 1.0278 - accuracy: 0.694 - ETA: 9s - loss: 1.0302 - accuracy: 0.692 - ETA: 9s - loss: 1.0265 - accuracy: 0.69 - ETA: 9s - loss: 1.0294 - accuracy: 0.69 - ETA: 9s - loss: 1.0296 - accuracy: 0.69 - ETA: 9s - loss: 1.0284 - accuracy: 0.69 - ETA: 9s - loss: 1.0299 - accuracy: 0.69 - ETA: 9s - loss: 1.0269 - accuracy: 0.69 - ETA: 8s - loss: 1.0219 - accuracy: 0.69 - ETA: 8s - loss: 1.0185 - accuracy: 0.69 - ETA: 8s - loss: 1.0156 - accuracy: 0.69 - ETA: 8s - loss: 1.0127 - accuracy: 0.69 - ETA: 8s - loss: 1.0101 - accuracy: 0.69 - ETA: 8s - loss: 1.0085 - accuracy: 0.69 - ETA: 8s - loss: 1.0078 - accuracy: 0.69 - ETA: 7s - loss: 1.0045 - accuracy: 0.69 - ETA: 7s - loss: 1.0075 - accuracy: 0.69 - ETA: 7s - loss: 1.0092 - accuracy: 0.69 - ETA: 7s - loss: 1.0087 - accuracy: 0.69 - ETA: 7s - loss: 1.0095 - accuracy: 0.69 - ETA: 7s - loss: 1.0076 - accuracy: 0.69 - ETA: 7s - loss: 1.0080 - accuracy: 0.69 - ETA: 6s - loss: 1.0096 - accuracy: 0.69 - ETA: 6s - loss: 1.0091 - accuracy: 0.69 - ETA: 6s - loss: 1.0081 - accuracy: 0.69 - ETA: 6s - loss: 1.0045 - accuracy: 0.70 - ETA: 6s - loss: 1.0025 - accuracy: 0.70 - ETA: 6s - loss: 1.0029 - accuracy: 0.70 - ETA: 6s - loss: 1.0019 - accuracy: 0.70 - ETA: 5s - loss: 1.0008 - accuracy: 0.70 - ETA: 5s - loss: 0.9999 - accuracy: 0.70 - ETA: 5s - loss: 0.9995 - accuracy: 0.70 - ETA: 5s - loss: 0.9994 - accuracy: 0.70 - ETA: 5s - loss: 0.9988 - accuracy: 0.70 - ETA: 5s - loss: 0.9971 - accuracy: 0.70 - ETA: 5s - loss: 0.9948 - accuracy: 0.70 - ETA: 4s - loss: 0.9945 - accuracy: 0.70 - ETA: 4s - loss: 0.9931 - accuracy: 0.70 - ETA: 4s - loss: 0.9926 - accuracy: 0.70 - ETA: 4s - loss: 0.9914 - accuracy: 0.70 - ETA: 4s - loss: 0.9937 - accuracy: 0.70 - ETA: 4s - loss: 0.9933 - accuracy: 0.70 - ETA: 4s - loss: 0.9918 - accuracy: 0.70 - ETA: 3s - loss: 0.9909 - accuracy: 0.70 - ETA: 3s - loss: 0.9904 - accuracy: 0.70 - ETA: 3s - loss: 0.9902 - accuracy: 0.70 - ETA: 3s - loss: 0.9885 - accuracy: 0.70 - ETA: 3s - loss: 0.9864 - accuracy: 0.70 - ETA: 3s - loss: 0.9856 - accuracy: 0.70 - ETA: 3s - loss: 0.9846 - accuracy: 0.70 - ETA: 2s - loss: 0.9841 - accuracy: 0.70 - ETA: 2s - loss: 0.9837 - accuracy: 0.70 - ETA: 2s - loss: 0.9829 - accuracy: 0.70 - ETA: 2s - loss: 0.9829 - accuracy: 0.70 - ETA: 2s - loss: 0.9807 - accuracy: 0.70 - ETA: 2s - loss: 0.9795 - accuracy: 0.70 - ETA: 2s - loss: 0.9792 - accuracy: 0.70 - ETA: 1s - loss: 0.9781 - accuracy: 0.70 - ETA: 1s - loss: 0.9762 - accuracy: 0.70 - ETA: 1s - loss: 0.9756 - accuracy: 0.71 - ETA: 1s - loss: 0.9744 - accuracy: 0.71 - ETA: 1s - loss: 0.9739 - accuracy: 0.71 - ETA: 1s - loss: 0.9732 - accuracy: 0.71 - ETA: 1s - loss: 0.9729 - accuracy: 0.71 - ETA: 0s - loss: 0.9725 - accuracy: 0.71 - ETA: 0s - loss: 0.9716 - accuracy: 0.71 - ETA: 0s - loss: 0.9705 - accuracy: 0.71 - ETA: 0s - loss: 0.9692 - accuracy: 0.71 - ETA: 0s - loss: 0.9677 - accuracy: 0.71 - ETA: 0s - loss: 0.9663 - accuracy: 0.71 - ETA: 0s - loss: 0.9661 - accuracy: 0.7131\n",
      "Epoch 00004: val_accuracy improved from 0.77833 to 0.78478, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 152us/sample - loss: 0.9673 - accuracy: 0.7129 - val_loss: 0.7590 - val_accuracy: 0.7848\n",
      "Epoch 5/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 1.0614 - accuracy: 0.692 - ETA: 11s - loss: 1.0415 - accuracy: 0.691 - ETA: 11s - loss: 0.9601 - accuracy: 0.718 - ETA: 11s - loss: 0.9233 - accuracy: 0.725 - ETA: 11s - loss: 0.9041 - accuracy: 0.732 - ETA: 11s - loss: 0.8948 - accuracy: 0.737 - ETA: 11s - loss: 0.8858 - accuracy: 0.738 - ETA: 10s - loss: 0.8864 - accuracy: 0.737 - ETA: 10s - loss: 0.8782 - accuracy: 0.738 - ETA: 10s - loss: 0.8727 - accuracy: 0.739 - ETA: 10s - loss: 0.8726 - accuracy: 0.739 - ETA: 10s - loss: 0.8648 - accuracy: 0.742 - ETA: 10s - loss: 0.8670 - accuracy: 0.741 - ETA: 10s - loss: 0.8677 - accuracy: 0.740 - ETA: 9s - loss: 0.8644 - accuracy: 0.741 - ETA: 9s - loss: 0.8643 - accuracy: 0.74 - ETA: 9s - loss: 0.8658 - accuracy: 0.74 - ETA: 9s - loss: 0.8621 - accuracy: 0.74 - ETA: 9s - loss: 0.8621 - accuracy: 0.74 - ETA: 9s - loss: 0.8632 - accuracy: 0.74 - ETA: 9s - loss: 0.8658 - accuracy: 0.74 - ETA: 8s - loss: 0.8674 - accuracy: 0.74 - ETA: 8s - loss: 0.8713 - accuracy: 0.74 - ETA: 8s - loss: 0.8706 - accuracy: 0.74 - ETA: 8s - loss: 0.8681 - accuracy: 0.74 - ETA: 8s - loss: 0.8645 - accuracy: 0.74 - ETA: 8s - loss: 0.8635 - accuracy: 0.74 - ETA: 8s - loss: 0.8616 - accuracy: 0.74 - ETA: 7s - loss: 0.8591 - accuracy: 0.74 - ETA: 7s - loss: 0.8535 - accuracy: 0.74 - ETA: 7s - loss: 0.8516 - accuracy: 0.74 - ETA: 7s - loss: 0.8528 - accuracy: 0.74 - ETA: 7s - loss: 0.8502 - accuracy: 0.74 - ETA: 7s - loss: 0.8488 - accuracy: 0.74 - ETA: 7s - loss: 0.8494 - accuracy: 0.74 - ETA: 6s - loss: 0.8500 - accuracy: 0.74 - ETA: 6s - loss: 0.8503 - accuracy: 0.74 - ETA: 6s - loss: 0.8518 - accuracy: 0.74 - ETA: 6s - loss: 0.8537 - accuracy: 0.74 - ETA: 6s - loss: 0.8550 - accuracy: 0.74 - ETA: 6s - loss: 0.8555 - accuracy: 0.74 - ETA: 6s - loss: 0.8532 - accuracy: 0.74 - ETA: 5s - loss: 0.8533 - accuracy: 0.74 - ETA: 5s - loss: 0.8525 - accuracy: 0.74 - ETA: 5s - loss: 0.8523 - accuracy: 0.74 - ETA: 5s - loss: 0.8516 - accuracy: 0.74 - ETA: 5s - loss: 0.8513 - accuracy: 0.74 - ETA: 5s - loss: 0.8512 - accuracy: 0.74 - ETA: 5s - loss: 0.8506 - accuracy: 0.74 - ETA: 4s - loss: 0.8497 - accuracy: 0.74 - ETA: 4s - loss: 0.8501 - accuracy: 0.74 - ETA: 4s - loss: 0.8491 - accuracy: 0.74 - ETA: 4s - loss: 0.8475 - accuracy: 0.74 - ETA: 4s - loss: 0.8456 - accuracy: 0.74 - ETA: 4s - loss: 0.8439 - accuracy: 0.74 - ETA: 4s - loss: 0.8449 - accuracy: 0.74 - ETA: 3s - loss: 0.8452 - accuracy: 0.74 - ETA: 3s - loss: 0.8440 - accuracy: 0.74 - ETA: 3s - loss: 0.8430 - accuracy: 0.74 - ETA: 3s - loss: 0.8431 - accuracy: 0.74 - ETA: 3s - loss: 0.8426 - accuracy: 0.74 - ETA: 3s - loss: 0.8436 - accuracy: 0.74 - ETA: 3s - loss: 0.8423 - accuracy: 0.74 - ETA: 2s - loss: 0.8418 - accuracy: 0.74 - ETA: 2s - loss: 0.8413 - accuracy: 0.74 - ETA: 2s - loss: 0.8408 - accuracy: 0.74 - ETA: 2s - loss: 0.8410 - accuracy: 0.74 - ETA: 2s - loss: 0.8404 - accuracy: 0.74 - ETA: 2s - loss: 0.8387 - accuracy: 0.74 - ETA: 2s - loss: 0.8375 - accuracy: 0.75 - ETA: 1s - loss: 0.8357 - accuracy: 0.75 - ETA: 1s - loss: 0.8351 - accuracy: 0.75 - ETA: 1s - loss: 0.8345 - accuracy: 0.75 - ETA: 1s - loss: 0.8331 - accuracy: 0.75 - ETA: 1s - loss: 0.8322 - accuracy: 0.75 - ETA: 1s - loss: 0.8312 - accuracy: 0.75 - ETA: 1s - loss: 0.8299 - accuracy: 0.75 - ETA: 0s - loss: 0.8303 - accuracy: 0.75 - ETA: 0s - loss: 0.8311 - accuracy: 0.75 - ETA: 0s - loss: 0.8315 - accuracy: 0.75 - ETA: 0s - loss: 0.8314 - accuracy: 0.75 - ETA: 0s - loss: 0.8303 - accuracy: 0.75 - ETA: 0s - loss: 0.8300 - accuracy: 0.75 - ETA: 0s - loss: 0.8305 - accuracy: 0.7516\n",
      "Epoch 00005: val_accuracy improved from 0.78478 to 0.84153, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.8295 - accuracy: 0.7520 - val_loss: 0.5542 - val_accuracy: 0.8415\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.7497 - accuracy: 0.792 - ETA: 11s - loss: 0.7844 - accuracy: 0.774 - ETA: 11s - loss: 0.7927 - accuracy: 0.769 - ETA: 11s - loss: 0.7889 - accuracy: 0.767 - ETA: 11s - loss: 0.7836 - accuracy: 0.770 - ETA: 11s - loss: 0.7765 - accuracy: 0.773 - ETA: 11s - loss: 0.7682 - accuracy: 0.776 - ETA: 10s - loss: 0.7630 - accuracy: 0.777 - ETA: 10s - loss: 0.7658 - accuracy: 0.773 - ETA: 10s - loss: 0.7585 - accuracy: 0.776 - ETA: 10s - loss: 0.7557 - accuracy: 0.775 - ETA: 10s - loss: 0.7587 - accuracy: 0.775 - ETA: 10s - loss: 0.7639 - accuracy: 0.773 - ETA: 10s - loss: 0.7621 - accuracy: 0.774 - ETA: 9s - loss: 0.7653 - accuracy: 0.773 - ETA: 9s - loss: 0.7614 - accuracy: 0.77 - ETA: 9s - loss: 0.7623 - accuracy: 0.77 - ETA: 9s - loss: 0.7653 - accuracy: 0.77 - ETA: 9s - loss: 0.7659 - accuracy: 0.77 - ETA: 9s - loss: 0.7632 - accuracy: 0.77 - ETA: 9s - loss: 0.7670 - accuracy: 0.77 - ETA: 8s - loss: 0.7702 - accuracy: 0.77 - ETA: 8s - loss: 0.7693 - accuracy: 0.77 - ETA: 8s - loss: 0.7690 - accuracy: 0.77 - ETA: 8s - loss: 0.7662 - accuracy: 0.77 - ETA: 8s - loss: 0.7611 - accuracy: 0.77 - ETA: 8s - loss: 0.7603 - accuracy: 0.77 - ETA: 8s - loss: 0.7566 - accuracy: 0.77 - ETA: 7s - loss: 0.7528 - accuracy: 0.77 - ETA: 7s - loss: 0.7503 - accuracy: 0.77 - ETA: 7s - loss: 0.7457 - accuracy: 0.77 - ETA: 7s - loss: 0.7444 - accuracy: 0.77 - ETA: 7s - loss: 0.7443 - accuracy: 0.77 - ETA: 7s - loss: 0.7438 - accuracy: 0.77 - ETA: 7s - loss: 0.7432 - accuracy: 0.77 - ETA: 6s - loss: 0.7429 - accuracy: 0.77 - ETA: 6s - loss: 0.7423 - accuracy: 0.77 - ETA: 6s - loss: 0.7431 - accuracy: 0.77 - ETA: 6s - loss: 0.7434 - accuracy: 0.77 - ETA: 6s - loss: 0.7432 - accuracy: 0.77 - ETA: 6s - loss: 0.7443 - accuracy: 0.77 - ETA: 6s - loss: 0.7439 - accuracy: 0.77 - ETA: 5s - loss: 0.7431 - accuracy: 0.77 - ETA: 5s - loss: 0.7422 - accuracy: 0.77 - ETA: 5s - loss: 0.7417 - accuracy: 0.77 - ETA: 5s - loss: 0.7413 - accuracy: 0.77 - ETA: 5s - loss: 0.7393 - accuracy: 0.77 - ETA: 5s - loss: 0.7393 - accuracy: 0.77 - ETA: 5s - loss: 0.7392 - accuracy: 0.77 - ETA: 4s - loss: 0.7382 - accuracy: 0.77 - ETA: 4s - loss: 0.7381 - accuracy: 0.77 - ETA: 4s - loss: 0.7382 - accuracy: 0.77 - ETA: 4s - loss: 0.7383 - accuracy: 0.77 - ETA: 4s - loss: 0.7390 - accuracy: 0.77 - ETA: 4s - loss: 0.7408 - accuracy: 0.77 - ETA: 4s - loss: 0.7410 - accuracy: 0.77 - ETA: 3s - loss: 0.7397 - accuracy: 0.77 - ETA: 3s - loss: 0.7398 - accuracy: 0.77 - ETA: 3s - loss: 0.7396 - accuracy: 0.77 - ETA: 3s - loss: 0.7389 - accuracy: 0.77 - ETA: 3s - loss: 0.7368 - accuracy: 0.78 - ETA: 3s - loss: 0.7367 - accuracy: 0.78 - ETA: 3s - loss: 0.7355 - accuracy: 0.78 - ETA: 2s - loss: 0.7350 - accuracy: 0.78 - ETA: 2s - loss: 0.7348 - accuracy: 0.78 - ETA: 2s - loss: 0.7342 - accuracy: 0.78 - ETA: 2s - loss: 0.7348 - accuracy: 0.78 - ETA: 2s - loss: 0.7347 - accuracy: 0.78 - ETA: 2s - loss: 0.7335 - accuracy: 0.78 - ETA: 2s - loss: 0.7342 - accuracy: 0.78 - ETA: 1s - loss: 0.7327 - accuracy: 0.78 - ETA: 1s - loss: 0.7321 - accuracy: 0.78 - ETA: 1s - loss: 0.7310 - accuracy: 0.78 - ETA: 1s - loss: 0.7306 - accuracy: 0.78 - ETA: 1s - loss: 0.7300 - accuracy: 0.78 - ETA: 1s - loss: 0.7296 - accuracy: 0.78 - ETA: 1s - loss: 0.7289 - accuracy: 0.78 - ETA: 0s - loss: 0.7294 - accuracy: 0.78 - ETA: 0s - loss: 0.7293 - accuracy: 0.78 - ETA: 0s - loss: 0.7291 - accuracy: 0.78 - ETA: 0s - loss: 0.7294 - accuracy: 0.78 - ETA: 0s - loss: 0.7285 - accuracy: 0.78 - ETA: 0s - loss: 0.7281 - accuracy: 0.78 - ETA: 0s - loss: 0.7278 - accuracy: 0.7814\n",
      "Epoch 00006: val_accuracy improved from 0.84153 to 0.85585, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.7273 - accuracy: 0.7815 - val_loss: 0.5190 - val_accuracy: 0.8558\n",
      "Epoch 7/100\n",
      "84000/84800 [============================>.] - ETA: 12s - loss: 0.6782 - accuracy: 0.798 - ETA: 12s - loss: 0.6894 - accuracy: 0.794 - ETA: 11s - loss: 0.6927 - accuracy: 0.791 - ETA: 11s - loss: 0.6931 - accuracy: 0.784 - ETA: 11s - loss: 0.6969 - accuracy: 0.785 - ETA: 11s - loss: 0.6841 - accuracy: 0.790 - ETA: 11s - loss: 0.6890 - accuracy: 0.790 - ETA: 11s - loss: 0.6847 - accuracy: 0.792 - ETA: 11s - loss: 0.6806 - accuracy: 0.793 - ETA: 11s - loss: 0.6791 - accuracy: 0.793 - ETA: 10s - loss: 0.6767 - accuracy: 0.793 - ETA: 10s - loss: 0.6768 - accuracy: 0.794 - ETA: 10s - loss: 0.6762 - accuracy: 0.793 - ETA: 10s - loss: 0.6694 - accuracy: 0.796 - ETA: 10s - loss: 0.6788 - accuracy: 0.794 - ETA: 10s - loss: 0.6818 - accuracy: 0.794 - ETA: 9s - loss: 0.6809 - accuracy: 0.794 - ETA: 9s - loss: 0.6791 - accuracy: 0.79 - ETA: 9s - loss: 0.6794 - accuracy: 0.79 - ETA: 9s - loss: 0.6801 - accuracy: 0.79 - ETA: 9s - loss: 0.6792 - accuracy: 0.79 - ETA: 9s - loss: 0.6823 - accuracy: 0.79 - ETA: 9s - loss: 0.6814 - accuracy: 0.79 - ETA: 8s - loss: 0.6848 - accuracy: 0.79 - ETA: 8s - loss: 0.6862 - accuracy: 0.79 - ETA: 8s - loss: 0.6868 - accuracy: 0.79 - ETA: 8s - loss: 0.6868 - accuracy: 0.79 - ETA: 8s - loss: 0.6848 - accuracy: 0.79 - ETA: 8s - loss: 0.6814 - accuracy: 0.79 - ETA: 7s - loss: 0.6795 - accuracy: 0.79 - ETA: 7s - loss: 0.6789 - accuracy: 0.79 - ETA: 7s - loss: 0.6779 - accuracy: 0.79 - ETA: 7s - loss: 0.6797 - accuracy: 0.79 - ETA: 7s - loss: 0.6805 - accuracy: 0.79 - ETA: 7s - loss: 0.6827 - accuracy: 0.79 - ETA: 7s - loss: 0.6818 - accuracy: 0.79 - ETA: 6s - loss: 0.6807 - accuracy: 0.79 - ETA: 6s - loss: 0.6805 - accuracy: 0.79 - ETA: 6s - loss: 0.6793 - accuracy: 0.79 - ETA: 6s - loss: 0.6777 - accuracy: 0.79 - ETA: 6s - loss: 0.6766 - accuracy: 0.79 - ETA: 6s - loss: 0.6776 - accuracy: 0.79 - ETA: 6s - loss: 0.6768 - accuracy: 0.79 - ETA: 5s - loss: 0.6751 - accuracy: 0.79 - ETA: 5s - loss: 0.6758 - accuracy: 0.79 - ETA: 5s - loss: 0.6751 - accuracy: 0.79 - ETA: 5s - loss: 0.6747 - accuracy: 0.79 - ETA: 5s - loss: 0.6739 - accuracy: 0.79 - ETA: 5s - loss: 0.6732 - accuracy: 0.79 - ETA: 5s - loss: 0.6728 - accuracy: 0.79 - ETA: 4s - loss: 0.6717 - accuracy: 0.79 - ETA: 4s - loss: 0.6693 - accuracy: 0.79 - ETA: 4s - loss: 0.6690 - accuracy: 0.79 - ETA: 4s - loss: 0.6698 - accuracy: 0.79 - ETA: 4s - loss: 0.6691 - accuracy: 0.79 - ETA: 4s - loss: 0.6696 - accuracy: 0.79 - ETA: 3s - loss: 0.6699 - accuracy: 0.79 - ETA: 3s - loss: 0.6704 - accuracy: 0.79 - ETA: 3s - loss: 0.6693 - accuracy: 0.79 - ETA: 3s - loss: 0.6700 - accuracy: 0.79 - ETA: 3s - loss: 0.6698 - accuracy: 0.79 - ETA: 3s - loss: 0.6692 - accuracy: 0.79 - ETA: 3s - loss: 0.6699 - accuracy: 0.79 - ETA: 2s - loss: 0.6708 - accuracy: 0.79 - ETA: 2s - loss: 0.6725 - accuracy: 0.79 - ETA: 2s - loss: 0.6726 - accuracy: 0.79 - ETA: 2s - loss: 0.6720 - accuracy: 0.79 - ETA: 2s - loss: 0.6703 - accuracy: 0.79 - ETA: 2s - loss: 0.6698 - accuracy: 0.79 - ETA: 2s - loss: 0.6700 - accuracy: 0.79 - ETA: 1s - loss: 0.6694 - accuracy: 0.79 - ETA: 1s - loss: 0.6680 - accuracy: 0.79 - ETA: 1s - loss: 0.6672 - accuracy: 0.79 - ETA: 1s - loss: 0.6660 - accuracy: 0.79 - ETA: 1s - loss: 0.6657 - accuracy: 0.79 - ETA: 1s - loss: 0.6664 - accuracy: 0.79 - ETA: 1s - loss: 0.6675 - accuracy: 0.79 - ETA: 0s - loss: 0.6673 - accuracy: 0.79 - ETA: 0s - loss: 0.6677 - accuracy: 0.79 - ETA: 0s - loss: 0.6688 - accuracy: 0.79 - ETA: 0s - loss: 0.6684 - accuracy: 0.79 - ETA: 0s - loss: 0.6681 - accuracy: 0.79 - ETA: 0s - loss: 0.6675 - accuracy: 0.79 - ETA: 0s - loss: 0.6673 - accuracy: 0.7987\n",
      "Epoch 00007: val_accuracy did not improve from 0.85585\n",
      "84800/84800 [==============================] - 13s 152us/sample - loss: 0.6675 - accuracy: 0.7988 - val_loss: 0.5047 - val_accuracy: 0.8532\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.5994 - accuracy: 0.825 - ETA: 11s - loss: 0.6253 - accuracy: 0.814 - ETA: 11s - loss: 0.6345 - accuracy: 0.808 - ETA: 11s - loss: 0.6376 - accuracy: 0.807 - ETA: 11s - loss: 0.6340 - accuracy: 0.808 - ETA: 11s - loss: 0.6323 - accuracy: 0.808 - ETA: 11s - loss: 0.6241 - accuracy: 0.809 - ETA: 10s - loss: 0.6192 - accuracy: 0.811 - ETA: 10s - loss: 0.6223 - accuracy: 0.809 - ETA: 10s - loss: 0.6197 - accuracy: 0.810 - ETA: 10s - loss: 0.6159 - accuracy: 0.812 - ETA: 10s - loss: 0.6124 - accuracy: 0.813 - ETA: 10s - loss: 0.6120 - accuracy: 0.813 - ETA: 10s - loss: 0.6129 - accuracy: 0.813 - ETA: 9s - loss: 0.6121 - accuracy: 0.813 - ETA: 9s - loss: 0.6124 - accuracy: 0.81 - ETA: 9s - loss: 0.6090 - accuracy: 0.81 - ETA: 9s - loss: 0.6099 - accuracy: 0.81 - ETA: 9s - loss: 0.6080 - accuracy: 0.81 - ETA: 9s - loss: 0.6051 - accuracy: 0.81 - ETA: 9s - loss: 0.6077 - accuracy: 0.81 - ETA: 8s - loss: 0.6126 - accuracy: 0.81 - ETA: 8s - loss: 0.6166 - accuracy: 0.81 - ETA: 8s - loss: 0.6179 - accuracy: 0.81 - ETA: 8s - loss: 0.6210 - accuracy: 0.81 - ETA: 8s - loss: 0.6188 - accuracy: 0.81 - ETA: 8s - loss: 0.6167 - accuracy: 0.81 - ETA: 8s - loss: 0.6154 - accuracy: 0.81 - ETA: 7s - loss: 0.6138 - accuracy: 0.81 - ETA: 7s - loss: 0.6137 - accuracy: 0.81 - ETA: 7s - loss: 0.6180 - accuracy: 0.81 - ETA: 7s - loss: 0.6222 - accuracy: 0.81 - ETA: 7s - loss: 0.6267 - accuracy: 0.80 - ETA: 7s - loss: 0.6282 - accuracy: 0.80 - ETA: 7s - loss: 0.6280 - accuracy: 0.80 - ETA: 6s - loss: 0.6278 - accuracy: 0.80 - ETA: 6s - loss: 0.6253 - accuracy: 0.81 - ETA: 6s - loss: 0.6255 - accuracy: 0.81 - ETA: 6s - loss: 0.6242 - accuracy: 0.81 - ETA: 6s - loss: 0.6230 - accuracy: 0.81 - ETA: 6s - loss: 0.6223 - accuracy: 0.81 - ETA: 6s - loss: 0.6235 - accuracy: 0.81 - ETA: 5s - loss: 0.6259 - accuracy: 0.81 - ETA: 5s - loss: 0.6262 - accuracy: 0.81 - ETA: 5s - loss: 0.6264 - accuracy: 0.81 - ETA: 5s - loss: 0.6259 - accuracy: 0.81 - ETA: 5s - loss: 0.6253 - accuracy: 0.81 - ETA: 5s - loss: 0.6235 - accuracy: 0.81 - ETA: 5s - loss: 0.6250 - accuracy: 0.81 - ETA: 4s - loss: 0.6245 - accuracy: 0.81 - ETA: 4s - loss: 0.6236 - accuracy: 0.81 - ETA: 4s - loss: 0.6232 - accuracy: 0.81 - ETA: 4s - loss: 0.6220 - accuracy: 0.81 - ETA: 4s - loss: 0.6209 - accuracy: 0.81 - ETA: 4s - loss: 0.6205 - accuracy: 0.81 - ETA: 4s - loss: 0.6200 - accuracy: 0.81 - ETA: 3s - loss: 0.6194 - accuracy: 0.81 - ETA: 3s - loss: 0.6184 - accuracy: 0.81 - ETA: 3s - loss: 0.6173 - accuracy: 0.81 - ETA: 3s - loss: 0.6178 - accuracy: 0.81 - ETA: 3s - loss: 0.6193 - accuracy: 0.81 - ETA: 3s - loss: 0.6205 - accuracy: 0.81 - ETA: 3s - loss: 0.6201 - accuracy: 0.81 - ETA: 2s - loss: 0.6194 - accuracy: 0.81 - ETA: 2s - loss: 0.6183 - accuracy: 0.81 - ETA: 2s - loss: 0.6180 - accuracy: 0.81 - ETA: 2s - loss: 0.6166 - accuracy: 0.81 - ETA: 2s - loss: 0.6156 - accuracy: 0.81 - ETA: 2s - loss: 0.6156 - accuracy: 0.81 - ETA: 2s - loss: 0.6151 - accuracy: 0.81 - ETA: 1s - loss: 0.6151 - accuracy: 0.81 - ETA: 1s - loss: 0.6149 - accuracy: 0.81 - ETA: 1s - loss: 0.6147 - accuracy: 0.81 - ETA: 1s - loss: 0.6153 - accuracy: 0.81 - ETA: 1s - loss: 0.6152 - accuracy: 0.81 - ETA: 1s - loss: 0.6145 - accuracy: 0.81 - ETA: 1s - loss: 0.6145 - accuracy: 0.81 - ETA: 0s - loss: 0.6144 - accuracy: 0.81 - ETA: 0s - loss: 0.6146 - accuracy: 0.81 - ETA: 0s - loss: 0.6153 - accuracy: 0.81 - ETA: 0s - loss: 0.6152 - accuracy: 0.81 - ETA: 0s - loss: 0.6151 - accuracy: 0.81 - ETA: 0s - loss: 0.6146 - accuracy: 0.81 - ETA: 0s - loss: 0.6146 - accuracy: 0.8133\n",
      "Epoch 00008: val_accuracy did not improve from 0.85585\n",
      "84800/84800 [==============================] - 13s 151us/sample - loss: 0.6151 - accuracy: 0.8131 - val_loss: 0.4928 - val_accuracy: 0.8544\n",
      "Epoch 9/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.6350 - accuracy: 0.802 - ETA: 11s - loss: 0.6286 - accuracy: 0.812 - ETA: 11s - loss: 0.5859 - accuracy: 0.822 - ETA: 11s - loss: 0.5870 - accuracy: 0.824 - ETA: 11s - loss: 0.5940 - accuracy: 0.820 - ETA: 11s - loss: 0.5945 - accuracy: 0.820 - ETA: 11s - loss: 0.5809 - accuracy: 0.824 - ETA: 10s - loss: 0.5768 - accuracy: 0.825 - ETA: 10s - loss: 0.5692 - accuracy: 0.829 - ETA: 10s - loss: 0.5682 - accuracy: 0.830 - ETA: 10s - loss: 0.5686 - accuracy: 0.830 - ETA: 10s - loss: 0.5698 - accuracy: 0.829 - ETA: 10s - loss: 0.5690 - accuracy: 0.829 - ETA: 10s - loss: 0.5685 - accuracy: 0.830 - ETA: 9s - loss: 0.5681 - accuracy: 0.830 - ETA: 9s - loss: 0.5702 - accuracy: 0.83 - ETA: 9s - loss: 0.5719 - accuracy: 0.82 - ETA: 9s - loss: 0.5707 - accuracy: 0.82 - ETA: 9s - loss: 0.5712 - accuracy: 0.82 - ETA: 9s - loss: 0.5763 - accuracy: 0.82 - ETA: 9s - loss: 0.5774 - accuracy: 0.82 - ETA: 8s - loss: 0.5758 - accuracy: 0.82 - ETA: 8s - loss: 0.5770 - accuracy: 0.82 - ETA: 8s - loss: 0.5766 - accuracy: 0.82 - ETA: 8s - loss: 0.5739 - accuracy: 0.82 - ETA: 8s - loss: 0.5743 - accuracy: 0.82 - ETA: 8s - loss: 0.5733 - accuracy: 0.82 - ETA: 8s - loss: 0.5736 - accuracy: 0.82 - ETA: 7s - loss: 0.5739 - accuracy: 0.82 - ETA: 7s - loss: 0.5727 - accuracy: 0.82 - ETA: 7s - loss: 0.5742 - accuracy: 0.82 - ETA: 7s - loss: 0.5741 - accuracy: 0.82 - ETA: 7s - loss: 0.5774 - accuracy: 0.82 - ETA: 7s - loss: 0.5789 - accuracy: 0.82 - ETA: 7s - loss: 0.5770 - accuracy: 0.82 - ETA: 6s - loss: 0.5749 - accuracy: 0.82 - ETA: 6s - loss: 0.5734 - accuracy: 0.82 - ETA: 6s - loss: 0.5731 - accuracy: 0.82 - ETA: 6s - loss: 0.5730 - accuracy: 0.82 - ETA: 6s - loss: 0.5728 - accuracy: 0.82 - ETA: 6s - loss: 0.5721 - accuracy: 0.82 - ETA: 6s - loss: 0.5735 - accuracy: 0.82 - ETA: 5s - loss: 0.5746 - accuracy: 0.82 - ETA: 5s - loss: 0.5754 - accuracy: 0.82 - ETA: 5s - loss: 0.5748 - accuracy: 0.82 - ETA: 5s - loss: 0.5745 - accuracy: 0.82 - ETA: 5s - loss: 0.5759 - accuracy: 0.82 - ETA: 5s - loss: 0.5752 - accuracy: 0.82 - ETA: 5s - loss: 0.5748 - accuracy: 0.82 - ETA: 4s - loss: 0.5742 - accuracy: 0.82 - ETA: 4s - loss: 0.5750 - accuracy: 0.82 - ETA: 4s - loss: 0.5752 - accuracy: 0.82 - ETA: 4s - loss: 0.5766 - accuracy: 0.82 - ETA: 4s - loss: 0.5769 - accuracy: 0.82 - ETA: 4s - loss: 0.5767 - accuracy: 0.82 - ETA: 4s - loss: 0.5772 - accuracy: 0.82 - ETA: 3s - loss: 0.5782 - accuracy: 0.82 - ETA: 3s - loss: 0.5768 - accuracy: 0.82 - ETA: 3s - loss: 0.5765 - accuracy: 0.82 - ETA: 3s - loss: 0.5772 - accuracy: 0.82 - ETA: 3s - loss: 0.5769 - accuracy: 0.82 - ETA: 3s - loss: 0.5761 - accuracy: 0.82 - ETA: 3s - loss: 0.5760 - accuracy: 0.82 - ETA: 2s - loss: 0.5777 - accuracy: 0.82 - ETA: 2s - loss: 0.5783 - accuracy: 0.82 - ETA: 2s - loss: 0.5784 - accuracy: 0.82 - ETA: 2s - loss: 0.5782 - accuracy: 0.82 - ETA: 2s - loss: 0.5778 - accuracy: 0.82 - ETA: 2s - loss: 0.5771 - accuracy: 0.82 - ETA: 2s - loss: 0.5770 - accuracy: 0.82 - ETA: 1s - loss: 0.5760 - accuracy: 0.82 - ETA: 1s - loss: 0.5758 - accuracy: 0.82 - ETA: 1s - loss: 0.5753 - accuracy: 0.82 - ETA: 1s - loss: 0.5755 - accuracy: 0.82 - ETA: 1s - loss: 0.5750 - accuracy: 0.82 - ETA: 1s - loss: 0.5750 - accuracy: 0.82 - ETA: 1s - loss: 0.5737 - accuracy: 0.82 - ETA: 0s - loss: 0.5731 - accuracy: 0.82 - ETA: 0s - loss: 0.5729 - accuracy: 0.82 - ETA: 0s - loss: 0.5727 - accuracy: 0.82 - ETA: 0s - loss: 0.5727 - accuracy: 0.82 - ETA: 0s - loss: 0.5724 - accuracy: 0.82 - ETA: 0s - loss: 0.5721 - accuracy: 0.82 - ETA: 0s - loss: 0.5718 - accuracy: 0.8273\n",
      "Epoch 00009: val_accuracy improved from 0.85585 to 0.87621, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.5713 - accuracy: 0.8274 - val_loss: 0.4137 - val_accuracy: 0.8762\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.5070 - accuracy: 0.854 - ETA: 11s - loss: 0.5056 - accuracy: 0.848 - ETA: 11s - loss: 0.5186 - accuracy: 0.843 - ETA: 11s - loss: 0.5508 - accuracy: 0.833 - ETA: 11s - loss: 0.5535 - accuracy: 0.831 - ETA: 11s - loss: 0.5499 - accuracy: 0.831 - ETA: 11s - loss: 0.5472 - accuracy: 0.831 - ETA: 10s - loss: 0.5465 - accuracy: 0.832 - ETA: 10s - loss: 0.5542 - accuracy: 0.829 - ETA: 10s - loss: 0.5524 - accuracy: 0.829 - ETA: 10s - loss: 0.5443 - accuracy: 0.831 - ETA: 10s - loss: 0.5390 - accuracy: 0.833 - ETA: 10s - loss: 0.5347 - accuracy: 0.834 - ETA: 10s - loss: 0.5359 - accuracy: 0.834 - ETA: 9s - loss: 0.5365 - accuracy: 0.834 - ETA: 9s - loss: 0.5355 - accuracy: 0.83 - ETA: 9s - loss: 0.5348 - accuracy: 0.83 - ETA: 9s - loss: 0.5318 - accuracy: 0.83 - ETA: 9s - loss: 0.5323 - accuracy: 0.83 - ETA: 9s - loss: 0.5321 - accuracy: 0.83 - ETA: 9s - loss: 0.5306 - accuracy: 0.83 - ETA: 8s - loss: 0.5308 - accuracy: 0.83 - ETA: 8s - loss: 0.5315 - accuracy: 0.83 - ETA: 8s - loss: 0.5298 - accuracy: 0.83 - ETA: 8s - loss: 0.5320 - accuracy: 0.83 - ETA: 8s - loss: 0.5345 - accuracy: 0.83 - ETA: 8s - loss: 0.5361 - accuracy: 0.83 - ETA: 8s - loss: 0.5353 - accuracy: 0.83 - ETA: 7s - loss: 0.5353 - accuracy: 0.83 - ETA: 7s - loss: 0.5343 - accuracy: 0.83 - ETA: 7s - loss: 0.5330 - accuracy: 0.83 - ETA: 7s - loss: 0.5339 - accuracy: 0.83 - ETA: 7s - loss: 0.5352 - accuracy: 0.83 - ETA: 7s - loss: 0.5347 - accuracy: 0.83 - ETA: 7s - loss: 0.5350 - accuracy: 0.83 - ETA: 6s - loss: 0.5346 - accuracy: 0.83 - ETA: 6s - loss: 0.5350 - accuracy: 0.83 - ETA: 6s - loss: 0.5360 - accuracy: 0.83 - ETA: 6s - loss: 0.5382 - accuracy: 0.83 - ETA: 6s - loss: 0.5389 - accuracy: 0.83 - ETA: 6s - loss: 0.5393 - accuracy: 0.83 - ETA: 6s - loss: 0.5394 - accuracy: 0.83 - ETA: 5s - loss: 0.5390 - accuracy: 0.83 - ETA: 5s - loss: 0.5381 - accuracy: 0.83 - ETA: 5s - loss: 0.5375 - accuracy: 0.83 - ETA: 5s - loss: 0.5362 - accuracy: 0.83 - ETA: 5s - loss: 0.5378 - accuracy: 0.83 - ETA: 5s - loss: 0.5368 - accuracy: 0.83 - ETA: 5s - loss: 0.5372 - accuracy: 0.83 - ETA: 4s - loss: 0.5363 - accuracy: 0.83 - ETA: 4s - loss: 0.5369 - accuracy: 0.83 - ETA: 4s - loss: 0.5369 - accuracy: 0.83 - ETA: 4s - loss: 0.5370 - accuracy: 0.83 - ETA: 4s - loss: 0.5387 - accuracy: 0.83 - ETA: 4s - loss: 0.5375 - accuracy: 0.83 - ETA: 4s - loss: 0.5376 - accuracy: 0.83 - ETA: 3s - loss: 0.5379 - accuracy: 0.83 - ETA: 3s - loss: 0.5376 - accuracy: 0.83 - ETA: 3s - loss: 0.5369 - accuracy: 0.83 - ETA: 3s - loss: 0.5375 - accuracy: 0.83 - ETA: 3s - loss: 0.5369 - accuracy: 0.83 - ETA: 3s - loss: 0.5384 - accuracy: 0.83 - ETA: 3s - loss: 0.5387 - accuracy: 0.83 - ETA: 2s - loss: 0.5384 - accuracy: 0.83 - ETA: 2s - loss: 0.5380 - accuracy: 0.83 - ETA: 2s - loss: 0.5374 - accuracy: 0.83 - ETA: 2s - loss: 0.5364 - accuracy: 0.83 - ETA: 2s - loss: 0.5368 - accuracy: 0.83 - ETA: 2s - loss: 0.5369 - accuracy: 0.83 - ETA: 2s - loss: 0.5371 - accuracy: 0.83 - ETA: 1s - loss: 0.5379 - accuracy: 0.83 - ETA: 1s - loss: 0.5378 - accuracy: 0.83 - ETA: 1s - loss: 0.5385 - accuracy: 0.83 - ETA: 1s - loss: 0.5391 - accuracy: 0.83 - ETA: 1s - loss: 0.5396 - accuracy: 0.83 - ETA: 1s - loss: 0.5400 - accuracy: 0.83 - ETA: 1s - loss: 0.5400 - accuracy: 0.83 - ETA: 0s - loss: 0.5397 - accuracy: 0.83 - ETA: 0s - loss: 0.5396 - accuracy: 0.83 - ETA: 0s - loss: 0.5402 - accuracy: 0.83 - ETA: 0s - loss: 0.5400 - accuracy: 0.83 - ETA: 0s - loss: 0.5400 - accuracy: 0.83 - ETA: 0s - loss: 0.5403 - accuracy: 0.83 - ETA: 0s - loss: 0.5396 - accuracy: 0.8354\n",
      "Epoch 00010: val_accuracy improved from 0.87621 to 0.88589, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.5392 - accuracy: 0.8356 - val_loss: 0.3882 - val_accuracy: 0.8859\n",
      "Epoch 11/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.5476 - accuracy: 0.827 - ETA: 11s - loss: 0.5084 - accuracy: 0.841 - ETA: 11s - loss: 0.5234 - accuracy: 0.840 - ETA: 11s - loss: 0.5130 - accuracy: 0.843 - ETA: 11s - loss: 0.5135 - accuracy: 0.841 - ETA: 11s - loss: 0.5143 - accuracy: 0.843 - ETA: 11s - loss: 0.5252 - accuracy: 0.840 - ETA: 10s - loss: 0.5246 - accuracy: 0.840 - ETA: 10s - loss: 0.5250 - accuracy: 0.841 - ETA: 10s - loss: 0.5202 - accuracy: 0.841 - ETA: 10s - loss: 0.5272 - accuracy: 0.839 - ETA: 10s - loss: 0.5297 - accuracy: 0.839 - ETA: 10s - loss: 0.5301 - accuracy: 0.839 - ETA: 10s - loss: 0.5258 - accuracy: 0.840 - ETA: 9s - loss: 0.5245 - accuracy: 0.840 - ETA: 9s - loss: 0.5234 - accuracy: 0.84 - ETA: 9s - loss: 0.5227 - accuracy: 0.84 - ETA: 9s - loss: 0.5216 - accuracy: 0.84 - ETA: 9s - loss: 0.5243 - accuracy: 0.84 - ETA: 9s - loss: 0.5222 - accuracy: 0.84 - ETA: 9s - loss: 0.5225 - accuracy: 0.84 - ETA: 8s - loss: 0.5199 - accuracy: 0.84 - ETA: 8s - loss: 0.5164 - accuracy: 0.84 - ETA: 8s - loss: 0.5177 - accuracy: 0.84 - ETA: 8s - loss: 0.5168 - accuracy: 0.84 - ETA: 8s - loss: 0.5163 - accuracy: 0.84 - ETA: 8s - loss: 0.5149 - accuracy: 0.84 - ETA: 8s - loss: 0.5128 - accuracy: 0.84 - ETA: 7s - loss: 0.5102 - accuracy: 0.84 - ETA: 7s - loss: 0.5102 - accuracy: 0.84 - ETA: 7s - loss: 0.5100 - accuracy: 0.84 - ETA: 7s - loss: 0.5093 - accuracy: 0.84 - ETA: 7s - loss: 0.5087 - accuracy: 0.84 - ETA: 7s - loss: 0.5099 - accuracy: 0.84 - ETA: 7s - loss: 0.5146 - accuracy: 0.84 - ETA: 6s - loss: 0.5172 - accuracy: 0.84 - ETA: 6s - loss: 0.5158 - accuracy: 0.84 - ETA: 6s - loss: 0.5162 - accuracy: 0.84 - ETA: 6s - loss: 0.5154 - accuracy: 0.84 - ETA: 6s - loss: 0.5141 - accuracy: 0.84 - ETA: 6s - loss: 0.5151 - accuracy: 0.84 - ETA: 6s - loss: 0.5157 - accuracy: 0.84 - ETA: 5s - loss: 0.5142 - accuracy: 0.84 - ETA: 5s - loss: 0.5132 - accuracy: 0.84 - ETA: 5s - loss: 0.5131 - accuracy: 0.84 - ETA: 5s - loss: 0.5126 - accuracy: 0.84 - ETA: 5s - loss: 0.5123 - accuracy: 0.84 - ETA: 5s - loss: 0.5111 - accuracy: 0.84 - ETA: 5s - loss: 0.5106 - accuracy: 0.84 - ETA: 5s - loss: 0.5113 - accuracy: 0.84 - ETA: 4s - loss: 0.5125 - accuracy: 0.84 - ETA: 4s - loss: 0.5116 - accuracy: 0.84 - ETA: 4s - loss: 0.5121 - accuracy: 0.84 - ETA: 4s - loss: 0.5115 - accuracy: 0.84 - ETA: 4s - loss: 0.5116 - accuracy: 0.84 - ETA: 4s - loss: 0.5122 - accuracy: 0.84 - ETA: 4s - loss: 0.5134 - accuracy: 0.84 - ETA: 3s - loss: 0.5129 - accuracy: 0.84 - ETA: 3s - loss: 0.5118 - accuracy: 0.84 - ETA: 3s - loss: 0.5132 - accuracy: 0.84 - ETA: 3s - loss: 0.5140 - accuracy: 0.84 - ETA: 3s - loss: 0.5135 - accuracy: 0.84 - ETA: 3s - loss: 0.5128 - accuracy: 0.84 - ETA: 3s - loss: 0.5124 - accuracy: 0.84 - ETA: 2s - loss: 0.5124 - accuracy: 0.84 - ETA: 2s - loss: 0.5124 - accuracy: 0.84 - ETA: 2s - loss: 0.5122 - accuracy: 0.84 - ETA: 2s - loss: 0.5120 - accuracy: 0.84 - ETA: 2s - loss: 0.5119 - accuracy: 0.84 - ETA: 2s - loss: 0.5125 - accuracy: 0.84 - ETA: 1s - loss: 0.5132 - accuracy: 0.84 - ETA: 1s - loss: 0.5132 - accuracy: 0.84 - ETA: 1s - loss: 0.5137 - accuracy: 0.84 - ETA: 1s - loss: 0.5133 - accuracy: 0.84 - ETA: 1s - loss: 0.5132 - accuracy: 0.84 - ETA: 1s - loss: 0.5133 - accuracy: 0.84 - ETA: 1s - loss: 0.5132 - accuracy: 0.84 - ETA: 0s - loss: 0.5130 - accuracy: 0.84 - ETA: 0s - loss: 0.5125 - accuracy: 0.84 - ETA: 0s - loss: 0.5131 - accuracy: 0.84 - ETA: 0s - loss: 0.5143 - accuracy: 0.84 - ETA: 0s - loss: 0.5138 - accuracy: 0.84 - ETA: 0s - loss: 0.5144 - accuracy: 0.84 - ETA: 0s - loss: 0.5140 - accuracy: 0.8427\n",
      "Epoch 00011: val_accuracy improved from 0.88589 to 0.88790, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.5142 - accuracy: 0.8426 - val_loss: 0.3837 - val_accuracy: 0.8879\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.5202 - accuracy: 0.834 - ETA: 11s - loss: 0.5063 - accuracy: 0.842 - ETA: 11s - loss: 0.4894 - accuracy: 0.848 - ETA: 11s - loss: 0.4874 - accuracy: 0.849 - ETA: 11s - loss: 0.4884 - accuracy: 0.848 - ETA: 11s - loss: 0.4884 - accuracy: 0.848 - ETA: 10s - loss: 0.4910 - accuracy: 0.847 - ETA: 10s - loss: 0.4905 - accuracy: 0.847 - ETA: 10s - loss: 0.4892 - accuracy: 0.849 - ETA: 10s - loss: 0.4930 - accuracy: 0.849 - ETA: 10s - loss: 0.4899 - accuracy: 0.850 - ETA: 10s - loss: 0.4896 - accuracy: 0.850 - ETA: 10s - loss: 0.4893 - accuracy: 0.849 - ETA: 9s - loss: 0.4900 - accuracy: 0.849 - ETA: 9s - loss: 0.4926 - accuracy: 0.84 - ETA: 9s - loss: 0.4905 - accuracy: 0.84 - ETA: 9s - loss: 0.4921 - accuracy: 0.84 - ETA: 9s - loss: 0.4891 - accuracy: 0.84 - ETA: 9s - loss: 0.4888 - accuracy: 0.84 - ETA: 9s - loss: 0.4858 - accuracy: 0.84 - ETA: 8s - loss: 0.4861 - accuracy: 0.84 - ETA: 8s - loss: 0.4833 - accuracy: 0.85 - ETA: 8s - loss: 0.4838 - accuracy: 0.85 - ETA: 8s - loss: 0.4843 - accuracy: 0.85 - ETA: 8s - loss: 0.4864 - accuracy: 0.85 - ETA: 8s - loss: 0.4850 - accuracy: 0.85 - ETA: 8s - loss: 0.4850 - accuracy: 0.85 - ETA: 7s - loss: 0.4849 - accuracy: 0.85 - ETA: 7s - loss: 0.4861 - accuracy: 0.85 - ETA: 7s - loss: 0.4841 - accuracy: 0.85 - ETA: 7s - loss: 0.4844 - accuracy: 0.85 - ETA: 7s - loss: 0.4865 - accuracy: 0.85 - ETA: 7s - loss: 0.4872 - accuracy: 0.85 - ETA: 7s - loss: 0.4883 - accuracy: 0.85 - ETA: 6s - loss: 0.4871 - accuracy: 0.85 - ETA: 6s - loss: 0.4875 - accuracy: 0.85 - ETA: 6s - loss: 0.4868 - accuracy: 0.85 - ETA: 6s - loss: 0.4868 - accuracy: 0.85 - ETA: 6s - loss: 0.4858 - accuracy: 0.85 - ETA: 6s - loss: 0.4867 - accuracy: 0.85 - ETA: 6s - loss: 0.4871 - accuracy: 0.85 - ETA: 5s - loss: 0.4872 - accuracy: 0.85 - ETA: 5s - loss: 0.4872 - accuracy: 0.85 - ETA: 5s - loss: 0.4878 - accuracy: 0.85 - ETA: 5s - loss: 0.4871 - accuracy: 0.85 - ETA: 5s - loss: 0.4873 - accuracy: 0.85 - ETA: 5s - loss: 0.4883 - accuracy: 0.85 - ETA: 5s - loss: 0.4887 - accuracy: 0.85 - ETA: 5s - loss: 0.4881 - accuracy: 0.85 - ETA: 4s - loss: 0.4883 - accuracy: 0.85 - ETA: 4s - loss: 0.4864 - accuracy: 0.85 - ETA: 4s - loss: 0.4872 - accuracy: 0.85 - ETA: 4s - loss: 0.4864 - accuracy: 0.85 - ETA: 4s - loss: 0.4852 - accuracy: 0.85 - ETA: 4s - loss: 0.4861 - accuracy: 0.85 - ETA: 4s - loss: 0.4861 - accuracy: 0.85 - ETA: 3s - loss: 0.4860 - accuracy: 0.85 - ETA: 3s - loss: 0.4860 - accuracy: 0.85 - ETA: 3s - loss: 0.4852 - accuracy: 0.85 - ETA: 3s - loss: 0.4852 - accuracy: 0.85 - ETA: 3s - loss: 0.4855 - accuracy: 0.85 - ETA: 3s - loss: 0.4847 - accuracy: 0.85 - ETA: 3s - loss: 0.4853 - accuracy: 0.85 - ETA: 2s - loss: 0.4849 - accuracy: 0.85 - ETA: 2s - loss: 0.4855 - accuracy: 0.85 - ETA: 2s - loss: 0.4851 - accuracy: 0.85 - ETA: 2s - loss: 0.4847 - accuracy: 0.85 - ETA: 2s - loss: 0.4856 - accuracy: 0.85 - ETA: 2s - loss: 0.4867 - accuracy: 0.85 - ETA: 2s - loss: 0.4877 - accuracy: 0.85 - ETA: 1s - loss: 0.4876 - accuracy: 0.85 - ETA: 1s - loss: 0.4878 - accuracy: 0.85 - ETA: 1s - loss: 0.4884 - accuracy: 0.85 - ETA: 1s - loss: 0.4885 - accuracy: 0.85 - ETA: 1s - loss: 0.4890 - accuracy: 0.85 - ETA: 1s - loss: 0.4895 - accuracy: 0.84 - ETA: 1s - loss: 0.4904 - accuracy: 0.84 - ETA: 0s - loss: 0.4905 - accuracy: 0.84 - ETA: 0s - loss: 0.4895 - accuracy: 0.84 - ETA: 0s - loss: 0.4898 - accuracy: 0.84 - ETA: 0s - loss: 0.4895 - accuracy: 0.84 - ETA: 0s - loss: 0.4890 - accuracy: 0.85 - ETA: 0s - loss: 0.4883 - accuracy: 0.85 - ETA: 0s - loss: 0.4876 - accuracy: 0.8503\n",
      "Epoch 00012: val_accuracy improved from 0.88790 to 0.89042, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 149us/sample - loss: 0.4872 - accuracy: 0.8505 - val_loss: 0.3685 - val_accuracy: 0.8904\n",
      "Epoch 13/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.4353 - accuracy: 0.869 - ETA: 11s - loss: 0.4620 - accuracy: 0.861 - ETA: 11s - loss: 0.4539 - accuracy: 0.863 - ETA: 11s - loss: 0.4620 - accuracy: 0.859 - ETA: 11s - loss: 0.4613 - accuracy: 0.857 - ETA: 11s - loss: 0.4594 - accuracy: 0.858 - ETA: 10s - loss: 0.4567 - accuracy: 0.858 - ETA: 10s - loss: 0.4593 - accuracy: 0.859 - ETA: 10s - loss: 0.4666 - accuracy: 0.857 - ETA: 10s - loss: 0.4697 - accuracy: 0.856 - ETA: 10s - loss: 0.4721 - accuracy: 0.856 - ETA: 10s - loss: 0.4676 - accuracy: 0.858 - ETA: 10s - loss: 0.4660 - accuracy: 0.858 - ETA: 9s - loss: 0.4667 - accuracy: 0.857 - ETA: 9s - loss: 0.4650 - accuracy: 0.85 - ETA: 9s - loss: 0.4599 - accuracy: 0.85 - ETA: 9s - loss: 0.4568 - accuracy: 0.85 - ETA: 9s - loss: 0.4567 - accuracy: 0.85 - ETA: 9s - loss: 0.4568 - accuracy: 0.85 - ETA: 9s - loss: 0.4590 - accuracy: 0.85 - ETA: 8s - loss: 0.4617 - accuracy: 0.85 - ETA: 8s - loss: 0.4622 - accuracy: 0.85 - ETA: 8s - loss: 0.4602 - accuracy: 0.85 - ETA: 8s - loss: 0.4587 - accuracy: 0.85 - ETA: 8s - loss: 0.4580 - accuracy: 0.85 - ETA: 8s - loss: 0.4577 - accuracy: 0.85 - ETA: 8s - loss: 0.4554 - accuracy: 0.85 - ETA: 7s - loss: 0.4526 - accuracy: 0.86 - ETA: 7s - loss: 0.4528 - accuracy: 0.86 - ETA: 7s - loss: 0.4541 - accuracy: 0.85 - ETA: 7s - loss: 0.4565 - accuracy: 0.85 - ETA: 7s - loss: 0.4572 - accuracy: 0.85 - ETA: 7s - loss: 0.4574 - accuracy: 0.85 - ETA: 7s - loss: 0.4592 - accuracy: 0.85 - ETA: 6s - loss: 0.4592 - accuracy: 0.85 - ETA: 6s - loss: 0.4586 - accuracy: 0.85 - ETA: 6s - loss: 0.4579 - accuracy: 0.85 - ETA: 6s - loss: 0.4589 - accuracy: 0.85 - ETA: 6s - loss: 0.4580 - accuracy: 0.85 - ETA: 6s - loss: 0.4582 - accuracy: 0.85 - ETA: 6s - loss: 0.4583 - accuracy: 0.85 - ETA: 6s - loss: 0.4585 - accuracy: 0.85 - ETA: 5s - loss: 0.4583 - accuracy: 0.85 - ETA: 5s - loss: 0.4596 - accuracy: 0.85 - ETA: 5s - loss: 0.4595 - accuracy: 0.85 - ETA: 5s - loss: 0.4591 - accuracy: 0.85 - ETA: 5s - loss: 0.4596 - accuracy: 0.85 - ETA: 5s - loss: 0.4600 - accuracy: 0.85 - ETA: 5s - loss: 0.4600 - accuracy: 0.85 - ETA: 4s - loss: 0.4599 - accuracy: 0.85 - ETA: 4s - loss: 0.4591 - accuracy: 0.85 - ETA: 4s - loss: 0.4597 - accuracy: 0.85 - ETA: 4s - loss: 0.4604 - accuracy: 0.85 - ETA: 4s - loss: 0.4602 - accuracy: 0.85 - ETA: 4s - loss: 0.4605 - accuracy: 0.85 - ETA: 4s - loss: 0.4600 - accuracy: 0.85 - ETA: 3s - loss: 0.4599 - accuracy: 0.85 - ETA: 3s - loss: 0.4597 - accuracy: 0.85 - ETA: 3s - loss: 0.4597 - accuracy: 0.85 - ETA: 3s - loss: 0.4593 - accuracy: 0.85 - ETA: 3s - loss: 0.4597 - accuracy: 0.85 - ETA: 3s - loss: 0.4608 - accuracy: 0.85 - ETA: 3s - loss: 0.4614 - accuracy: 0.85 - ETA: 2s - loss: 0.4618 - accuracy: 0.85 - ETA: 2s - loss: 0.4618 - accuracy: 0.85 - ETA: 2s - loss: 0.4623 - accuracy: 0.85 - ETA: 2s - loss: 0.4618 - accuracy: 0.85 - ETA: 2s - loss: 0.4614 - accuracy: 0.85 - ETA: 2s - loss: 0.4611 - accuracy: 0.85 - ETA: 2s - loss: 0.4607 - accuracy: 0.85 - ETA: 1s - loss: 0.4603 - accuracy: 0.85 - ETA: 1s - loss: 0.4615 - accuracy: 0.85 - ETA: 1s - loss: 0.4622 - accuracy: 0.85 - ETA: 1s - loss: 0.4621 - accuracy: 0.85 - ETA: 1s - loss: 0.4623 - accuracy: 0.85 - ETA: 1s - loss: 0.4626 - accuracy: 0.85 - ETA: 1s - loss: 0.4622 - accuracy: 0.85 - ETA: 0s - loss: 0.4627 - accuracy: 0.85 - ETA: 0s - loss: 0.4629 - accuracy: 0.85 - ETA: 0s - loss: 0.4634 - accuracy: 0.85 - ETA: 0s - loss: 0.4634 - accuracy: 0.85 - ETA: 0s - loss: 0.4635 - accuracy: 0.85 - ETA: 0s - loss: 0.4638 - accuracy: 0.85 - ETA: 0s - loss: 0.4635 - accuracy: 0.8562\n",
      "Epoch 00013: val_accuracy improved from 0.89042 to 0.89254, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 149us/sample - loss: 0.4635 - accuracy: 0.8562 - val_loss: 0.3617 - val_accuracy: 0.8925\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.4041 - accuracy: 0.884 - ETA: 11s - loss: 0.4162 - accuracy: 0.878 - ETA: 11s - loss: 0.4381 - accuracy: 0.866 - ETA: 11s - loss: 0.4357 - accuracy: 0.867 - ETA: 11s - loss: 0.4286 - accuracy: 0.867 - ETA: 10s - loss: 0.4357 - accuracy: 0.866 - ETA: 10s - loss: 0.4302 - accuracy: 0.867 - ETA: 10s - loss: 0.4336 - accuracy: 0.866 - ETA: 10s - loss: 0.4336 - accuracy: 0.865 - ETA: 10s - loss: 0.4271 - accuracy: 0.868 - ETA: 10s - loss: 0.4315 - accuracy: 0.865 - ETA: 10s - loss: 0.4367 - accuracy: 0.863 - ETA: 10s - loss: 0.4372 - accuracy: 0.864 - ETA: 9s - loss: 0.4373 - accuracy: 0.864 - ETA: 9s - loss: 0.4407 - accuracy: 0.86 - ETA: 9s - loss: 0.4417 - accuracy: 0.86 - ETA: 9s - loss: 0.4433 - accuracy: 0.86 - ETA: 9s - loss: 0.4447 - accuracy: 0.86 - ETA: 9s - loss: 0.4457 - accuracy: 0.86 - ETA: 9s - loss: 0.4432 - accuracy: 0.86 - ETA: 9s - loss: 0.4423 - accuracy: 0.86 - ETA: 8s - loss: 0.4433 - accuracy: 0.86 - ETA: 8s - loss: 0.4433 - accuracy: 0.86 - ETA: 8s - loss: 0.4435 - accuracy: 0.86 - ETA: 8s - loss: 0.4437 - accuracy: 0.86 - ETA: 8s - loss: 0.4429 - accuracy: 0.86 - ETA: 8s - loss: 0.4411 - accuracy: 0.86 - ETA: 8s - loss: 0.4415 - accuracy: 0.86 - ETA: 7s - loss: 0.4400 - accuracy: 0.86 - ETA: 7s - loss: 0.4417 - accuracy: 0.86 - ETA: 7s - loss: 0.4425 - accuracy: 0.86 - ETA: 7s - loss: 0.4427 - accuracy: 0.86 - ETA: 7s - loss: 0.4429 - accuracy: 0.86 - ETA: 7s - loss: 0.4437 - accuracy: 0.86 - ETA: 7s - loss: 0.4450 - accuracy: 0.86 - ETA: 6s - loss: 0.4463 - accuracy: 0.86 - ETA: 6s - loss: 0.4452 - accuracy: 0.86 - ETA: 6s - loss: 0.4454 - accuracy: 0.86 - ETA: 6s - loss: 0.4442 - accuracy: 0.86 - ETA: 6s - loss: 0.4437 - accuracy: 0.86 - ETA: 6s - loss: 0.4445 - accuracy: 0.86 - ETA: 6s - loss: 0.4446 - accuracy: 0.86 - ETA: 5s - loss: 0.4447 - accuracy: 0.86 - ETA: 5s - loss: 0.4456 - accuracy: 0.86 - ETA: 5s - loss: 0.4468 - accuracy: 0.86 - ETA: 5s - loss: 0.4466 - accuracy: 0.86 - ETA: 5s - loss: 0.4468 - accuracy: 0.86 - ETA: 5s - loss: 0.4470 - accuracy: 0.86 - ETA: 5s - loss: 0.4472 - accuracy: 0.86 - ETA: 4s - loss: 0.4474 - accuracy: 0.86 - ETA: 4s - loss: 0.4478 - accuracy: 0.86 - ETA: 4s - loss: 0.4481 - accuracy: 0.86 - ETA: 4s - loss: 0.4484 - accuracy: 0.86 - ETA: 4s - loss: 0.4472 - accuracy: 0.86 - ETA: 4s - loss: 0.4468 - accuracy: 0.86 - ETA: 4s - loss: 0.4467 - accuracy: 0.86 - ETA: 3s - loss: 0.4461 - accuracy: 0.86 - ETA: 3s - loss: 0.4477 - accuracy: 0.86 - ETA: 3s - loss: 0.4482 - accuracy: 0.86 - ETA: 3s - loss: 0.4482 - accuracy: 0.86 - ETA: 3s - loss: 0.4480 - accuracy: 0.86 - ETA: 3s - loss: 0.4474 - accuracy: 0.86 - ETA: 3s - loss: 0.4479 - accuracy: 0.86 - ETA: 2s - loss: 0.4482 - accuracy: 0.86 - ETA: 2s - loss: 0.4477 - accuracy: 0.86 - ETA: 2s - loss: 0.4478 - accuracy: 0.86 - ETA: 2s - loss: 0.4478 - accuracy: 0.86 - ETA: 2s - loss: 0.4477 - accuracy: 0.86 - ETA: 2s - loss: 0.4462 - accuracy: 0.86 - ETA: 2s - loss: 0.4463 - accuracy: 0.86 - ETA: 1s - loss: 0.4466 - accuracy: 0.86 - ETA: 1s - loss: 0.4466 - accuracy: 0.86 - ETA: 1s - loss: 0.4471 - accuracy: 0.86 - ETA: 1s - loss: 0.4469 - accuracy: 0.86 - ETA: 1s - loss: 0.4467 - accuracy: 0.86 - ETA: 1s - loss: 0.4467 - accuracy: 0.86 - ETA: 1s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4467 - accuracy: 0.86 - ETA: 0s - loss: 0.4461 - accuracy: 0.86 - ETA: 0s - loss: 0.4462 - accuracy: 0.86 - ETA: 0s - loss: 0.4465 - accuracy: 0.86 - ETA: 0s - loss: 0.4459 - accuracy: 0.86 - ETA: 0s - loss: 0.4466 - accuracy: 0.86 - ETA: 0s - loss: 0.4473 - accuracy: 0.8620\n",
      "Epoch 00014: val_accuracy did not improve from 0.89254\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.4479 - accuracy: 0.8619 - val_loss: 0.3560 - val_accuracy: 0.8902\n",
      "Epoch 15/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.4432 - accuracy: 0.849 - ETA: 11s - loss: 0.4149 - accuracy: 0.866 - ETA: 11s - loss: 0.4205 - accuracy: 0.869 - ETA: 11s - loss: 0.4299 - accuracy: 0.865 - ETA: 11s - loss: 0.4461 - accuracy: 0.859 - ETA: 11s - loss: 0.4515 - accuracy: 0.858 - ETA: 10s - loss: 0.4460 - accuracy: 0.860 - ETA: 10s - loss: 0.4440 - accuracy: 0.860 - ETA: 10s - loss: 0.4444 - accuracy: 0.861 - ETA: 10s - loss: 0.4407 - accuracy: 0.862 - ETA: 10s - loss: 0.4418 - accuracy: 0.862 - ETA: 10s - loss: 0.4413 - accuracy: 0.860 - ETA: 10s - loss: 0.4408 - accuracy: 0.862 - ETA: 9s - loss: 0.4393 - accuracy: 0.862 - ETA: 9s - loss: 0.4399 - accuracy: 0.86 - ETA: 9s - loss: 0.4471 - accuracy: 0.86 - ETA: 9s - loss: 0.4460 - accuracy: 0.86 - ETA: 9s - loss: 0.4446 - accuracy: 0.86 - ETA: 9s - loss: 0.4450 - accuracy: 0.86 - ETA: 9s - loss: 0.4426 - accuracy: 0.86 - ETA: 8s - loss: 0.4428 - accuracy: 0.86 - ETA: 8s - loss: 0.4416 - accuracy: 0.86 - ETA: 8s - loss: 0.4406 - accuracy: 0.86 - ETA: 8s - loss: 0.4416 - accuracy: 0.86 - ETA: 8s - loss: 0.4429 - accuracy: 0.86 - ETA: 8s - loss: 0.4435 - accuracy: 0.86 - ETA: 8s - loss: 0.4430 - accuracy: 0.86 - ETA: 7s - loss: 0.4445 - accuracy: 0.86 - ETA: 7s - loss: 0.4435 - accuracy: 0.86 - ETA: 7s - loss: 0.4419 - accuracy: 0.86 - ETA: 7s - loss: 0.4397 - accuracy: 0.86 - ETA: 7s - loss: 0.4393 - accuracy: 0.86 - ETA: 7s - loss: 0.4369 - accuracy: 0.86 - ETA: 7s - loss: 0.4362 - accuracy: 0.86 - ETA: 6s - loss: 0.4363 - accuracy: 0.86 - ETA: 6s - loss: 0.4362 - accuracy: 0.86 - ETA: 6s - loss: 0.4363 - accuracy: 0.86 - ETA: 6s - loss: 0.4380 - accuracy: 0.86 - ETA: 6s - loss: 0.4383 - accuracy: 0.86 - ETA: 6s - loss: 0.4376 - accuracy: 0.86 - ETA: 6s - loss: 0.4374 - accuracy: 0.86 - ETA: 6s - loss: 0.4358 - accuracy: 0.86 - ETA: 5s - loss: 0.4358 - accuracy: 0.86 - ETA: 5s - loss: 0.4362 - accuracy: 0.86 - ETA: 5s - loss: 0.4360 - accuracy: 0.86 - ETA: 5s - loss: 0.4362 - accuracy: 0.86 - ETA: 5s - loss: 0.4366 - accuracy: 0.86 - ETA: 5s - loss: 0.4361 - accuracy: 0.86 - ETA: 5s - loss: 0.4367 - accuracy: 0.86 - ETA: 4s - loss: 0.4366 - accuracy: 0.86 - ETA: 4s - loss: 0.4362 - accuracy: 0.86 - ETA: 4s - loss: 0.4360 - accuracy: 0.86 - ETA: 4s - loss: 0.4364 - accuracy: 0.86 - ETA: 4s - loss: 0.4362 - accuracy: 0.86 - ETA: 4s - loss: 0.4359 - accuracy: 0.86 - ETA: 4s - loss: 0.4366 - accuracy: 0.86 - ETA: 3s - loss: 0.4368 - accuracy: 0.86 - ETA: 3s - loss: 0.4370 - accuracy: 0.86 - ETA: 3s - loss: 0.4370 - accuracy: 0.86 - ETA: 3s - loss: 0.4374 - accuracy: 0.86 - ETA: 3s - loss: 0.4369 - accuracy: 0.86 - ETA: 3s - loss: 0.4372 - accuracy: 0.86 - ETA: 3s - loss: 0.4369 - accuracy: 0.86 - ETA: 2s - loss: 0.4365 - accuracy: 0.86 - ETA: 2s - loss: 0.4369 - accuracy: 0.86 - ETA: 2s - loss: 0.4366 - accuracy: 0.86 - ETA: 2s - loss: 0.4364 - accuracy: 0.86 - ETA: 2s - loss: 0.4369 - accuracy: 0.86 - ETA: 2s - loss: 0.4380 - accuracy: 0.86 - ETA: 2s - loss: 0.4380 - accuracy: 0.86 - ETA: 1s - loss: 0.4381 - accuracy: 0.86 - ETA: 1s - loss: 0.4372 - accuracy: 0.86 - ETA: 1s - loss: 0.4377 - accuracy: 0.86 - ETA: 1s - loss: 0.4376 - accuracy: 0.86 - ETA: 1s - loss: 0.4369 - accuracy: 0.86 - ETA: 1s - loss: 0.4365 - accuracy: 0.86 - ETA: 1s - loss: 0.4357 - accuracy: 0.86 - ETA: 0s - loss: 0.4358 - accuracy: 0.86 - ETA: 0s - loss: 0.4346 - accuracy: 0.86 - ETA: 0s - loss: 0.4342 - accuracy: 0.86 - ETA: 0s - loss: 0.4340 - accuracy: 0.86 - ETA: 0s - loss: 0.4343 - accuracy: 0.86 - ETA: 0s - loss: 0.4342 - accuracy: 0.86 - ETA: 0s - loss: 0.4336 - accuracy: 0.8662\n",
      "Epoch 00015: val_accuracy improved from 0.89254 to 0.89677, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 151us/sample - loss: 0.4333 - accuracy: 0.8662 - val_loss: 0.3472 - val_accuracy: 0.8968\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.4315 - accuracy: 0.873 - ETA: 11s - loss: 0.4400 - accuracy: 0.863 - ETA: 11s - loss: 0.4260 - accuracy: 0.874 - ETA: 11s - loss: 0.4336 - accuracy: 0.870 - ETA: 11s - loss: 0.4344 - accuracy: 0.869 - ETA: 11s - loss: 0.4348 - accuracy: 0.871 - ETA: 10s - loss: 0.4377 - accuracy: 0.869 - ETA: 10s - loss: 0.4400 - accuracy: 0.868 - ETA: 10s - loss: 0.4327 - accuracy: 0.869 - ETA: 10s - loss: 0.4294 - accuracy: 0.869 - ETA: 10s - loss: 0.4291 - accuracy: 0.869 - ETA: 10s - loss: 0.4325 - accuracy: 0.867 - ETA: 10s - loss: 0.4359 - accuracy: 0.866 - ETA: 9s - loss: 0.4369 - accuracy: 0.867 - ETA: 9s - loss: 0.4355 - accuracy: 0.86 - ETA: 9s - loss: 0.4348 - accuracy: 0.86 - ETA: 9s - loss: 0.4361 - accuracy: 0.86 - ETA: 9s - loss: 0.4348 - accuracy: 0.86 - ETA: 9s - loss: 0.4348 - accuracy: 0.86 - ETA: 9s - loss: 0.4324 - accuracy: 0.86 - ETA: 8s - loss: 0.4307 - accuracy: 0.86 - ETA: 8s - loss: 0.4302 - accuracy: 0.86 - ETA: 8s - loss: 0.4304 - accuracy: 0.86 - ETA: 8s - loss: 0.4306 - accuracy: 0.86 - ETA: 8s - loss: 0.4308 - accuracy: 0.86 - ETA: 8s - loss: 0.4291 - accuracy: 0.86 - ETA: 8s - loss: 0.4271 - accuracy: 0.86 - ETA: 8s - loss: 0.4278 - accuracy: 0.86 - ETA: 7s - loss: 0.4275 - accuracy: 0.86 - ETA: 7s - loss: 0.4304 - accuracy: 0.86 - ETA: 7s - loss: 0.4328 - accuracy: 0.86 - ETA: 7s - loss: 0.4337 - accuracy: 0.86 - ETA: 7s - loss: 0.4327 - accuracy: 0.86 - ETA: 7s - loss: 0.4302 - accuracy: 0.86 - ETA: 7s - loss: 0.4291 - accuracy: 0.86 - ETA: 7s - loss: 0.4282 - accuracy: 0.86 - ETA: 6s - loss: 0.4273 - accuracy: 0.86 - ETA: 6s - loss: 0.4266 - accuracy: 0.86 - ETA: 6s - loss: 0.4269 - accuracy: 0.86 - ETA: 6s - loss: 0.4267 - accuracy: 0.86 - ETA: 6s - loss: 0.4266 - accuracy: 0.86 - ETA: 6s - loss: 0.4277 - accuracy: 0.86 - ETA: 6s - loss: 0.4280 - accuracy: 0.86 - ETA: 5s - loss: 0.4290 - accuracy: 0.86 - ETA: 5s - loss: 0.4298 - accuracy: 0.86 - ETA: 5s - loss: 0.4291 - accuracy: 0.86 - ETA: 5s - loss: 0.4270 - accuracy: 0.86 - ETA: 5s - loss: 0.4250 - accuracy: 0.86 - ETA: 5s - loss: 0.4251 - accuracy: 0.86 - ETA: 5s - loss: 0.4240 - accuracy: 0.86 - ETA: 4s - loss: 0.4236 - accuracy: 0.86 - ETA: 4s - loss: 0.4219 - accuracy: 0.86 - ETA: 4s - loss: 0.4219 - accuracy: 0.86 - ETA: 4s - loss: 0.4227 - accuracy: 0.86 - ETA: 4s - loss: 0.4232 - accuracy: 0.86 - ETA: 4s - loss: 0.4228 - accuracy: 0.86 - ETA: 4s - loss: 0.4232 - accuracy: 0.86 - ETA: 3s - loss: 0.4232 - accuracy: 0.86 - ETA: 3s - loss: 0.4232 - accuracy: 0.86 - ETA: 3s - loss: 0.4228 - accuracy: 0.86 - ETA: 3s - loss: 0.4224 - accuracy: 0.86 - ETA: 3s - loss: 0.4222 - accuracy: 0.86 - ETA: 3s - loss: 0.4223 - accuracy: 0.86 - ETA: 3s - loss: 0.4218 - accuracy: 0.86 - ETA: 2s - loss: 0.4227 - accuracy: 0.86 - ETA: 2s - loss: 0.4221 - accuracy: 0.86 - ETA: 2s - loss: 0.4217 - accuracy: 0.86 - ETA: 2s - loss: 0.4218 - accuracy: 0.86 - ETA: 2s - loss: 0.4221 - accuracy: 0.86 - ETA: 2s - loss: 0.4224 - accuracy: 0.86 - ETA: 2s - loss: 0.4222 - accuracy: 0.86 - ETA: 1s - loss: 0.4220 - accuracy: 0.86 - ETA: 1s - loss: 0.4220 - accuracy: 0.86 - ETA: 1s - loss: 0.4215 - accuracy: 0.86 - ETA: 1s - loss: 0.4217 - accuracy: 0.86 - ETA: 1s - loss: 0.4208 - accuracy: 0.86 - ETA: 1s - loss: 0.4211 - accuracy: 0.86 - ETA: 0s - loss: 0.4208 - accuracy: 0.86 - ETA: 0s - loss: 0.4213 - accuracy: 0.86 - ETA: 0s - loss: 0.4217 - accuracy: 0.86 - ETA: 0s - loss: 0.4216 - accuracy: 0.86 - ETA: 0s - loss: 0.4209 - accuracy: 0.86 - ETA: 0s - loss: 0.4211 - accuracy: 0.86 - ETA: 0s - loss: 0.4208 - accuracy: 0.8683\n",
      "Epoch 00016: val_accuracy improved from 0.89677 to 0.89990, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.4208 - accuracy: 0.8682 - val_loss: 0.3408 - val_accuracy: 0.8999\n",
      "Epoch 17/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3698 - accuracy: 0.895 - ETA: 11s - loss: 0.3649 - accuracy: 0.893 - ETA: 11s - loss: 0.3525 - accuracy: 0.892 - ETA: 11s - loss: 0.3623 - accuracy: 0.887 - ETA: 11s - loss: 0.3605 - accuracy: 0.887 - ETA: 11s - loss: 0.3655 - accuracy: 0.886 - ETA: 10s - loss: 0.3726 - accuracy: 0.885 - ETA: 10s - loss: 0.3760 - accuracy: 0.884 - ETA: 10s - loss: 0.3831 - accuracy: 0.883 - ETA: 10s - loss: 0.3847 - accuracy: 0.882 - ETA: 10s - loss: 0.3870 - accuracy: 0.881 - ETA: 10s - loss: 0.3856 - accuracy: 0.881 - ETA: 10s - loss: 0.3863 - accuracy: 0.880 - ETA: 10s - loss: 0.3830 - accuracy: 0.881 - ETA: 10s - loss: 0.3812 - accuracy: 0.882 - ETA: 9s - loss: 0.3842 - accuracy: 0.881 - ETA: 9s - loss: 0.3892 - accuracy: 0.88 - ETA: 9s - loss: 0.3884 - accuracy: 0.87 - ETA: 9s - loss: 0.3905 - accuracy: 0.87 - ETA: 9s - loss: 0.3909 - accuracy: 0.87 - ETA: 9s - loss: 0.3910 - accuracy: 0.87 - ETA: 8s - loss: 0.3927 - accuracy: 0.87 - ETA: 8s - loss: 0.3924 - accuracy: 0.87 - ETA: 8s - loss: 0.3917 - accuracy: 0.87 - ETA: 8s - loss: 0.3921 - accuracy: 0.87 - ETA: 8s - loss: 0.3925 - accuracy: 0.87 - ETA: 8s - loss: 0.3921 - accuracy: 0.87 - ETA: 8s - loss: 0.3933 - accuracy: 0.87 - ETA: 7s - loss: 0.3921 - accuracy: 0.87 - ETA: 7s - loss: 0.3928 - accuracy: 0.87 - ETA: 7s - loss: 0.3937 - accuracy: 0.87 - ETA: 7s - loss: 0.3955 - accuracy: 0.87 - ETA: 7s - loss: 0.3956 - accuracy: 0.87 - ETA: 7s - loss: 0.3965 - accuracy: 0.87 - ETA: 7s - loss: 0.3952 - accuracy: 0.87 - ETA: 6s - loss: 0.3961 - accuracy: 0.87 - ETA: 6s - loss: 0.3942 - accuracy: 0.87 - ETA: 6s - loss: 0.3946 - accuracy: 0.87 - ETA: 6s - loss: 0.3944 - accuracy: 0.87 - ETA: 6s - loss: 0.3934 - accuracy: 0.87 - ETA: 6s - loss: 0.3941 - accuracy: 0.87 - ETA: 6s - loss: 0.3957 - accuracy: 0.87 - ETA: 5s - loss: 0.3960 - accuracy: 0.87 - ETA: 5s - loss: 0.3965 - accuracy: 0.87 - ETA: 5s - loss: 0.3961 - accuracy: 0.87 - ETA: 5s - loss: 0.3950 - accuracy: 0.87 - ETA: 5s - loss: 0.3952 - accuracy: 0.87 - ETA: 5s - loss: 0.3962 - accuracy: 0.87 - ETA: 5s - loss: 0.3964 - accuracy: 0.87 - ETA: 4s - loss: 0.3965 - accuracy: 0.87 - ETA: 4s - loss: 0.3963 - accuracy: 0.87 - ETA: 4s - loss: 0.3964 - accuracy: 0.87 - ETA: 4s - loss: 0.3962 - accuracy: 0.87 - ETA: 4s - loss: 0.3958 - accuracy: 0.87 - ETA: 4s - loss: 0.3959 - accuracy: 0.87 - ETA: 4s - loss: 0.3960 - accuracy: 0.87 - ETA: 3s - loss: 0.3961 - accuracy: 0.87 - ETA: 3s - loss: 0.3964 - accuracy: 0.87 - ETA: 3s - loss: 0.3963 - accuracy: 0.87 - ETA: 3s - loss: 0.3959 - accuracy: 0.87 - ETA: 3s - loss: 0.3966 - accuracy: 0.87 - ETA: 3s - loss: 0.3983 - accuracy: 0.87 - ETA: 3s - loss: 0.4001 - accuracy: 0.87 - ETA: 2s - loss: 0.4007 - accuracy: 0.87 - ETA: 2s - loss: 0.4002 - accuracy: 0.87 - ETA: 2s - loss: 0.4000 - accuracy: 0.87 - ETA: 2s - loss: 0.4003 - accuracy: 0.87 - ETA: 2s - loss: 0.4007 - accuracy: 0.87 - ETA: 2s - loss: 0.4010 - accuracy: 0.87 - ETA: 2s - loss: 0.4010 - accuracy: 0.87 - ETA: 1s - loss: 0.4001 - accuracy: 0.87 - ETA: 1s - loss: 0.3994 - accuracy: 0.87 - ETA: 1s - loss: 0.4000 - accuracy: 0.87 - ETA: 1s - loss: 0.4005 - accuracy: 0.87 - ETA: 1s - loss: 0.4005 - accuracy: 0.87 - ETA: 1s - loss: 0.4004 - accuracy: 0.87 - ETA: 1s - loss: 0.4007 - accuracy: 0.87 - ETA: 0s - loss: 0.4012 - accuracy: 0.87 - ETA: 0s - loss: 0.4018 - accuracy: 0.87 - ETA: 0s - loss: 0.4018 - accuracy: 0.87 - ETA: 0s - loss: 0.4020 - accuracy: 0.87 - ETA: 0s - loss: 0.4025 - accuracy: 0.87 - ETA: 0s - loss: 0.4024 - accuracy: 0.87 - ETA: 0s - loss: 0.4023 - accuracy: 0.8755\n",
      "Epoch 00017: val_accuracy improved from 0.89990 to 0.90091, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 151us/sample - loss: 0.4016 - accuracy: 0.8757 - val_loss: 0.3358 - val_accuracy: 0.9009\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.4040 - accuracy: 0.878 - ETA: 11s - loss: 0.4015 - accuracy: 0.880 - ETA: 11s - loss: 0.4048 - accuracy: 0.875 - ETA: 11s - loss: 0.4048 - accuracy: 0.875 - ETA: 11s - loss: 0.4031 - accuracy: 0.875 - ETA: 11s - loss: 0.4154 - accuracy: 0.871 - ETA: 10s - loss: 0.4009 - accuracy: 0.874 - ETA: 10s - loss: 0.3955 - accuracy: 0.874 - ETA: 10s - loss: 0.3958 - accuracy: 0.873 - ETA: 10s - loss: 0.3933 - accuracy: 0.874 - ETA: 10s - loss: 0.3900 - accuracy: 0.875 - ETA: 10s - loss: 0.3921 - accuracy: 0.874 - ETA: 10s - loss: 0.3948 - accuracy: 0.873 - ETA: 9s - loss: 0.3993 - accuracy: 0.873 - ETA: 9s - loss: 0.3971 - accuracy: 0.87 - ETA: 9s - loss: 0.3957 - accuracy: 0.87 - ETA: 9s - loss: 0.3967 - accuracy: 0.87 - ETA: 9s - loss: 0.3977 - accuracy: 0.87 - ETA: 9s - loss: 0.3973 - accuracy: 0.87 - ETA: 9s - loss: 0.3981 - accuracy: 0.87 - ETA: 8s - loss: 0.3977 - accuracy: 0.87 - ETA: 8s - loss: 0.3965 - accuracy: 0.87 - ETA: 8s - loss: 0.3988 - accuracy: 0.87 - ETA: 8s - loss: 0.3985 - accuracy: 0.87 - ETA: 8s - loss: 0.3979 - accuracy: 0.87 - ETA: 8s - loss: 0.3975 - accuracy: 0.87 - ETA: 8s - loss: 0.3980 - accuracy: 0.87 - ETA: 7s - loss: 0.3969 - accuracy: 0.87 - ETA: 7s - loss: 0.3973 - accuracy: 0.87 - ETA: 7s - loss: 0.3955 - accuracy: 0.87 - ETA: 7s - loss: 0.3954 - accuracy: 0.87 - ETA: 7s - loss: 0.3957 - accuracy: 0.87 - ETA: 7s - loss: 0.3952 - accuracy: 0.87 - ETA: 7s - loss: 0.3950 - accuracy: 0.87 - ETA: 6s - loss: 0.3961 - accuracy: 0.87 - ETA: 6s - loss: 0.3974 - accuracy: 0.87 - ETA: 6s - loss: 0.3965 - accuracy: 0.87 - ETA: 6s - loss: 0.3953 - accuracy: 0.87 - ETA: 6s - loss: 0.3944 - accuracy: 0.87 - ETA: 6s - loss: 0.3937 - accuracy: 0.87 - ETA: 6s - loss: 0.3927 - accuracy: 0.87 - ETA: 6s - loss: 0.3931 - accuracy: 0.87 - ETA: 5s - loss: 0.3937 - accuracy: 0.87 - ETA: 5s - loss: 0.3942 - accuracy: 0.87 - ETA: 5s - loss: 0.3946 - accuracy: 0.87 - ETA: 5s - loss: 0.3944 - accuracy: 0.87 - ETA: 5s - loss: 0.3951 - accuracy: 0.87 - ETA: 5s - loss: 0.3953 - accuracy: 0.87 - ETA: 5s - loss: 0.3956 - accuracy: 0.87 - ETA: 4s - loss: 0.3971 - accuracy: 0.87 - ETA: 4s - loss: 0.3966 - accuracy: 0.87 - ETA: 4s - loss: 0.3965 - accuracy: 0.87 - ETA: 4s - loss: 0.3963 - accuracy: 0.87 - ETA: 4s - loss: 0.3953 - accuracy: 0.87 - ETA: 4s - loss: 0.3947 - accuracy: 0.87 - ETA: 4s - loss: 0.3947 - accuracy: 0.87 - ETA: 3s - loss: 0.3944 - accuracy: 0.87 - ETA: 3s - loss: 0.3938 - accuracy: 0.87 - ETA: 3s - loss: 0.3946 - accuracy: 0.87 - ETA: 3s - loss: 0.3947 - accuracy: 0.87 - ETA: 3s - loss: 0.3949 - accuracy: 0.87 - ETA: 3s - loss: 0.3949 - accuracy: 0.87 - ETA: 3s - loss: 0.3942 - accuracy: 0.87 - ETA: 2s - loss: 0.3945 - accuracy: 0.87 - ETA: 2s - loss: 0.3946 - accuracy: 0.87 - ETA: 2s - loss: 0.3947 - accuracy: 0.87 - ETA: 2s - loss: 0.3950 - accuracy: 0.87 - ETA: 2s - loss: 0.3956 - accuracy: 0.87 - ETA: 2s - loss: 0.3945 - accuracy: 0.87 - ETA: 2s - loss: 0.3942 - accuracy: 0.87 - ETA: 1s - loss: 0.3935 - accuracy: 0.87 - ETA: 1s - loss: 0.3931 - accuracy: 0.87 - ETA: 1s - loss: 0.3938 - accuracy: 0.87 - ETA: 1s - loss: 0.3945 - accuracy: 0.87 - ETA: 1s - loss: 0.3949 - accuracy: 0.87 - ETA: 1s - loss: 0.3954 - accuracy: 0.87 - ETA: 1s - loss: 0.3964 - accuracy: 0.87 - ETA: 0s - loss: 0.3959 - accuracy: 0.87 - ETA: 0s - loss: 0.3957 - accuracy: 0.87 - ETA: 0s - loss: 0.3959 - accuracy: 0.87 - ETA: 0s - loss: 0.3950 - accuracy: 0.87 - ETA: 0s - loss: 0.3950 - accuracy: 0.87 - ETA: 0s - loss: 0.3950 - accuracy: 0.87 - ETA: 0s - loss: 0.3948 - accuracy: 0.8767\n",
      "Epoch 00018: val_accuracy did not improve from 0.90091\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3952 - accuracy: 0.8766 - val_loss: 0.3420 - val_accuracy: 0.8988\n",
      "Epoch 19/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3382 - accuracy: 0.900 - ETA: 11s - loss: 0.3742 - accuracy: 0.888 - ETA: 11s - loss: 0.3678 - accuracy: 0.885 - ETA: 11s - loss: 0.3578 - accuracy: 0.886 - ETA: 11s - loss: 0.3647 - accuracy: 0.883 - ETA: 11s - loss: 0.3735 - accuracy: 0.881 - ETA: 10s - loss: 0.3743 - accuracy: 0.881 - ETA: 10s - loss: 0.3821 - accuracy: 0.879 - ETA: 10s - loss: 0.3805 - accuracy: 0.879 - ETA: 10s - loss: 0.3795 - accuracy: 0.880 - ETA: 10s - loss: 0.3771 - accuracy: 0.881 - ETA: 10s - loss: 0.3771 - accuracy: 0.881 - ETA: 10s - loss: 0.3749 - accuracy: 0.881 - ETA: 9s - loss: 0.3757 - accuracy: 0.881 - ETA: 9s - loss: 0.3756 - accuracy: 0.88 - ETA: 9s - loss: 0.3770 - accuracy: 0.88 - ETA: 9s - loss: 0.3763 - accuracy: 0.88 - ETA: 9s - loss: 0.3772 - accuracy: 0.88 - ETA: 9s - loss: 0.3763 - accuracy: 0.88 - ETA: 9s - loss: 0.3753 - accuracy: 0.88 - ETA: 8s - loss: 0.3742 - accuracy: 0.88 - ETA: 8s - loss: 0.3738 - accuracy: 0.88 - ETA: 8s - loss: 0.3745 - accuracy: 0.88 - ETA: 8s - loss: 0.3733 - accuracy: 0.88 - ETA: 8s - loss: 0.3771 - accuracy: 0.88 - ETA: 8s - loss: 0.3783 - accuracy: 0.88 - ETA: 8s - loss: 0.3778 - accuracy: 0.88 - ETA: 7s - loss: 0.3793 - accuracy: 0.88 - ETA: 7s - loss: 0.3802 - accuracy: 0.87 - ETA: 7s - loss: 0.3798 - accuracy: 0.88 - ETA: 7s - loss: 0.3783 - accuracy: 0.88 - ETA: 7s - loss: 0.3796 - accuracy: 0.88 - ETA: 7s - loss: 0.3808 - accuracy: 0.88 - ETA: 7s - loss: 0.3800 - accuracy: 0.88 - ETA: 6s - loss: 0.3815 - accuracy: 0.88 - ETA: 6s - loss: 0.3825 - accuracy: 0.88 - ETA: 6s - loss: 0.3818 - accuracy: 0.88 - ETA: 6s - loss: 0.3823 - accuracy: 0.88 - ETA: 6s - loss: 0.3822 - accuracy: 0.88 - ETA: 6s - loss: 0.3838 - accuracy: 0.87 - ETA: 6s - loss: 0.3837 - accuracy: 0.87 - ETA: 6s - loss: 0.3838 - accuracy: 0.87 - ETA: 5s - loss: 0.3845 - accuracy: 0.87 - ETA: 5s - loss: 0.3840 - accuracy: 0.87 - ETA: 5s - loss: 0.3846 - accuracy: 0.87 - ETA: 5s - loss: 0.3835 - accuracy: 0.87 - ETA: 5s - loss: 0.3832 - accuracy: 0.88 - ETA: 5s - loss: 0.3832 - accuracy: 0.87 - ETA: 5s - loss: 0.3824 - accuracy: 0.88 - ETA: 4s - loss: 0.3829 - accuracy: 0.87 - ETA: 4s - loss: 0.3833 - accuracy: 0.87 - ETA: 4s - loss: 0.3824 - accuracy: 0.88 - ETA: 4s - loss: 0.3825 - accuracy: 0.88 - ETA: 4s - loss: 0.3823 - accuracy: 0.88 - ETA: 4s - loss: 0.3820 - accuracy: 0.88 - ETA: 4s - loss: 0.3816 - accuracy: 0.88 - ETA: 3s - loss: 0.3809 - accuracy: 0.88 - ETA: 3s - loss: 0.3811 - accuracy: 0.88 - ETA: 3s - loss: 0.3809 - accuracy: 0.88 - ETA: 3s - loss: 0.3814 - accuracy: 0.88 - ETA: 3s - loss: 0.3815 - accuracy: 0.87 - ETA: 3s - loss: 0.3822 - accuracy: 0.87 - ETA: 3s - loss: 0.3823 - accuracy: 0.87 - ETA: 2s - loss: 0.3825 - accuracy: 0.87 - ETA: 2s - loss: 0.3831 - accuracy: 0.87 - ETA: 2s - loss: 0.3831 - accuracy: 0.87 - ETA: 2s - loss: 0.3824 - accuracy: 0.87 - ETA: 2s - loss: 0.3821 - accuracy: 0.87 - ETA: 2s - loss: 0.3819 - accuracy: 0.87 - ETA: 2s - loss: 0.3823 - accuracy: 0.87 - ETA: 1s - loss: 0.3824 - accuracy: 0.87 - ETA: 1s - loss: 0.3824 - accuracy: 0.87 - ETA: 1s - loss: 0.3821 - accuracy: 0.87 - ETA: 1s - loss: 0.3819 - accuracy: 0.87 - ETA: 1s - loss: 0.3816 - accuracy: 0.87 - ETA: 1s - loss: 0.3809 - accuracy: 0.87 - ETA: 1s - loss: 0.3803 - accuracy: 0.87 - ETA: 0s - loss: 0.3804 - accuracy: 0.87 - ETA: 0s - loss: 0.3807 - accuracy: 0.87 - ETA: 0s - loss: 0.3812 - accuracy: 0.87 - ETA: 0s - loss: 0.3814 - accuracy: 0.87 - ETA: 0s - loss: 0.3812 - accuracy: 0.87 - ETA: 0s - loss: 0.3816 - accuracy: 0.87 - ETA: 0s - loss: 0.3823 - accuracy: 0.8795\n",
      "Epoch 00019: val_accuracy did not improve from 0.90091\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3826 - accuracy: 0.8794 - val_loss: 0.3398 - val_accuracy: 0.8997\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3655 - accuracy: 0.881 - ETA: 11s - loss: 0.3659 - accuracy: 0.879 - ETA: 11s - loss: 0.3684 - accuracy: 0.881 - ETA: 11s - loss: 0.3676 - accuracy: 0.880 - ETA: 11s - loss: 0.3640 - accuracy: 0.880 - ETA: 11s - loss: 0.3669 - accuracy: 0.881 - ETA: 10s - loss: 0.3635 - accuracy: 0.882 - ETA: 10s - loss: 0.3591 - accuracy: 0.884 - ETA: 10s - loss: 0.3616 - accuracy: 0.883 - ETA: 10s - loss: 0.3672 - accuracy: 0.882 - ETA: 10s - loss: 0.3683 - accuracy: 0.883 - ETA: 10s - loss: 0.3681 - accuracy: 0.883 - ETA: 10s - loss: 0.3694 - accuracy: 0.884 - ETA: 10s - loss: 0.3739 - accuracy: 0.882 - ETA: 9s - loss: 0.3785 - accuracy: 0.880 - ETA: 9s - loss: 0.3782 - accuracy: 0.88 - ETA: 9s - loss: 0.3777 - accuracy: 0.88 - ETA: 9s - loss: 0.3741 - accuracy: 0.88 - ETA: 9s - loss: 0.3700 - accuracy: 0.88 - ETA: 9s - loss: 0.3668 - accuracy: 0.88 - ETA: 9s - loss: 0.3683 - accuracy: 0.88 - ETA: 8s - loss: 0.3685 - accuracy: 0.88 - ETA: 8s - loss: 0.3692 - accuracy: 0.88 - ETA: 8s - loss: 0.3700 - accuracy: 0.88 - ETA: 8s - loss: 0.3713 - accuracy: 0.88 - ETA: 8s - loss: 0.3708 - accuracy: 0.88 - ETA: 8s - loss: 0.3715 - accuracy: 0.88 - ETA: 8s - loss: 0.3710 - accuracy: 0.88 - ETA: 7s - loss: 0.3714 - accuracy: 0.88 - ETA: 7s - loss: 0.3721 - accuracy: 0.88 - ETA: 7s - loss: 0.3759 - accuracy: 0.88 - ETA: 7s - loss: 0.3755 - accuracy: 0.88 - ETA: 7s - loss: 0.3759 - accuracy: 0.88 - ETA: 7s - loss: 0.3755 - accuracy: 0.88 - ETA: 7s - loss: 0.3739 - accuracy: 0.88 - ETA: 6s - loss: 0.3744 - accuracy: 0.88 - ETA: 6s - loss: 0.3752 - accuracy: 0.88 - ETA: 6s - loss: 0.3753 - accuracy: 0.88 - ETA: 6s - loss: 0.3749 - accuracy: 0.88 - ETA: 6s - loss: 0.3752 - accuracy: 0.88 - ETA: 6s - loss: 0.3742 - accuracy: 0.88 - ETA: 6s - loss: 0.3750 - accuracy: 0.88 - ETA: 5s - loss: 0.3750 - accuracy: 0.88 - ETA: 5s - loss: 0.3755 - accuracy: 0.88 - ETA: 5s - loss: 0.3763 - accuracy: 0.88 - ETA: 5s - loss: 0.3752 - accuracy: 0.88 - ETA: 5s - loss: 0.3746 - accuracy: 0.88 - ETA: 5s - loss: 0.3757 - accuracy: 0.88 - ETA: 5s - loss: 0.3745 - accuracy: 0.88 - ETA: 4s - loss: 0.3736 - accuracy: 0.88 - ETA: 4s - loss: 0.3743 - accuracy: 0.88 - ETA: 4s - loss: 0.3738 - accuracy: 0.88 - ETA: 4s - loss: 0.3747 - accuracy: 0.88 - ETA: 4s - loss: 0.3747 - accuracy: 0.88 - ETA: 4s - loss: 0.3752 - accuracy: 0.88 - ETA: 4s - loss: 0.3747 - accuracy: 0.88 - ETA: 3s - loss: 0.3745 - accuracy: 0.88 - ETA: 3s - loss: 0.3752 - accuracy: 0.88 - ETA: 3s - loss: 0.3754 - accuracy: 0.88 - ETA: 3s - loss: 0.3757 - accuracy: 0.88 - ETA: 3s - loss: 0.3758 - accuracy: 0.88 - ETA: 3s - loss: 0.3755 - accuracy: 0.88 - ETA: 3s - loss: 0.3761 - accuracy: 0.88 - ETA: 2s - loss: 0.3766 - accuracy: 0.88 - ETA: 2s - loss: 0.3766 - accuracy: 0.88 - ETA: 2s - loss: 0.3769 - accuracy: 0.88 - ETA: 2s - loss: 0.3773 - accuracy: 0.88 - ETA: 2s - loss: 0.3769 - accuracy: 0.88 - ETA: 2s - loss: 0.3769 - accuracy: 0.88 - ETA: 2s - loss: 0.3769 - accuracy: 0.88 - ETA: 1s - loss: 0.3770 - accuracy: 0.88 - ETA: 1s - loss: 0.3770 - accuracy: 0.88 - ETA: 1s - loss: 0.3770 - accuracy: 0.88 - ETA: 1s - loss: 0.3772 - accuracy: 0.88 - ETA: 1s - loss: 0.3773 - accuracy: 0.88 - ETA: 1s - loss: 0.3773 - accuracy: 0.88 - ETA: 1s - loss: 0.3774 - accuracy: 0.88 - ETA: 0s - loss: 0.3765 - accuracy: 0.88 - ETA: 0s - loss: 0.3765 - accuracy: 0.88 - ETA: 0s - loss: 0.3764 - accuracy: 0.88 - ETA: 0s - loss: 0.3768 - accuracy: 0.88 - ETA: 0s - loss: 0.3770 - accuracy: 0.88 - ETA: 0s - loss: 0.3771 - accuracy: 0.88 - ETA: 0s - loss: 0.3771 - accuracy: 0.8816\n",
      "Epoch 00020: val_accuracy improved from 0.90091 to 0.90433, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3770 - accuracy: 0.8817 - val_loss: 0.3245 - val_accuracy: 0.9043\n",
      "Epoch 21/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3238 - accuracy: 0.896 - ETA: 11s - loss: 0.3376 - accuracy: 0.893 - ETA: 11s - loss: 0.3349 - accuracy: 0.896 - ETA: 11s - loss: 0.3302 - accuracy: 0.897 - ETA: 11s - loss: 0.3288 - accuracy: 0.895 - ETA: 11s - loss: 0.3308 - accuracy: 0.894 - ETA: 10s - loss: 0.3408 - accuracy: 0.891 - ETA: 10s - loss: 0.3401 - accuracy: 0.891 - ETA: 10s - loss: 0.3405 - accuracy: 0.891 - ETA: 10s - loss: 0.3441 - accuracy: 0.890 - ETA: 10s - loss: 0.3448 - accuracy: 0.888 - ETA: 10s - loss: 0.3457 - accuracy: 0.888 - ETA: 10s - loss: 0.3446 - accuracy: 0.888 - ETA: 9s - loss: 0.3446 - accuracy: 0.888 - ETA: 9s - loss: 0.3437 - accuracy: 0.88 - ETA: 9s - loss: 0.3428 - accuracy: 0.88 - ETA: 9s - loss: 0.3429 - accuracy: 0.88 - ETA: 9s - loss: 0.3442 - accuracy: 0.88 - ETA: 9s - loss: 0.3443 - accuracy: 0.88 - ETA: 9s - loss: 0.3429 - accuracy: 0.88 - ETA: 9s - loss: 0.3429 - accuracy: 0.88 - ETA: 8s - loss: 0.3452 - accuracy: 0.88 - ETA: 8s - loss: 0.3471 - accuracy: 0.88 - ETA: 8s - loss: 0.3493 - accuracy: 0.88 - ETA: 8s - loss: 0.3508 - accuracy: 0.88 - ETA: 8s - loss: 0.3521 - accuracy: 0.88 - ETA: 8s - loss: 0.3518 - accuracy: 0.88 - ETA: 8s - loss: 0.3523 - accuracy: 0.88 - ETA: 7s - loss: 0.3538 - accuracy: 0.88 - ETA: 7s - loss: 0.3556 - accuracy: 0.88 - ETA: 7s - loss: 0.3569 - accuracy: 0.88 - ETA: 7s - loss: 0.3567 - accuracy: 0.88 - ETA: 7s - loss: 0.3565 - accuracy: 0.88 - ETA: 7s - loss: 0.3558 - accuracy: 0.88 - ETA: 7s - loss: 0.3559 - accuracy: 0.88 - ETA: 6s - loss: 0.3563 - accuracy: 0.88 - ETA: 6s - loss: 0.3565 - accuracy: 0.88 - ETA: 6s - loss: 0.3588 - accuracy: 0.88 - ETA: 6s - loss: 0.3585 - accuracy: 0.88 - ETA: 6s - loss: 0.3593 - accuracy: 0.88 - ETA: 6s - loss: 0.3593 - accuracy: 0.88 - ETA: 6s - loss: 0.3594 - accuracy: 0.88 - ETA: 5s - loss: 0.3597 - accuracy: 0.88 - ETA: 5s - loss: 0.3588 - accuracy: 0.88 - ETA: 5s - loss: 0.3586 - accuracy: 0.88 - ETA: 5s - loss: 0.3592 - accuracy: 0.88 - ETA: 5s - loss: 0.3602 - accuracy: 0.88 - ETA: 5s - loss: 0.3617 - accuracy: 0.88 - ETA: 5s - loss: 0.3621 - accuracy: 0.88 - ETA: 4s - loss: 0.3635 - accuracy: 0.88 - ETA: 4s - loss: 0.3649 - accuracy: 0.88 - ETA: 4s - loss: 0.3645 - accuracy: 0.88 - ETA: 4s - loss: 0.3645 - accuracy: 0.88 - ETA: 4s - loss: 0.3640 - accuracy: 0.88 - ETA: 4s - loss: 0.3633 - accuracy: 0.88 - ETA: 4s - loss: 0.3640 - accuracy: 0.88 - ETA: 3s - loss: 0.3631 - accuracy: 0.88 - ETA: 3s - loss: 0.3627 - accuracy: 0.88 - ETA: 3s - loss: 0.3624 - accuracy: 0.88 - ETA: 3s - loss: 0.3621 - accuracy: 0.88 - ETA: 3s - loss: 0.3620 - accuracy: 0.88 - ETA: 3s - loss: 0.3621 - accuracy: 0.88 - ETA: 3s - loss: 0.3619 - accuracy: 0.88 - ETA: 2s - loss: 0.3619 - accuracy: 0.88 - ETA: 2s - loss: 0.3617 - accuracy: 0.88 - ETA: 2s - loss: 0.3621 - accuracy: 0.88 - ETA: 2s - loss: 0.3620 - accuracy: 0.88 - ETA: 2s - loss: 0.3627 - accuracy: 0.88 - ETA: 2s - loss: 0.3629 - accuracy: 0.88 - ETA: 2s - loss: 0.3628 - accuracy: 0.88 - ETA: 1s - loss: 0.3621 - accuracy: 0.88 - ETA: 1s - loss: 0.3617 - accuracy: 0.88 - ETA: 1s - loss: 0.3616 - accuracy: 0.88 - ETA: 1s - loss: 0.3619 - accuracy: 0.88 - ETA: 1s - loss: 0.3627 - accuracy: 0.88 - ETA: 1s - loss: 0.3638 - accuracy: 0.88 - ETA: 1s - loss: 0.3641 - accuracy: 0.88 - ETA: 0s - loss: 0.3639 - accuracy: 0.88 - ETA: 0s - loss: 0.3641 - accuracy: 0.88 - ETA: 0s - loss: 0.3636 - accuracy: 0.88 - ETA: 0s - loss: 0.3636 - accuracy: 0.88 - ETA: 0s - loss: 0.3635 - accuracy: 0.88 - ETA: 0s - loss: 0.3638 - accuracy: 0.88 - ETA: 0s - loss: 0.3634 - accuracy: 0.8851\n",
      "Epoch 00021: val_accuracy did not improve from 0.90433\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3642 - accuracy: 0.8849 - val_loss: 0.3434 - val_accuracy: 0.9019\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 12s - loss: 0.3802 - accuracy: 0.873 - ETA: 12s - loss: 0.3811 - accuracy: 0.874 - ETA: 11s - loss: 0.3509 - accuracy: 0.887 - ETA: 11s - loss: 0.3408 - accuracy: 0.890 - ETA: 11s - loss: 0.3390 - accuracy: 0.891 - ETA: 11s - loss: 0.3474 - accuracy: 0.889 - ETA: 11s - loss: 0.3473 - accuracy: 0.888 - ETA: 10s - loss: 0.3511 - accuracy: 0.887 - ETA: 10s - loss: 0.3558 - accuracy: 0.886 - ETA: 10s - loss: 0.3570 - accuracy: 0.887 - ETA: 10s - loss: 0.3548 - accuracy: 0.887 - ETA: 10s - loss: 0.3549 - accuracy: 0.888 - ETA: 10s - loss: 0.3533 - accuracy: 0.888 - ETA: 10s - loss: 0.3526 - accuracy: 0.889 - ETA: 9s - loss: 0.3509 - accuracy: 0.889 - ETA: 9s - loss: 0.3475 - accuracy: 0.89 - ETA: 9s - loss: 0.3470 - accuracy: 0.89 - ETA: 9s - loss: 0.3460 - accuracy: 0.89 - ETA: 9s - loss: 0.3471 - accuracy: 0.89 - ETA: 9s - loss: 0.3502 - accuracy: 0.88 - ETA: 9s - loss: 0.3497 - accuracy: 0.88 - ETA: 8s - loss: 0.3495 - accuracy: 0.89 - ETA: 8s - loss: 0.3490 - accuracy: 0.89 - ETA: 8s - loss: 0.3499 - accuracy: 0.89 - ETA: 8s - loss: 0.3508 - accuracy: 0.89 - ETA: 8s - loss: 0.3493 - accuracy: 0.89 - ETA: 8s - loss: 0.3513 - accuracy: 0.89 - ETA: 8s - loss: 0.3526 - accuracy: 0.88 - ETA: 7s - loss: 0.3526 - accuracy: 0.88 - ETA: 7s - loss: 0.3531 - accuracy: 0.88 - ETA: 7s - loss: 0.3534 - accuracy: 0.88 - ETA: 7s - loss: 0.3552 - accuracy: 0.88 - ETA: 7s - loss: 0.3535 - accuracy: 0.88 - ETA: 7s - loss: 0.3530 - accuracy: 0.88 - ETA: 7s - loss: 0.3541 - accuracy: 0.88 - ETA: 6s - loss: 0.3543 - accuracy: 0.88 - ETA: 6s - loss: 0.3519 - accuracy: 0.88 - ETA: 6s - loss: 0.3506 - accuracy: 0.89 - ETA: 6s - loss: 0.3502 - accuracy: 0.89 - ETA: 6s - loss: 0.3503 - accuracy: 0.89 - ETA: 6s - loss: 0.3504 - accuracy: 0.89 - ETA: 6s - loss: 0.3492 - accuracy: 0.89 - ETA: 5s - loss: 0.3502 - accuracy: 0.89 - ETA: 5s - loss: 0.3519 - accuracy: 0.89 - ETA: 5s - loss: 0.3522 - accuracy: 0.88 - ETA: 5s - loss: 0.3512 - accuracy: 0.89 - ETA: 5s - loss: 0.3510 - accuracy: 0.89 - ETA: 5s - loss: 0.3517 - accuracy: 0.89 - ETA: 5s - loss: 0.3518 - accuracy: 0.89 - ETA: 4s - loss: 0.3522 - accuracy: 0.89 - ETA: 4s - loss: 0.3520 - accuracy: 0.89 - ETA: 4s - loss: 0.3529 - accuracy: 0.88 - ETA: 4s - loss: 0.3539 - accuracy: 0.88 - ETA: 4s - loss: 0.3546 - accuracy: 0.88 - ETA: 4s - loss: 0.3536 - accuracy: 0.88 - ETA: 4s - loss: 0.3525 - accuracy: 0.88 - ETA: 3s - loss: 0.3518 - accuracy: 0.88 - ETA: 3s - loss: 0.3525 - accuracy: 0.88 - ETA: 3s - loss: 0.3523 - accuracy: 0.88 - ETA: 3s - loss: 0.3531 - accuracy: 0.88 - ETA: 3s - loss: 0.3538 - accuracy: 0.88 - ETA: 3s - loss: 0.3537 - accuracy: 0.88 - ETA: 3s - loss: 0.3535 - accuracy: 0.88 - ETA: 2s - loss: 0.3522 - accuracy: 0.89 - ETA: 2s - loss: 0.3517 - accuracy: 0.89 - ETA: 2s - loss: 0.3511 - accuracy: 0.89 - ETA: 2s - loss: 0.3511 - accuracy: 0.89 - ETA: 2s - loss: 0.3519 - accuracy: 0.88 - ETA: 2s - loss: 0.3519 - accuracy: 0.89 - ETA: 2s - loss: 0.3525 - accuracy: 0.88 - ETA: 1s - loss: 0.3532 - accuracy: 0.88 - ETA: 1s - loss: 0.3532 - accuracy: 0.88 - ETA: 1s - loss: 0.3538 - accuracy: 0.88 - ETA: 1s - loss: 0.3537 - accuracy: 0.88 - ETA: 1s - loss: 0.3538 - accuracy: 0.88 - ETA: 1s - loss: 0.3537 - accuracy: 0.88 - ETA: 1s - loss: 0.3543 - accuracy: 0.88 - ETA: 0s - loss: 0.3548 - accuracy: 0.88 - ETA: 0s - loss: 0.3557 - accuracy: 0.88 - ETA: 0s - loss: 0.3564 - accuracy: 0.88 - ETA: 0s - loss: 0.3564 - accuracy: 0.88 - ETA: 0s - loss: 0.3562 - accuracy: 0.88 - ETA: 0s - loss: 0.3561 - accuracy: 0.88 - ETA: 0s - loss: 0.3561 - accuracy: 0.8891\n",
      "Epoch 00022: val_accuracy did not improve from 0.90433\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3561 - accuracy: 0.8890 - val_loss: 0.3311 - val_accuracy: 0.9017\n",
      "Epoch 23/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3723 - accuracy: 0.874 - ETA: 11s - loss: 0.3722 - accuracy: 0.878 - ETA: 11s - loss: 0.3634 - accuracy: 0.879 - ETA: 11s - loss: 0.3493 - accuracy: 0.885 - ETA: 11s - loss: 0.3396 - accuracy: 0.890 - ETA: 11s - loss: 0.3378 - accuracy: 0.891 - ETA: 10s - loss: 0.3404 - accuracy: 0.891 - ETA: 10s - loss: 0.3361 - accuracy: 0.892 - ETA: 10s - loss: 0.3373 - accuracy: 0.893 - ETA: 10s - loss: 0.3378 - accuracy: 0.892 - ETA: 10s - loss: 0.3390 - accuracy: 0.892 - ETA: 10s - loss: 0.3444 - accuracy: 0.891 - ETA: 10s - loss: 0.3448 - accuracy: 0.891 - ETA: 9s - loss: 0.3478 - accuracy: 0.891 - ETA: 9s - loss: 0.3461 - accuracy: 0.89 - ETA: 9s - loss: 0.3488 - accuracy: 0.89 - ETA: 9s - loss: 0.3522 - accuracy: 0.88 - ETA: 9s - loss: 0.3540 - accuracy: 0.88 - ETA: 9s - loss: 0.3530 - accuracy: 0.88 - ETA: 9s - loss: 0.3510 - accuracy: 0.88 - ETA: 8s - loss: 0.3504 - accuracy: 0.88 - ETA: 8s - loss: 0.3476 - accuracy: 0.89 - ETA: 8s - loss: 0.3464 - accuracy: 0.89 - ETA: 8s - loss: 0.3462 - accuracy: 0.89 - ETA: 8s - loss: 0.3456 - accuracy: 0.89 - ETA: 8s - loss: 0.3446 - accuracy: 0.89 - ETA: 8s - loss: 0.3437 - accuracy: 0.89 - ETA: 7s - loss: 0.3434 - accuracy: 0.89 - ETA: 7s - loss: 0.3429 - accuracy: 0.89 - ETA: 7s - loss: 0.3436 - accuracy: 0.89 - ETA: 7s - loss: 0.3452 - accuracy: 0.89 - ETA: 7s - loss: 0.3482 - accuracy: 0.88 - ETA: 7s - loss: 0.3482 - accuracy: 0.89 - ETA: 7s - loss: 0.3487 - accuracy: 0.89 - ETA: 7s - loss: 0.3483 - accuracy: 0.89 - ETA: 6s - loss: 0.3496 - accuracy: 0.89 - ETA: 6s - loss: 0.3498 - accuracy: 0.88 - ETA: 6s - loss: 0.3492 - accuracy: 0.88 - ETA: 6s - loss: 0.3483 - accuracy: 0.89 - ETA: 6s - loss: 0.3480 - accuracy: 0.89 - ETA: 6s - loss: 0.3480 - accuracy: 0.89 - ETA: 6s - loss: 0.3477 - accuracy: 0.89 - ETA: 5s - loss: 0.3483 - accuracy: 0.88 - ETA: 5s - loss: 0.3486 - accuracy: 0.88 - ETA: 5s - loss: 0.3485 - accuracy: 0.88 - ETA: 5s - loss: 0.3484 - accuracy: 0.88 - ETA: 5s - loss: 0.3478 - accuracy: 0.88 - ETA: 5s - loss: 0.3479 - accuracy: 0.88 - ETA: 5s - loss: 0.3472 - accuracy: 0.88 - ETA: 4s - loss: 0.3473 - accuracy: 0.88 - ETA: 4s - loss: 0.3481 - accuracy: 0.88 - ETA: 4s - loss: 0.3474 - accuracy: 0.88 - ETA: 4s - loss: 0.3477 - accuracy: 0.88 - ETA: 4s - loss: 0.3475 - accuracy: 0.88 - ETA: 4s - loss: 0.3474 - accuracy: 0.88 - ETA: 4s - loss: 0.3475 - accuracy: 0.88 - ETA: 3s - loss: 0.3480 - accuracy: 0.88 - ETA: 3s - loss: 0.3480 - accuracy: 0.88 - ETA: 3s - loss: 0.3494 - accuracy: 0.88 - ETA: 3s - loss: 0.3494 - accuracy: 0.88 - ETA: 3s - loss: 0.3494 - accuracy: 0.88 - ETA: 3s - loss: 0.3496 - accuracy: 0.88 - ETA: 3s - loss: 0.3496 - accuracy: 0.88 - ETA: 2s - loss: 0.3498 - accuracy: 0.88 - ETA: 2s - loss: 0.3500 - accuracy: 0.88 - ETA: 2s - loss: 0.3496 - accuracy: 0.88 - ETA: 2s - loss: 0.3493 - accuracy: 0.88 - ETA: 2s - loss: 0.3494 - accuracy: 0.88 - ETA: 2s - loss: 0.3493 - accuracy: 0.88 - ETA: 2s - loss: 0.3492 - accuracy: 0.88 - ETA: 1s - loss: 0.3492 - accuracy: 0.88 - ETA: 1s - loss: 0.3491 - accuracy: 0.88 - ETA: 1s - loss: 0.3493 - accuracy: 0.88 - ETA: 1s - loss: 0.3492 - accuracy: 0.88 - ETA: 1s - loss: 0.3491 - accuracy: 0.88 - ETA: 1s - loss: 0.3492 - accuracy: 0.88 - ETA: 1s - loss: 0.3492 - accuracy: 0.88 - ETA: 0s - loss: 0.3493 - accuracy: 0.89 - ETA: 0s - loss: 0.3495 - accuracy: 0.88 - ETA: 0s - loss: 0.3489 - accuracy: 0.89 - ETA: 0s - loss: 0.3489 - accuracy: 0.89 - ETA: 0s - loss: 0.3494 - accuracy: 0.88 - ETA: 0s - loss: 0.3493 - accuracy: 0.88 - ETA: 0s - loss: 0.3487 - accuracy: 0.8900\n",
      "Epoch 00023: val_accuracy did not improve from 0.90433\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3491 - accuracy: 0.8899 - val_loss: 0.3416 - val_accuracy: 0.9005\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3681 - accuracy: 0.887 - ETA: 11s - loss: 0.3241 - accuracy: 0.895 - ETA: 11s - loss: 0.3317 - accuracy: 0.890 - ETA: 11s - loss: 0.3268 - accuracy: 0.894 - ETA: 11s - loss: 0.3257 - accuracy: 0.894 - ETA: 11s - loss: 0.3276 - accuracy: 0.893 - ETA: 10s - loss: 0.3310 - accuracy: 0.893 - ETA: 10s - loss: 0.3338 - accuracy: 0.894 - ETA: 10s - loss: 0.3297 - accuracy: 0.895 - ETA: 10s - loss: 0.3291 - accuracy: 0.895 - ETA: 10s - loss: 0.3311 - accuracy: 0.894 - ETA: 10s - loss: 0.3312 - accuracy: 0.894 - ETA: 10s - loss: 0.3347 - accuracy: 0.892 - ETA: 9s - loss: 0.3348 - accuracy: 0.892 - ETA: 9s - loss: 0.3349 - accuracy: 0.89 - ETA: 9s - loss: 0.3330 - accuracy: 0.89 - ETA: 9s - loss: 0.3350 - accuracy: 0.89 - ETA: 9s - loss: 0.3346 - accuracy: 0.89 - ETA: 9s - loss: 0.3350 - accuracy: 0.89 - ETA: 9s - loss: 0.3354 - accuracy: 0.89 - ETA: 8s - loss: 0.3343 - accuracy: 0.89 - ETA: 8s - loss: 0.3339 - accuracy: 0.89 - ETA: 8s - loss: 0.3335 - accuracy: 0.89 - ETA: 8s - loss: 0.3332 - accuracy: 0.89 - ETA: 8s - loss: 0.3327 - accuracy: 0.89 - ETA: 8s - loss: 0.3332 - accuracy: 0.89 - ETA: 8s - loss: 0.3327 - accuracy: 0.89 - ETA: 7s - loss: 0.3324 - accuracy: 0.89 - ETA: 7s - loss: 0.3328 - accuracy: 0.89 - ETA: 7s - loss: 0.3332 - accuracy: 0.89 - ETA: 7s - loss: 0.3335 - accuracy: 0.89 - ETA: 7s - loss: 0.3353 - accuracy: 0.89 - ETA: 7s - loss: 0.3338 - accuracy: 0.89 - ETA: 7s - loss: 0.3339 - accuracy: 0.89 - ETA: 6s - loss: 0.3334 - accuracy: 0.89 - ETA: 6s - loss: 0.3333 - accuracy: 0.89 - ETA: 6s - loss: 0.3333 - accuracy: 0.89 - ETA: 6s - loss: 0.3343 - accuracy: 0.89 - ETA: 6s - loss: 0.3351 - accuracy: 0.89 - ETA: 6s - loss: 0.3351 - accuracy: 0.89 - ETA: 6s - loss: 0.3348 - accuracy: 0.89 - ETA: 6s - loss: 0.3343 - accuracy: 0.89 - ETA: 5s - loss: 0.3352 - accuracy: 0.89 - ETA: 5s - loss: 0.3353 - accuracy: 0.89 - ETA: 5s - loss: 0.3363 - accuracy: 0.89 - ETA: 5s - loss: 0.3370 - accuracy: 0.89 - ETA: 5s - loss: 0.3372 - accuracy: 0.89 - ETA: 5s - loss: 0.3367 - accuracy: 0.89 - ETA: 5s - loss: 0.3357 - accuracy: 0.89 - ETA: 4s - loss: 0.3359 - accuracy: 0.89 - ETA: 4s - loss: 0.3368 - accuracy: 0.89 - ETA: 4s - loss: 0.3370 - accuracy: 0.89 - ETA: 4s - loss: 0.3367 - accuracy: 0.89 - ETA: 4s - loss: 0.3360 - accuracy: 0.89 - ETA: 4s - loss: 0.3360 - accuracy: 0.89 - ETA: 4s - loss: 0.3352 - accuracy: 0.89 - ETA: 3s - loss: 0.3354 - accuracy: 0.89 - ETA: 3s - loss: 0.3357 - accuracy: 0.89 - ETA: 3s - loss: 0.3366 - accuracy: 0.89 - ETA: 3s - loss: 0.3378 - accuracy: 0.89 - ETA: 3s - loss: 0.3378 - accuracy: 0.89 - ETA: 3s - loss: 0.3381 - accuracy: 0.89 - ETA: 3s - loss: 0.3379 - accuracy: 0.89 - ETA: 2s - loss: 0.3379 - accuracy: 0.89 - ETA: 2s - loss: 0.3380 - accuracy: 0.89 - ETA: 2s - loss: 0.3378 - accuracy: 0.89 - ETA: 2s - loss: 0.3379 - accuracy: 0.89 - ETA: 2s - loss: 0.3381 - accuracy: 0.89 - ETA: 2s - loss: 0.3385 - accuracy: 0.89 - ETA: 2s - loss: 0.3387 - accuracy: 0.89 - ETA: 1s - loss: 0.3381 - accuracy: 0.89 - ETA: 1s - loss: 0.3374 - accuracy: 0.89 - ETA: 1s - loss: 0.3378 - accuracy: 0.89 - ETA: 1s - loss: 0.3385 - accuracy: 0.89 - ETA: 1s - loss: 0.3385 - accuracy: 0.89 - ETA: 1s - loss: 0.3386 - accuracy: 0.89 - ETA: 1s - loss: 0.3391 - accuracy: 0.89 - ETA: 0s - loss: 0.3395 - accuracy: 0.89 - ETA: 0s - loss: 0.3398 - accuracy: 0.89 - ETA: 0s - loss: 0.3398 - accuracy: 0.89 - ETA: 0s - loss: 0.3395 - accuracy: 0.89 - ETA: 0s - loss: 0.3405 - accuracy: 0.89 - ETA: 0s - loss: 0.3407 - accuracy: 0.89 - ETA: 0s - loss: 0.3408 - accuracy: 0.8922\n",
      "Epoch 00024: val_accuracy improved from 0.90433 to 0.90645, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3410 - accuracy: 0.8921 - val_loss: 0.3166 - val_accuracy: 0.9065\n",
      "Epoch 25/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3662 - accuracy: 0.886 - ETA: 11s - loss: 0.3518 - accuracy: 0.894 - ETA: 11s - loss: 0.3543 - accuracy: 0.897 - ETA: 11s - loss: 0.3523 - accuracy: 0.893 - ETA: 11s - loss: 0.3393 - accuracy: 0.896 - ETA: 11s - loss: 0.3395 - accuracy: 0.894 - ETA: 10s - loss: 0.3374 - accuracy: 0.895 - ETA: 10s - loss: 0.3366 - accuracy: 0.896 - ETA: 10s - loss: 0.3339 - accuracy: 0.896 - ETA: 10s - loss: 0.3320 - accuracy: 0.896 - ETA: 10s - loss: 0.3363 - accuracy: 0.894 - ETA: 10s - loss: 0.3354 - accuracy: 0.894 - ETA: 10s - loss: 0.3335 - accuracy: 0.894 - ETA: 9s - loss: 0.3359 - accuracy: 0.894 - ETA: 9s - loss: 0.3334 - accuracy: 0.89 - ETA: 9s - loss: 0.3328 - accuracy: 0.89 - ETA: 9s - loss: 0.3342 - accuracy: 0.89 - ETA: 9s - loss: 0.3338 - accuracy: 0.89 - ETA: 9s - loss: 0.3332 - accuracy: 0.89 - ETA: 9s - loss: 0.3331 - accuracy: 0.89 - ETA: 8s - loss: 0.3321 - accuracy: 0.89 - ETA: 8s - loss: 0.3305 - accuracy: 0.89 - ETA: 8s - loss: 0.3297 - accuracy: 0.89 - ETA: 8s - loss: 0.3292 - accuracy: 0.89 - ETA: 8s - loss: 0.3285 - accuracy: 0.89 - ETA: 8s - loss: 0.3275 - accuracy: 0.89 - ETA: 8s - loss: 0.3272 - accuracy: 0.89 - ETA: 8s - loss: 0.3278 - accuracy: 0.89 - ETA: 7s - loss: 0.3283 - accuracy: 0.89 - ETA: 7s - loss: 0.3280 - accuracy: 0.89 - ETA: 7s - loss: 0.3288 - accuracy: 0.89 - ETA: 7s - loss: 0.3291 - accuracy: 0.89 - ETA: 7s - loss: 0.3293 - accuracy: 0.89 - ETA: 7s - loss: 0.3299 - accuracy: 0.89 - ETA: 7s - loss: 0.3304 - accuracy: 0.89 - ETA: 6s - loss: 0.3314 - accuracy: 0.89 - ETA: 6s - loss: 0.3302 - accuracy: 0.89 - ETA: 6s - loss: 0.3320 - accuracy: 0.89 - ETA: 6s - loss: 0.3328 - accuracy: 0.89 - ETA: 6s - loss: 0.3332 - accuracy: 0.89 - ETA: 6s - loss: 0.3335 - accuracy: 0.89 - ETA: 6s - loss: 0.3332 - accuracy: 0.89 - ETA: 5s - loss: 0.3326 - accuracy: 0.89 - ETA: 5s - loss: 0.3320 - accuracy: 0.89 - ETA: 5s - loss: 0.3328 - accuracy: 0.89 - ETA: 5s - loss: 0.3327 - accuracy: 0.89 - ETA: 5s - loss: 0.3323 - accuracy: 0.89 - ETA: 5s - loss: 0.3323 - accuracy: 0.89 - ETA: 5s - loss: 0.3318 - accuracy: 0.89 - ETA: 4s - loss: 0.3321 - accuracy: 0.89 - ETA: 4s - loss: 0.3322 - accuracy: 0.89 - ETA: 4s - loss: 0.3317 - accuracy: 0.89 - ETA: 4s - loss: 0.3319 - accuracy: 0.89 - ETA: 4s - loss: 0.3324 - accuracy: 0.89 - ETA: 4s - loss: 0.3317 - accuracy: 0.89 - ETA: 4s - loss: 0.3312 - accuracy: 0.89 - ETA: 3s - loss: 0.3314 - accuracy: 0.89 - ETA: 3s - loss: 0.3324 - accuracy: 0.89 - ETA: 3s - loss: 0.3335 - accuracy: 0.89 - ETA: 3s - loss: 0.3341 - accuracy: 0.89 - ETA: 3s - loss: 0.3345 - accuracy: 0.89 - ETA: 3s - loss: 0.3350 - accuracy: 0.89 - ETA: 3s - loss: 0.3349 - accuracy: 0.89 - ETA: 2s - loss: 0.3347 - accuracy: 0.89 - ETA: 2s - loss: 0.3342 - accuracy: 0.89 - ETA: 2s - loss: 0.3345 - accuracy: 0.89 - ETA: 2s - loss: 0.3344 - accuracy: 0.89 - ETA: 2s - loss: 0.3340 - accuracy: 0.89 - ETA: 2s - loss: 0.3341 - accuracy: 0.89 - ETA: 2s - loss: 0.3338 - accuracy: 0.89 - ETA: 1s - loss: 0.3339 - accuracy: 0.89 - ETA: 1s - loss: 0.3329 - accuracy: 0.89 - ETA: 1s - loss: 0.3330 - accuracy: 0.89 - ETA: 1s - loss: 0.3329 - accuracy: 0.89 - ETA: 1s - loss: 0.3333 - accuracy: 0.89 - ETA: 1s - loss: 0.3326 - accuracy: 0.89 - ETA: 1s - loss: 0.3321 - accuracy: 0.89 - ETA: 0s - loss: 0.3314 - accuracy: 0.89 - ETA: 0s - loss: 0.3313 - accuracy: 0.89 - ETA: 0s - loss: 0.3310 - accuracy: 0.89 - ETA: 0s - loss: 0.3311 - accuracy: 0.89 - ETA: 0s - loss: 0.3311 - accuracy: 0.89 - ETA: 0s - loss: 0.3308 - accuracy: 0.89 - ETA: 0s - loss: 0.3303 - accuracy: 0.8958\n",
      "Epoch 00025: val_accuracy improved from 0.90645 to 0.90706, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.3304 - accuracy: 0.8957 - val_loss: 0.3170 - val_accuracy: 0.9071\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3028 - accuracy: 0.907 - ETA: 11s - loss: 0.3096 - accuracy: 0.903 - ETA: 11s - loss: 0.3177 - accuracy: 0.898 - ETA: 11s - loss: 0.3161 - accuracy: 0.899 - ETA: 11s - loss: 0.3220 - accuracy: 0.899 - ETA: 11s - loss: 0.3202 - accuracy: 0.899 - ETA: 10s - loss: 0.3229 - accuracy: 0.899 - ETA: 10s - loss: 0.3225 - accuracy: 0.898 - ETA: 10s - loss: 0.3223 - accuracy: 0.898 - ETA: 10s - loss: 0.3279 - accuracy: 0.895 - ETA: 10s - loss: 0.3306 - accuracy: 0.895 - ETA: 10s - loss: 0.3310 - accuracy: 0.895 - ETA: 10s - loss: 0.3304 - accuracy: 0.895 - ETA: 9s - loss: 0.3283 - accuracy: 0.895 - ETA: 9s - loss: 0.3296 - accuracy: 0.89 - ETA: 9s - loss: 0.3264 - accuracy: 0.89 - ETA: 9s - loss: 0.3292 - accuracy: 0.89 - ETA: 9s - loss: 0.3330 - accuracy: 0.89 - ETA: 9s - loss: 0.3341 - accuracy: 0.89 - ETA: 9s - loss: 0.3332 - accuracy: 0.89 - ETA: 8s - loss: 0.3309 - accuracy: 0.89 - ETA: 8s - loss: 0.3309 - accuracy: 0.89 - ETA: 8s - loss: 0.3288 - accuracy: 0.89 - ETA: 8s - loss: 0.3289 - accuracy: 0.89 - ETA: 8s - loss: 0.3276 - accuracy: 0.89 - ETA: 8s - loss: 0.3266 - accuracy: 0.89 - ETA: 8s - loss: 0.3274 - accuracy: 0.89 - ETA: 8s - loss: 0.3283 - accuracy: 0.89 - ETA: 7s - loss: 0.3257 - accuracy: 0.89 - ETA: 7s - loss: 0.3260 - accuracy: 0.89 - ETA: 7s - loss: 0.3268 - accuracy: 0.89 - ETA: 7s - loss: 0.3260 - accuracy: 0.89 - ETA: 7s - loss: 0.3264 - accuracy: 0.89 - ETA: 7s - loss: 0.3275 - accuracy: 0.89 - ETA: 7s - loss: 0.3261 - accuracy: 0.89 - ETA: 6s - loss: 0.3269 - accuracy: 0.89 - ETA: 6s - loss: 0.3263 - accuracy: 0.89 - ETA: 6s - loss: 0.3260 - accuracy: 0.89 - ETA: 6s - loss: 0.3253 - accuracy: 0.89 - ETA: 6s - loss: 0.3262 - accuracy: 0.89 - ETA: 6s - loss: 0.3263 - accuracy: 0.89 - ETA: 6s - loss: 0.3273 - accuracy: 0.89 - ETA: 5s - loss: 0.3271 - accuracy: 0.89 - ETA: 5s - loss: 0.3266 - accuracy: 0.89 - ETA: 5s - loss: 0.3266 - accuracy: 0.89 - ETA: 5s - loss: 0.3254 - accuracy: 0.89 - ETA: 5s - loss: 0.3246 - accuracy: 0.89 - ETA: 5s - loss: 0.3249 - accuracy: 0.89 - ETA: 5s - loss: 0.3239 - accuracy: 0.89 - ETA: 4s - loss: 0.3244 - accuracy: 0.89 - ETA: 4s - loss: 0.3243 - accuracy: 0.89 - ETA: 4s - loss: 0.3252 - accuracy: 0.89 - ETA: 4s - loss: 0.3249 - accuracy: 0.89 - ETA: 4s - loss: 0.3258 - accuracy: 0.89 - ETA: 4s - loss: 0.3251 - accuracy: 0.89 - ETA: 4s - loss: 0.3251 - accuracy: 0.89 - ETA: 3s - loss: 0.3250 - accuracy: 0.89 - ETA: 3s - loss: 0.3242 - accuracy: 0.89 - ETA: 3s - loss: 0.3242 - accuracy: 0.89 - ETA: 3s - loss: 0.3247 - accuracy: 0.89 - ETA: 3s - loss: 0.3253 - accuracy: 0.89 - ETA: 3s - loss: 0.3254 - accuracy: 0.89 - ETA: 3s - loss: 0.3258 - accuracy: 0.89 - ETA: 2s - loss: 0.3264 - accuracy: 0.89 - ETA: 2s - loss: 0.3272 - accuracy: 0.89 - ETA: 2s - loss: 0.3277 - accuracy: 0.89 - ETA: 2s - loss: 0.3276 - accuracy: 0.89 - ETA: 2s - loss: 0.3271 - accuracy: 0.89 - ETA: 2s - loss: 0.3271 - accuracy: 0.89 - ETA: 2s - loss: 0.3266 - accuracy: 0.89 - ETA: 1s - loss: 0.3269 - accuracy: 0.89 - ETA: 1s - loss: 0.3269 - accuracy: 0.89 - ETA: 1s - loss: 0.3271 - accuracy: 0.89 - ETA: 1s - loss: 0.3271 - accuracy: 0.89 - ETA: 1s - loss: 0.3269 - accuracy: 0.89 - ETA: 1s - loss: 0.3266 - accuracy: 0.89 - ETA: 1s - loss: 0.3265 - accuracy: 0.89 - ETA: 0s - loss: 0.3261 - accuracy: 0.89 - ETA: 0s - loss: 0.3266 - accuracy: 0.89 - ETA: 0s - loss: 0.3268 - accuracy: 0.89 - ETA: 0s - loss: 0.3271 - accuracy: 0.89 - ETA: 0s - loss: 0.3274 - accuracy: 0.89 - ETA: 0s - loss: 0.3280 - accuracy: 0.89 - ETA: 0s - loss: 0.3277 - accuracy: 0.8966\n",
      "Epoch 00026: val_accuracy improved from 0.90706 to 0.90786, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.3275 - accuracy: 0.8966 - val_loss: 0.3138 - val_accuracy: 0.9079\n",
      "Epoch 27/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3195 - accuracy: 0.890 - ETA: 11s - loss: 0.3197 - accuracy: 0.891 - ETA: 11s - loss: 0.3088 - accuracy: 0.896 - ETA: 11s - loss: 0.3001 - accuracy: 0.898 - ETA: 11s - loss: 0.2991 - accuracy: 0.899 - ETA: 11s - loss: 0.2987 - accuracy: 0.900 - ETA: 11s - loss: 0.3037 - accuracy: 0.900 - ETA: 11s - loss: 0.3068 - accuracy: 0.900 - ETA: 11s - loss: 0.3070 - accuracy: 0.901 - ETA: 11s - loss: 0.3092 - accuracy: 0.901 - ETA: 11s - loss: 0.3086 - accuracy: 0.901 - ETA: 10s - loss: 0.3114 - accuracy: 0.899 - ETA: 10s - loss: 0.3105 - accuracy: 0.899 - ETA: 10s - loss: 0.3113 - accuracy: 0.899 - ETA: 10s - loss: 0.3089 - accuracy: 0.900 - ETA: 10s - loss: 0.3105 - accuracy: 0.901 - ETA: 10s - loss: 0.3124 - accuracy: 0.900 - ETA: 9s - loss: 0.3149 - accuracy: 0.899 - ETA: 9s - loss: 0.3163 - accuracy: 0.89 - ETA: 9s - loss: 0.3191 - accuracy: 0.89 - ETA: 9s - loss: 0.3186 - accuracy: 0.89 - ETA: 9s - loss: 0.3193 - accuracy: 0.89 - ETA: 9s - loss: 0.3182 - accuracy: 0.89 - ETA: 8s - loss: 0.3180 - accuracy: 0.89 - ETA: 8s - loss: 0.3226 - accuracy: 0.89 - ETA: 8s - loss: 0.3231 - accuracy: 0.89 - ETA: 8s - loss: 0.3217 - accuracy: 0.89 - ETA: 8s - loss: 0.3205 - accuracy: 0.89 - ETA: 8s - loss: 0.3200 - accuracy: 0.89 - ETA: 8s - loss: 0.3194 - accuracy: 0.89 - ETA: 7s - loss: 0.3209 - accuracy: 0.89 - ETA: 7s - loss: 0.3221 - accuracy: 0.89 - ETA: 7s - loss: 0.3220 - accuracy: 0.89 - ETA: 7s - loss: 0.3229 - accuracy: 0.89 - ETA: 7s - loss: 0.3225 - accuracy: 0.89 - ETA: 7s - loss: 0.3234 - accuracy: 0.89 - ETA: 6s - loss: 0.3233 - accuracy: 0.89 - ETA: 6s - loss: 0.3235 - accuracy: 0.89 - ETA: 6s - loss: 0.3214 - accuracy: 0.89 - ETA: 6s - loss: 0.3199 - accuracy: 0.89 - ETA: 6s - loss: 0.3209 - accuracy: 0.89 - ETA: 6s - loss: 0.3207 - accuracy: 0.89 - ETA: 6s - loss: 0.3209 - accuracy: 0.89 - ETA: 5s - loss: 0.3212 - accuracy: 0.89 - ETA: 5s - loss: 0.3225 - accuracy: 0.89 - ETA: 5s - loss: 0.3237 - accuracy: 0.89 - ETA: 5s - loss: 0.3248 - accuracy: 0.89 - ETA: 5s - loss: 0.3241 - accuracy: 0.89 - ETA: 5s - loss: 0.3241 - accuracy: 0.89 - ETA: 5s - loss: 0.3242 - accuracy: 0.89 - ETA: 4s - loss: 0.3236 - accuracy: 0.89 - ETA: 4s - loss: 0.3240 - accuracy: 0.89 - ETA: 4s - loss: 0.3239 - accuracy: 0.89 - ETA: 4s - loss: 0.3239 - accuracy: 0.89 - ETA: 4s - loss: 0.3246 - accuracy: 0.89 - ETA: 4s - loss: 0.3247 - accuracy: 0.89 - ETA: 4s - loss: 0.3246 - accuracy: 0.89 - ETA: 3s - loss: 0.3243 - accuracy: 0.89 - ETA: 3s - loss: 0.3243 - accuracy: 0.89 - ETA: 3s - loss: 0.3243 - accuracy: 0.89 - ETA: 3s - loss: 0.3241 - accuracy: 0.89 - ETA: 3s - loss: 0.3240 - accuracy: 0.89 - ETA: 3s - loss: 0.3236 - accuracy: 0.89 - ETA: 3s - loss: 0.3229 - accuracy: 0.89 - ETA: 2s - loss: 0.3232 - accuracy: 0.89 - ETA: 2s - loss: 0.3228 - accuracy: 0.89 - ETA: 2s - loss: 0.3224 - accuracy: 0.89 - ETA: 2s - loss: 0.3220 - accuracy: 0.89 - ETA: 2s - loss: 0.3218 - accuracy: 0.89 - ETA: 2s - loss: 0.3217 - accuracy: 0.89 - ETA: 2s - loss: 0.3217 - accuracy: 0.89 - ETA: 1s - loss: 0.3230 - accuracy: 0.89 - ETA: 1s - loss: 0.3237 - accuracy: 0.89 - ETA: 1s - loss: 0.3243 - accuracy: 0.89 - ETA: 1s - loss: 0.3245 - accuracy: 0.89 - ETA: 1s - loss: 0.3251 - accuracy: 0.89 - ETA: 1s - loss: 0.3252 - accuracy: 0.89 - ETA: 0s - loss: 0.3252 - accuracy: 0.89 - ETA: 0s - loss: 0.3247 - accuracy: 0.89 - ETA: 0s - loss: 0.3249 - accuracy: 0.89 - ETA: 0s - loss: 0.3256 - accuracy: 0.89 - ETA: 0s - loss: 0.3253 - accuracy: 0.89 - ETA: 0s - loss: 0.3254 - accuracy: 0.89 - ETA: 0s - loss: 0.3251 - accuracy: 0.8972\n",
      "Epoch 00027: val_accuracy improved from 0.90786 to 0.91129, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 156us/sample - loss: 0.3252 - accuracy: 0.8973 - val_loss: 0.3138 - val_accuracy: 0.9113\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 12s - loss: 0.3593 - accuracy: 0.895 - ETA: 12s - loss: 0.3299 - accuracy: 0.898 - ETA: 12s - loss: 0.3199 - accuracy: 0.901 - ETA: 12s - loss: 0.3104 - accuracy: 0.903 - ETA: 11s - loss: 0.3045 - accuracy: 0.905 - ETA: 11s - loss: 0.2976 - accuracy: 0.906 - ETA: 11s - loss: 0.2967 - accuracy: 0.907 - ETA: 11s - loss: 0.2980 - accuracy: 0.906 - ETA: 11s - loss: 0.2967 - accuracy: 0.905 - ETA: 10s - loss: 0.2993 - accuracy: 0.903 - ETA: 10s - loss: 0.2973 - accuracy: 0.904 - ETA: 10s - loss: 0.3007 - accuracy: 0.903 - ETA: 10s - loss: 0.2993 - accuracy: 0.904 - ETA: 10s - loss: 0.3004 - accuracy: 0.903 - ETA: 10s - loss: 0.3000 - accuracy: 0.903 - ETA: 9s - loss: 0.2977 - accuracy: 0.904 - ETA: 9s - loss: 0.2999 - accuracy: 0.90 - ETA: 9s - loss: 0.2992 - accuracy: 0.90 - ETA: 9s - loss: 0.2994 - accuracy: 0.90 - ETA: 9s - loss: 0.3003 - accuracy: 0.90 - ETA: 9s - loss: 0.3022 - accuracy: 0.90 - ETA: 9s - loss: 0.3040 - accuracy: 0.90 - ETA: 8s - loss: 0.3032 - accuracy: 0.90 - ETA: 8s - loss: 0.3022 - accuracy: 0.90 - ETA: 8s - loss: 0.3007 - accuracy: 0.90 - ETA: 8s - loss: 0.3035 - accuracy: 0.90 - ETA: 8s - loss: 0.3046 - accuracy: 0.90 - ETA: 8s - loss: 0.3061 - accuracy: 0.90 - ETA: 8s - loss: 0.3072 - accuracy: 0.90 - ETA: 7s - loss: 0.3090 - accuracy: 0.90 - ETA: 7s - loss: 0.3087 - accuracy: 0.90 - ETA: 7s - loss: 0.3097 - accuracy: 0.90 - ETA: 7s - loss: 0.3107 - accuracy: 0.90 - ETA: 7s - loss: 0.3119 - accuracy: 0.90 - ETA: 7s - loss: 0.3120 - accuracy: 0.90 - ETA: 7s - loss: 0.3125 - accuracy: 0.90 - ETA: 6s - loss: 0.3126 - accuracy: 0.90 - ETA: 6s - loss: 0.3118 - accuracy: 0.90 - ETA: 6s - loss: 0.3117 - accuracy: 0.90 - ETA: 6s - loss: 0.3126 - accuracy: 0.90 - ETA: 6s - loss: 0.3133 - accuracy: 0.90 - ETA: 6s - loss: 0.3127 - accuracy: 0.90 - ETA: 6s - loss: 0.3139 - accuracy: 0.90 - ETA: 5s - loss: 0.3142 - accuracy: 0.90 - ETA: 5s - loss: 0.3132 - accuracy: 0.90 - ETA: 5s - loss: 0.3139 - accuracy: 0.90 - ETA: 5s - loss: 0.3152 - accuracy: 0.90 - ETA: 5s - loss: 0.3153 - accuracy: 0.90 - ETA: 5s - loss: 0.3159 - accuracy: 0.89 - ETA: 5s - loss: 0.3164 - accuracy: 0.89 - ETA: 4s - loss: 0.3156 - accuracy: 0.89 - ETA: 4s - loss: 0.3155 - accuracy: 0.89 - ETA: 4s - loss: 0.3146 - accuracy: 0.90 - ETA: 4s - loss: 0.3145 - accuracy: 0.90 - ETA: 4s - loss: 0.3154 - accuracy: 0.89 - ETA: 4s - loss: 0.3159 - accuracy: 0.89 - ETA: 4s - loss: 0.3160 - accuracy: 0.89 - ETA: 3s - loss: 0.3160 - accuracy: 0.89 - ETA: 3s - loss: 0.3161 - accuracy: 0.89 - ETA: 3s - loss: 0.3162 - accuracy: 0.89 - ETA: 3s - loss: 0.3160 - accuracy: 0.89 - ETA: 3s - loss: 0.3166 - accuracy: 0.89 - ETA: 3s - loss: 0.3165 - accuracy: 0.89 - ETA: 3s - loss: 0.3170 - accuracy: 0.89 - ETA: 2s - loss: 0.3168 - accuracy: 0.89 - ETA: 2s - loss: 0.3164 - accuracy: 0.89 - ETA: 2s - loss: 0.3157 - accuracy: 0.89 - ETA: 2s - loss: 0.3156 - accuracy: 0.89 - ETA: 2s - loss: 0.3154 - accuracy: 0.89 - ETA: 2s - loss: 0.3146 - accuracy: 0.89 - ETA: 1s - loss: 0.3148 - accuracy: 0.89 - ETA: 1s - loss: 0.3150 - accuracy: 0.89 - ETA: 1s - loss: 0.3146 - accuracy: 0.89 - ETA: 1s - loss: 0.3149 - accuracy: 0.89 - ETA: 1s - loss: 0.3145 - accuracy: 0.89 - ETA: 1s - loss: 0.3146 - accuracy: 0.89 - ETA: 1s - loss: 0.3150 - accuracy: 0.89 - ETA: 0s - loss: 0.3157 - accuracy: 0.89 - ETA: 0s - loss: 0.3163 - accuracy: 0.89 - ETA: 0s - loss: 0.3166 - accuracy: 0.89 - ETA: 0s - loss: 0.3167 - accuracy: 0.89 - ETA: 0s - loss: 0.3163 - accuracy: 0.89 - ETA: 0s - loss: 0.3166 - accuracy: 0.89 - ETA: 0s - loss: 0.3166 - accuracy: 0.8988\n",
      "Epoch 00028: val_accuracy did not improve from 0.91129\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.3166 - accuracy: 0.8988 - val_loss: 0.3146 - val_accuracy: 0.9086\n",
      "Epoch 29/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2777 - accuracy: 0.908 - ETA: 11s - loss: 0.2973 - accuracy: 0.903 - ETA: 11s - loss: 0.2965 - accuracy: 0.904 - ETA: 11s - loss: 0.2879 - accuracy: 0.905 - ETA: 11s - loss: 0.2921 - accuracy: 0.904 - ETA: 11s - loss: 0.2925 - accuracy: 0.904 - ETA: 11s - loss: 0.3009 - accuracy: 0.903 - ETA: 10s - loss: 0.3019 - accuracy: 0.902 - ETA: 10s - loss: 0.2999 - accuracy: 0.903 - ETA: 10s - loss: 0.2990 - accuracy: 0.902 - ETA: 10s - loss: 0.3000 - accuracy: 0.901 - ETA: 10s - loss: 0.2982 - accuracy: 0.902 - ETA: 10s - loss: 0.2990 - accuracy: 0.902 - ETA: 10s - loss: 0.2983 - accuracy: 0.903 - ETA: 9s - loss: 0.2992 - accuracy: 0.903 - ETA: 9s - loss: 0.3011 - accuracy: 0.90 - ETA: 9s - loss: 0.3036 - accuracy: 0.90 - ETA: 9s - loss: 0.3059 - accuracy: 0.90 - ETA: 9s - loss: 0.3070 - accuracy: 0.90 - ETA: 9s - loss: 0.3061 - accuracy: 0.90 - ETA: 9s - loss: 0.3081 - accuracy: 0.90 - ETA: 8s - loss: 0.3077 - accuracy: 0.90 - ETA: 8s - loss: 0.3063 - accuracy: 0.90 - ETA: 8s - loss: 0.3049 - accuracy: 0.90 - ETA: 8s - loss: 0.3036 - accuracy: 0.90 - ETA: 8s - loss: 0.3036 - accuracy: 0.90 - ETA: 8s - loss: 0.3032 - accuracy: 0.90 - ETA: 8s - loss: 0.3041 - accuracy: 0.90 - ETA: 7s - loss: 0.3049 - accuracy: 0.90 - ETA: 7s - loss: 0.3045 - accuracy: 0.90 - ETA: 7s - loss: 0.3040 - accuracy: 0.90 - ETA: 7s - loss: 0.3040 - accuracy: 0.90 - ETA: 7s - loss: 0.3042 - accuracy: 0.90 - ETA: 7s - loss: 0.3039 - accuracy: 0.90 - ETA: 7s - loss: 0.3047 - accuracy: 0.90 - ETA: 6s - loss: 0.3051 - accuracy: 0.90 - ETA: 6s - loss: 0.3055 - accuracy: 0.90 - ETA: 6s - loss: 0.3060 - accuracy: 0.90 - ETA: 6s - loss: 0.3065 - accuracy: 0.90 - ETA: 6s - loss: 0.3065 - accuracy: 0.90 - ETA: 6s - loss: 0.3063 - accuracy: 0.90 - ETA: 6s - loss: 0.3060 - accuracy: 0.90 - ETA: 5s - loss: 0.3065 - accuracy: 0.90 - ETA: 5s - loss: 0.3068 - accuracy: 0.90 - ETA: 5s - loss: 0.3072 - accuracy: 0.90 - ETA: 5s - loss: 0.3080 - accuracy: 0.90 - ETA: 5s - loss: 0.3088 - accuracy: 0.90 - ETA: 5s - loss: 0.3090 - accuracy: 0.90 - ETA: 5s - loss: 0.3088 - accuracy: 0.90 - ETA: 4s - loss: 0.3088 - accuracy: 0.90 - ETA: 4s - loss: 0.3091 - accuracy: 0.90 - ETA: 4s - loss: 0.3085 - accuracy: 0.90 - ETA: 4s - loss: 0.3083 - accuracy: 0.90 - ETA: 4s - loss: 0.3082 - accuracy: 0.90 - ETA: 4s - loss: 0.3081 - accuracy: 0.90 - ETA: 4s - loss: 0.3082 - accuracy: 0.90 - ETA: 3s - loss: 0.3080 - accuracy: 0.90 - ETA: 3s - loss: 0.3076 - accuracy: 0.90 - ETA: 3s - loss: 0.3077 - accuracy: 0.90 - ETA: 3s - loss: 0.3081 - accuracy: 0.90 - ETA: 3s - loss: 0.3079 - accuracy: 0.90 - ETA: 3s - loss: 0.3073 - accuracy: 0.90 - ETA: 3s - loss: 0.3075 - accuracy: 0.90 - ETA: 2s - loss: 0.3073 - accuracy: 0.90 - ETA: 2s - loss: 0.3078 - accuracy: 0.90 - ETA: 2s - loss: 0.3086 - accuracy: 0.90 - ETA: 2s - loss: 0.3092 - accuracy: 0.90 - ETA: 2s - loss: 0.3101 - accuracy: 0.90 - ETA: 2s - loss: 0.3105 - accuracy: 0.90 - ETA: 2s - loss: 0.3105 - accuracy: 0.90 - ETA: 1s - loss: 0.3114 - accuracy: 0.90 - ETA: 1s - loss: 0.3110 - accuracy: 0.90 - ETA: 1s - loss: 0.3105 - accuracy: 0.90 - ETA: 1s - loss: 0.3109 - accuracy: 0.90 - ETA: 1s - loss: 0.3110 - accuracy: 0.90 - ETA: 1s - loss: 0.3107 - accuracy: 0.90 - ETA: 1s - loss: 0.3113 - accuracy: 0.90 - ETA: 0s - loss: 0.3109 - accuracy: 0.90 - ETA: 0s - loss: 0.3105 - accuracy: 0.90 - ETA: 0s - loss: 0.3109 - accuracy: 0.90 - ETA: 0s - loss: 0.3106 - accuracy: 0.90 - ETA: 0s - loss: 0.3106 - accuracy: 0.90 - ETA: 0s - loss: 0.3109 - accuracy: 0.90 - ETA: 0s - loss: 0.3103 - accuracy: 0.9012\n",
      "Epoch 00029: val_accuracy improved from 0.91129 to 0.91290, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.3099 - accuracy: 0.9013 - val_loss: 0.3001 - val_accuracy: 0.9129\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2689 - accuracy: 0.913 - ETA: 11s - loss: 0.2783 - accuracy: 0.911 - ETA: 11s - loss: 0.2743 - accuracy: 0.913 - ETA: 11s - loss: 0.2861 - accuracy: 0.908 - ETA: 11s - loss: 0.2853 - accuracy: 0.909 - ETA: 11s - loss: 0.2871 - accuracy: 0.908 - ETA: 11s - loss: 0.2830 - accuracy: 0.908 - ETA: 11s - loss: 0.2835 - accuracy: 0.908 - ETA: 10s - loss: 0.2813 - accuracy: 0.908 - ETA: 10s - loss: 0.2847 - accuracy: 0.907 - ETA: 10s - loss: 0.2854 - accuracy: 0.907 - ETA: 10s - loss: 0.2860 - accuracy: 0.907 - ETA: 10s - loss: 0.2864 - accuracy: 0.908 - ETA: 10s - loss: 0.2901 - accuracy: 0.906 - ETA: 9s - loss: 0.2914 - accuracy: 0.906 - ETA: 9s - loss: 0.2973 - accuracy: 0.90 - ETA: 9s - loss: 0.3017 - accuracy: 0.90 - ETA: 9s - loss: 0.3041 - accuracy: 0.90 - ETA: 9s - loss: 0.3047 - accuracy: 0.90 - ETA: 9s - loss: 0.3050 - accuracy: 0.90 - ETA: 9s - loss: 0.3026 - accuracy: 0.90 - ETA: 8s - loss: 0.3036 - accuracy: 0.90 - ETA: 8s - loss: 0.3016 - accuracy: 0.90 - ETA: 8s - loss: 0.3016 - accuracy: 0.90 - ETA: 8s - loss: 0.3016 - accuracy: 0.90 - ETA: 8s - loss: 0.3007 - accuracy: 0.90 - ETA: 8s - loss: 0.3012 - accuracy: 0.90 - ETA: 8s - loss: 0.3002 - accuracy: 0.90 - ETA: 7s - loss: 0.3006 - accuracy: 0.90 - ETA: 7s - loss: 0.3000 - accuracy: 0.90 - ETA: 7s - loss: 0.3007 - accuracy: 0.90 - ETA: 7s - loss: 0.3016 - accuracy: 0.90 - ETA: 7s - loss: 0.3018 - accuracy: 0.90 - ETA: 7s - loss: 0.3011 - accuracy: 0.90 - ETA: 7s - loss: 0.3017 - accuracy: 0.90 - ETA: 6s - loss: 0.3012 - accuracy: 0.90 - ETA: 6s - loss: 0.3007 - accuracy: 0.90 - ETA: 6s - loss: 0.3010 - accuracy: 0.90 - ETA: 6s - loss: 0.3027 - accuracy: 0.90 - ETA: 6s - loss: 0.3033 - accuracy: 0.90 - ETA: 6s - loss: 0.3026 - accuracy: 0.90 - ETA: 6s - loss: 0.3045 - accuracy: 0.90 - ETA: 5s - loss: 0.3052 - accuracy: 0.90 - ETA: 5s - loss: 0.3042 - accuracy: 0.90 - ETA: 5s - loss: 0.3050 - accuracy: 0.90 - ETA: 5s - loss: 0.3052 - accuracy: 0.90 - ETA: 5s - loss: 0.3072 - accuracy: 0.90 - ETA: 5s - loss: 0.3082 - accuracy: 0.90 - ETA: 5s - loss: 0.3082 - accuracy: 0.90 - ETA: 4s - loss: 0.3083 - accuracy: 0.90 - ETA: 4s - loss: 0.3088 - accuracy: 0.90 - ETA: 4s - loss: 0.3084 - accuracy: 0.90 - ETA: 4s - loss: 0.3083 - accuracy: 0.90 - ETA: 4s - loss: 0.3070 - accuracy: 0.90 - ETA: 4s - loss: 0.3066 - accuracy: 0.90 - ETA: 4s - loss: 0.3063 - accuracy: 0.90 - ETA: 3s - loss: 0.3064 - accuracy: 0.90 - ETA: 3s - loss: 0.3058 - accuracy: 0.90 - ETA: 3s - loss: 0.3058 - accuracy: 0.90 - ETA: 3s - loss: 0.3065 - accuracy: 0.90 - ETA: 3s - loss: 0.3072 - accuracy: 0.90 - ETA: 3s - loss: 0.3085 - accuracy: 0.90 - ETA: 3s - loss: 0.3090 - accuracy: 0.90 - ETA: 2s - loss: 0.3089 - accuracy: 0.90 - ETA: 2s - loss: 0.3086 - accuracy: 0.90 - ETA: 2s - loss: 0.3088 - accuracy: 0.90 - ETA: 2s - loss: 0.3090 - accuracy: 0.90 - ETA: 2s - loss: 0.3087 - accuracy: 0.90 - ETA: 2s - loss: 0.3081 - accuracy: 0.90 - ETA: 2s - loss: 0.3078 - accuracy: 0.90 - ETA: 1s - loss: 0.3076 - accuracy: 0.90 - ETA: 1s - loss: 0.3076 - accuracy: 0.90 - ETA: 1s - loss: 0.3079 - accuracy: 0.90 - ETA: 1s - loss: 0.3080 - accuracy: 0.90 - ETA: 1s - loss: 0.3073 - accuracy: 0.90 - ETA: 1s - loss: 0.3070 - accuracy: 0.90 - ETA: 1s - loss: 0.3074 - accuracy: 0.90 - ETA: 0s - loss: 0.3079 - accuracy: 0.90 - ETA: 0s - loss: 0.3078 - accuracy: 0.90 - ETA: 0s - loss: 0.3081 - accuracy: 0.90 - ETA: 0s - loss: 0.3076 - accuracy: 0.90 - ETA: 0s - loss: 0.3076 - accuracy: 0.90 - ETA: 0s - loss: 0.3075 - accuracy: 0.90 - ETA: 0s - loss: 0.3074 - accuracy: 0.9032\n",
      "Epoch 00030: val_accuracy did not improve from 0.91290\n",
      "84800/84800 [==============================] - 13s 152us/sample - loss: 0.3074 - accuracy: 0.9032 - val_loss: 0.3320 - val_accuracy: 0.9036\n",
      "Epoch 31/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2867 - accuracy: 0.910 - ETA: 11s - loss: 0.2529 - accuracy: 0.919 - ETA: 11s - loss: 0.2760 - accuracy: 0.911 - ETA: 11s - loss: 0.2823 - accuracy: 0.910 - ETA: 11s - loss: 0.2820 - accuracy: 0.909 - ETA: 11s - loss: 0.2903 - accuracy: 0.907 - ETA: 11s - loss: 0.2889 - accuracy: 0.907 - ETA: 10s - loss: 0.2941 - accuracy: 0.906 - ETA: 10s - loss: 0.2891 - accuracy: 0.907 - ETA: 10s - loss: 0.2902 - accuracy: 0.907 - ETA: 10s - loss: 0.2940 - accuracy: 0.907 - ETA: 10s - loss: 0.2984 - accuracy: 0.905 - ETA: 10s - loss: 0.2959 - accuracy: 0.906 - ETA: 10s - loss: 0.2932 - accuracy: 0.906 - ETA: 9s - loss: 0.2935 - accuracy: 0.906 - ETA: 9s - loss: 0.2925 - accuracy: 0.90 - ETA: 9s - loss: 0.2950 - accuracy: 0.90 - ETA: 9s - loss: 0.2932 - accuracy: 0.90 - ETA: 9s - loss: 0.2936 - accuracy: 0.90 - ETA: 9s - loss: 0.2942 - accuracy: 0.90 - ETA: 9s - loss: 0.2949 - accuracy: 0.90 - ETA: 8s - loss: 0.2972 - accuracy: 0.90 - ETA: 8s - loss: 0.2987 - accuracy: 0.90 - ETA: 8s - loss: 0.2994 - accuracy: 0.90 - ETA: 8s - loss: 0.2998 - accuracy: 0.90 - ETA: 8s - loss: 0.3009 - accuracy: 0.90 - ETA: 8s - loss: 0.3003 - accuracy: 0.90 - ETA: 8s - loss: 0.2994 - accuracy: 0.90 - ETA: 7s - loss: 0.2997 - accuracy: 0.90 - ETA: 7s - loss: 0.2987 - accuracy: 0.90 - ETA: 7s - loss: 0.2986 - accuracy: 0.90 - ETA: 7s - loss: 0.2985 - accuracy: 0.90 - ETA: 7s - loss: 0.2987 - accuracy: 0.90 - ETA: 7s - loss: 0.2976 - accuracy: 0.90 - ETA: 7s - loss: 0.2977 - accuracy: 0.90 - ETA: 6s - loss: 0.2977 - accuracy: 0.90 - ETA: 6s - loss: 0.2970 - accuracy: 0.90 - ETA: 6s - loss: 0.2964 - accuracy: 0.90 - ETA: 6s - loss: 0.2967 - accuracy: 0.90 - ETA: 6s - loss: 0.2972 - accuracy: 0.90 - ETA: 6s - loss: 0.2963 - accuracy: 0.90 - ETA: 6s - loss: 0.2966 - accuracy: 0.90 - ETA: 5s - loss: 0.2969 - accuracy: 0.90 - ETA: 5s - loss: 0.2963 - accuracy: 0.90 - ETA: 5s - loss: 0.2966 - accuracy: 0.90 - ETA: 5s - loss: 0.2972 - accuracy: 0.90 - ETA: 5s - loss: 0.2968 - accuracy: 0.90 - ETA: 5s - loss: 0.2966 - accuracy: 0.90 - ETA: 5s - loss: 0.2960 - accuracy: 0.90 - ETA: 4s - loss: 0.2953 - accuracy: 0.90 - ETA: 4s - loss: 0.2953 - accuracy: 0.90 - ETA: 4s - loss: 0.2957 - accuracy: 0.90 - ETA: 4s - loss: 0.2959 - accuracy: 0.90 - ETA: 4s - loss: 0.2960 - accuracy: 0.90 - ETA: 4s - loss: 0.2975 - accuracy: 0.90 - ETA: 4s - loss: 0.2980 - accuracy: 0.90 - ETA: 3s - loss: 0.2976 - accuracy: 0.90 - ETA: 3s - loss: 0.2965 - accuracy: 0.90 - ETA: 3s - loss: 0.2966 - accuracy: 0.90 - ETA: 3s - loss: 0.2969 - accuracy: 0.90 - ETA: 3s - loss: 0.2965 - accuracy: 0.90 - ETA: 3s - loss: 0.2958 - accuracy: 0.90 - ETA: 3s - loss: 0.2955 - accuracy: 0.90 - ETA: 2s - loss: 0.2960 - accuracy: 0.90 - ETA: 2s - loss: 0.2968 - accuracy: 0.90 - ETA: 2s - loss: 0.2968 - accuracy: 0.90 - ETA: 2s - loss: 0.2969 - accuracy: 0.90 - ETA: 2s - loss: 0.2979 - accuracy: 0.90 - ETA: 2s - loss: 0.2981 - accuracy: 0.90 - ETA: 2s - loss: 0.2983 - accuracy: 0.90 - ETA: 1s - loss: 0.2986 - accuracy: 0.90 - ETA: 1s - loss: 0.2992 - accuracy: 0.90 - ETA: 1s - loss: 0.2994 - accuracy: 0.90 - ETA: 1s - loss: 0.2992 - accuracy: 0.90 - ETA: 1s - loss: 0.2985 - accuracy: 0.90 - ETA: 1s - loss: 0.2990 - accuracy: 0.90 - ETA: 1s - loss: 0.2990 - accuracy: 0.90 - ETA: 0s - loss: 0.2999 - accuracy: 0.90 - ETA: 0s - loss: 0.3005 - accuracy: 0.90 - ETA: 0s - loss: 0.3016 - accuracy: 0.90 - ETA: 0s - loss: 0.3011 - accuracy: 0.90 - ETA: 0s - loss: 0.3010 - accuracy: 0.90 - ETA: 0s - loss: 0.3004 - accuracy: 0.90 - ETA: 0s - loss: 0.3008 - accuracy: 0.9037\n",
      "Epoch 00031: val_accuracy did not improve from 0.91290\n",
      "84800/84800 [==============================] - 13s 151us/sample - loss: 0.3009 - accuracy: 0.9036 - val_loss: 0.3230 - val_accuracy: 0.9058\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.3440 - accuracy: 0.889 - ETA: 11s - loss: 0.3383 - accuracy: 0.890 - ETA: 11s - loss: 0.3194 - accuracy: 0.897 - ETA: 11s - loss: 0.3203 - accuracy: 0.898 - ETA: 11s - loss: 0.3055 - accuracy: 0.904 - ETA: 11s - loss: 0.3016 - accuracy: 0.905 - ETA: 11s - loss: 0.2962 - accuracy: 0.907 - ETA: 10s - loss: 0.2942 - accuracy: 0.906 - ETA: 10s - loss: 0.2922 - accuracy: 0.908 - ETA: 10s - loss: 0.2923 - accuracy: 0.907 - ETA: 10s - loss: 0.2904 - accuracy: 0.908 - ETA: 10s - loss: 0.2903 - accuracy: 0.907 - ETA: 10s - loss: 0.2925 - accuracy: 0.907 - ETA: 10s - loss: 0.2917 - accuracy: 0.908 - ETA: 9s - loss: 0.2894 - accuracy: 0.908 - ETA: 9s - loss: 0.2909 - accuracy: 0.90 - ETA: 9s - loss: 0.2921 - accuracy: 0.90 - ETA: 9s - loss: 0.2956 - accuracy: 0.90 - ETA: 9s - loss: 0.2967 - accuracy: 0.90 - ETA: 9s - loss: 0.2952 - accuracy: 0.90 - ETA: 9s - loss: 0.2964 - accuracy: 0.90 - ETA: 8s - loss: 0.2988 - accuracy: 0.90 - ETA: 8s - loss: 0.2979 - accuracy: 0.90 - ETA: 8s - loss: 0.2965 - accuracy: 0.90 - ETA: 8s - loss: 0.2965 - accuracy: 0.90 - ETA: 8s - loss: 0.2977 - accuracy: 0.90 - ETA: 8s - loss: 0.2993 - accuracy: 0.90 - ETA: 8s - loss: 0.2981 - accuracy: 0.90 - ETA: 7s - loss: 0.2974 - accuracy: 0.90 - ETA: 7s - loss: 0.2972 - accuracy: 0.90 - ETA: 7s - loss: 0.2973 - accuracy: 0.90 - ETA: 7s - loss: 0.2968 - accuracy: 0.90 - ETA: 7s - loss: 0.2968 - accuracy: 0.90 - ETA: 7s - loss: 0.2974 - accuracy: 0.90 - ETA: 7s - loss: 0.2950 - accuracy: 0.90 - ETA: 6s - loss: 0.2946 - accuracy: 0.90 - ETA: 6s - loss: 0.2938 - accuracy: 0.90 - ETA: 6s - loss: 0.2941 - accuracy: 0.90 - ETA: 6s - loss: 0.2943 - accuracy: 0.90 - ETA: 6s - loss: 0.2955 - accuracy: 0.90 - ETA: 6s - loss: 0.2952 - accuracy: 0.90 - ETA: 6s - loss: 0.2962 - accuracy: 0.90 - ETA: 6s - loss: 0.2954 - accuracy: 0.90 - ETA: 5s - loss: 0.2958 - accuracy: 0.90 - ETA: 5s - loss: 0.2967 - accuracy: 0.90 - ETA: 5s - loss: 0.2969 - accuracy: 0.90 - ETA: 5s - loss: 0.2977 - accuracy: 0.90 - ETA: 5s - loss: 0.2977 - accuracy: 0.90 - ETA: 5s - loss: 0.2980 - accuracy: 0.90 - ETA: 5s - loss: 0.2981 - accuracy: 0.90 - ETA: 4s - loss: 0.2977 - accuracy: 0.90 - ETA: 4s - loss: 0.2979 - accuracy: 0.90 - ETA: 4s - loss: 0.2979 - accuracy: 0.90 - ETA: 4s - loss: 0.2980 - accuracy: 0.90 - ETA: 4s - loss: 0.2979 - accuracy: 0.90 - ETA: 4s - loss: 0.2984 - accuracy: 0.90 - ETA: 4s - loss: 0.2972 - accuracy: 0.90 - ETA: 3s - loss: 0.2977 - accuracy: 0.90 - ETA: 3s - loss: 0.2976 - accuracy: 0.90 - ETA: 3s - loss: 0.2968 - accuracy: 0.90 - ETA: 3s - loss: 0.2969 - accuracy: 0.90 - ETA: 3s - loss: 0.2969 - accuracy: 0.90 - ETA: 3s - loss: 0.2971 - accuracy: 0.90 - ETA: 3s - loss: 0.2983 - accuracy: 0.90 - ETA: 2s - loss: 0.2985 - accuracy: 0.90 - ETA: 2s - loss: 0.2988 - accuracy: 0.90 - ETA: 2s - loss: 0.2993 - accuracy: 0.90 - ETA: 2s - loss: 0.2999 - accuracy: 0.90 - ETA: 2s - loss: 0.2995 - accuracy: 0.90 - ETA: 2s - loss: 0.3001 - accuracy: 0.90 - ETA: 1s - loss: 0.3009 - accuracy: 0.90 - ETA: 1s - loss: 0.3010 - accuracy: 0.90 - ETA: 1s - loss: 0.3011 - accuracy: 0.90 - ETA: 1s - loss: 0.3015 - accuracy: 0.90 - ETA: 1s - loss: 0.3015 - accuracy: 0.90 - ETA: 1s - loss: 0.3013 - accuracy: 0.90 - ETA: 1s - loss: 0.3008 - accuracy: 0.90 - ETA: 0s - loss: 0.3017 - accuracy: 0.90 - ETA: 0s - loss: 0.3016 - accuracy: 0.90 - ETA: 0s - loss: 0.3018 - accuracy: 0.90 - ETA: 0s - loss: 0.3016 - accuracy: 0.90 - ETA: 0s - loss: 0.3019 - accuracy: 0.90 - ETA: 0s - loss: 0.3020 - accuracy: 0.90 - ETA: 0s - loss: 0.3019 - accuracy: 0.9045\n",
      "Epoch 00032: val_accuracy did not improve from 0.91290\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.3020 - accuracy: 0.9044 - val_loss: 0.3491 - val_accuracy: 0.9031\n",
      "Epoch 33/100\n",
      "84000/84800 [============================>.] - ETA: 12s - loss: 0.2624 - accuracy: 0.917 - ETA: 11s - loss: 0.2873 - accuracy: 0.909 - ETA: 11s - loss: 0.2908 - accuracy: 0.908 - ETA: 11s - loss: 0.2849 - accuracy: 0.909 - ETA: 11s - loss: 0.2845 - accuracy: 0.909 - ETA: 11s - loss: 0.2821 - accuracy: 0.909 - ETA: 11s - loss: 0.2819 - accuracy: 0.908 - ETA: 11s - loss: 0.2806 - accuracy: 0.909 - ETA: 10s - loss: 0.2830 - accuracy: 0.909 - ETA: 10s - loss: 0.2802 - accuracy: 0.909 - ETA: 10s - loss: 0.2829 - accuracy: 0.908 - ETA: 10s - loss: 0.2828 - accuracy: 0.907 - ETA: 10s - loss: 0.2859 - accuracy: 0.906 - ETA: 10s - loss: 0.2826 - accuracy: 0.907 - ETA: 10s - loss: 0.2824 - accuracy: 0.906 - ETA: 10s - loss: 0.2847 - accuracy: 0.906 - ETA: 9s - loss: 0.2850 - accuracy: 0.905 - ETA: 9s - loss: 0.2842 - accuracy: 0.90 - ETA: 9s - loss: 0.2853 - accuracy: 0.90 - ETA: 9s - loss: 0.2852 - accuracy: 0.90 - ETA: 9s - loss: 0.2854 - accuracy: 0.90 - ETA: 9s - loss: 0.2855 - accuracy: 0.90 - ETA: 9s - loss: 0.2859 - accuracy: 0.90 - ETA: 8s - loss: 0.2845 - accuracy: 0.90 - ETA: 8s - loss: 0.2838 - accuracy: 0.90 - ETA: 8s - loss: 0.2834 - accuracy: 0.90 - ETA: 8s - loss: 0.2831 - accuracy: 0.90 - ETA: 8s - loss: 0.2829 - accuracy: 0.90 - ETA: 8s - loss: 0.2831 - accuracy: 0.90 - ETA: 7s - loss: 0.2835 - accuracy: 0.90 - ETA: 7s - loss: 0.2845 - accuracy: 0.90 - ETA: 7s - loss: 0.2860 - accuracy: 0.90 - ETA: 7s - loss: 0.2867 - accuracy: 0.90 - ETA: 7s - loss: 0.2866 - accuracy: 0.90 - ETA: 7s - loss: 0.2885 - accuracy: 0.90 - ETA: 7s - loss: 0.2893 - accuracy: 0.90 - ETA: 6s - loss: 0.2907 - accuracy: 0.90 - ETA: 6s - loss: 0.2915 - accuracy: 0.90 - ETA: 6s - loss: 0.2914 - accuracy: 0.90 - ETA: 6s - loss: 0.2908 - accuracy: 0.90 - ETA: 6s - loss: 0.2907 - accuracy: 0.90 - ETA: 6s - loss: 0.2909 - accuracy: 0.90 - ETA: 6s - loss: 0.2905 - accuracy: 0.90 - ETA: 5s - loss: 0.2900 - accuracy: 0.90 - ETA: 5s - loss: 0.2898 - accuracy: 0.90 - ETA: 5s - loss: 0.2895 - accuracy: 0.90 - ETA: 5s - loss: 0.2894 - accuracy: 0.90 - ETA: 5s - loss: 0.2916 - accuracy: 0.90 - ETA: 5s - loss: 0.2927 - accuracy: 0.90 - ETA: 5s - loss: 0.2943 - accuracy: 0.90 - ETA: 4s - loss: 0.2943 - accuracy: 0.90 - ETA: 4s - loss: 0.2942 - accuracy: 0.90 - ETA: 4s - loss: 0.2939 - accuracy: 0.90 - ETA: 4s - loss: 0.2940 - accuracy: 0.90 - ETA: 4s - loss: 0.2939 - accuracy: 0.90 - ETA: 4s - loss: 0.2946 - accuracy: 0.90 - ETA: 4s - loss: 0.2943 - accuracy: 0.90 - ETA: 3s - loss: 0.2935 - accuracy: 0.90 - ETA: 3s - loss: 0.2936 - accuracy: 0.90 - ETA: 3s - loss: 0.2932 - accuracy: 0.90 - ETA: 3s - loss: 0.2934 - accuracy: 0.90 - ETA: 3s - loss: 0.2927 - accuracy: 0.90 - ETA: 3s - loss: 0.2920 - accuracy: 0.90 - ETA: 2s - loss: 0.2919 - accuracy: 0.90 - ETA: 2s - loss: 0.2922 - accuracy: 0.90 - ETA: 2s - loss: 0.2915 - accuracy: 0.90 - ETA: 2s - loss: 0.2912 - accuracy: 0.90 - ETA: 2s - loss: 0.2913 - accuracy: 0.90 - ETA: 2s - loss: 0.2913 - accuracy: 0.90 - ETA: 2s - loss: 0.2911 - accuracy: 0.90 - ETA: 1s - loss: 0.2907 - accuracy: 0.90 - ETA: 1s - loss: 0.2902 - accuracy: 0.90 - ETA: 1s - loss: 0.2902 - accuracy: 0.90 - ETA: 1s - loss: 0.2911 - accuracy: 0.90 - ETA: 1s - loss: 0.2914 - accuracy: 0.90 - ETA: 1s - loss: 0.2918 - accuracy: 0.90 - ETA: 1s - loss: 0.2920 - accuracy: 0.90 - ETA: 0s - loss: 0.2921 - accuracy: 0.90 - ETA: 0s - loss: 0.2922 - accuracy: 0.90 - ETA: 0s - loss: 0.2925 - accuracy: 0.90 - ETA: 0s - loss: 0.2932 - accuracy: 0.90 - ETA: 0s - loss: 0.2942 - accuracy: 0.90 - ETA: 0s - loss: 0.2945 - accuracy: 0.90 - ETA: 0s - loss: 0.2948 - accuracy: 0.9054\n",
      "Epoch 00033: val_accuracy did not improve from 0.91290\n",
      "84800/84800 [==============================] - 13s 152us/sample - loss: 0.2952 - accuracy: 0.9053 - val_loss: 0.3274 - val_accuracy: 0.9071\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 12s - loss: 0.3299 - accuracy: 0.901 - ETA: 12s - loss: 0.3252 - accuracy: 0.892 - ETA: 12s - loss: 0.3108 - accuracy: 0.896 - ETA: 12s - loss: 0.3008 - accuracy: 0.900 - ETA: 12s - loss: 0.2992 - accuracy: 0.900 - ETA: 11s - loss: 0.3037 - accuracy: 0.900 - ETA: 11s - loss: 0.2963 - accuracy: 0.902 - ETA: 11s - loss: 0.2925 - accuracy: 0.903 - ETA: 11s - loss: 0.2945 - accuracy: 0.902 - ETA: 10s - loss: 0.2930 - accuracy: 0.903 - ETA: 10s - loss: 0.2916 - accuracy: 0.903 - ETA: 10s - loss: 0.2912 - accuracy: 0.903 - ETA: 10s - loss: 0.2893 - accuracy: 0.904 - ETA: 10s - loss: 0.2884 - accuracy: 0.905 - ETA: 10s - loss: 0.2871 - accuracy: 0.905 - ETA: 9s - loss: 0.2873 - accuracy: 0.905 - ETA: 9s - loss: 0.2870 - accuracy: 0.90 - ETA: 9s - loss: 0.2884 - accuracy: 0.90 - ETA: 9s - loss: 0.2911 - accuracy: 0.90 - ETA: 9s - loss: 0.2901 - accuracy: 0.90 - ETA: 9s - loss: 0.2871 - accuracy: 0.90 - ETA: 9s - loss: 0.2859 - accuracy: 0.90 - ETA: 8s - loss: 0.2865 - accuracy: 0.90 - ETA: 8s - loss: 0.2859 - accuracy: 0.90 - ETA: 8s - loss: 0.2869 - accuracy: 0.90 - ETA: 8s - loss: 0.2877 - accuracy: 0.90 - ETA: 8s - loss: 0.2861 - accuracy: 0.90 - ETA: 8s - loss: 0.2860 - accuracy: 0.90 - ETA: 7s - loss: 0.2862 - accuracy: 0.90 - ETA: 7s - loss: 0.2866 - accuracy: 0.90 - ETA: 7s - loss: 0.2878 - accuracy: 0.90 - ETA: 7s - loss: 0.2890 - accuracy: 0.90 - ETA: 7s - loss: 0.2883 - accuracy: 0.90 - ETA: 7s - loss: 0.2885 - accuracy: 0.90 - ETA: 7s - loss: 0.2880 - accuracy: 0.90 - ETA: 6s - loss: 0.2888 - accuracy: 0.90 - ETA: 6s - loss: 0.2875 - accuracy: 0.90 - ETA: 6s - loss: 0.2885 - accuracy: 0.90 - ETA: 6s - loss: 0.2896 - accuracy: 0.90 - ETA: 6s - loss: 0.2893 - accuracy: 0.90 - ETA: 6s - loss: 0.2897 - accuracy: 0.90 - ETA: 6s - loss: 0.2895 - accuracy: 0.90 - ETA: 5s - loss: 0.2897 - accuracy: 0.90 - ETA: 5s - loss: 0.2891 - accuracy: 0.90 - ETA: 5s - loss: 0.2890 - accuracy: 0.90 - ETA: 5s - loss: 0.2895 - accuracy: 0.90 - ETA: 5s - loss: 0.2892 - accuracy: 0.90 - ETA: 5s - loss: 0.2892 - accuracy: 0.90 - ETA: 5s - loss: 0.2895 - accuracy: 0.90 - ETA: 4s - loss: 0.2900 - accuracy: 0.90 - ETA: 4s - loss: 0.2891 - accuracy: 0.90 - ETA: 4s - loss: 0.2889 - accuracy: 0.90 - ETA: 4s - loss: 0.2896 - accuracy: 0.90 - ETA: 4s - loss: 0.2894 - accuracy: 0.90 - ETA: 4s - loss: 0.2890 - accuracy: 0.90 - ETA: 4s - loss: 0.2890 - accuracy: 0.90 - ETA: 3s - loss: 0.2894 - accuracy: 0.90 - ETA: 3s - loss: 0.2898 - accuracy: 0.90 - ETA: 3s - loss: 0.2893 - accuracy: 0.90 - ETA: 3s - loss: 0.2895 - accuracy: 0.90 - ETA: 3s - loss: 0.2903 - accuracy: 0.90 - ETA: 3s - loss: 0.2909 - accuracy: 0.90 - ETA: 3s - loss: 0.2913 - accuracy: 0.90 - ETA: 2s - loss: 0.2914 - accuracy: 0.90 - ETA: 2s - loss: 0.2917 - accuracy: 0.90 - ETA: 2s - loss: 0.2920 - accuracy: 0.90 - ETA: 2s - loss: 0.2915 - accuracy: 0.90 - ETA: 2s - loss: 0.2910 - accuracy: 0.90 - ETA: 2s - loss: 0.2914 - accuracy: 0.90 - ETA: 2s - loss: 0.2912 - accuracy: 0.90 - ETA: 1s - loss: 0.2909 - accuracy: 0.90 - ETA: 1s - loss: 0.2906 - accuracy: 0.90 - ETA: 1s - loss: 0.2905 - accuracy: 0.90 - ETA: 1s - loss: 0.2905 - accuracy: 0.90 - ETA: 1s - loss: 0.2915 - accuracy: 0.90 - ETA: 1s - loss: 0.2922 - accuracy: 0.90 - ETA: 1s - loss: 0.2925 - accuracy: 0.90 - ETA: 0s - loss: 0.2928 - accuracy: 0.90 - ETA: 0s - loss: 0.2926 - accuracy: 0.90 - ETA: 0s - loss: 0.2926 - accuracy: 0.90 - ETA: 0s - loss: 0.2930 - accuracy: 0.90 - ETA: 0s - loss: 0.2933 - accuracy: 0.90 - ETA: 0s - loss: 0.2933 - accuracy: 0.90 - ETA: 0s - loss: 0.2933 - accuracy: 0.9058\n",
      "Epoch 00034: val_accuracy did not improve from 0.91290\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.2933 - accuracy: 0.9059 - val_loss: 0.3092 - val_accuracy: 0.9106\n",
      "Epoch 35/100\n",
      "84000/84800 [============================>.] - ETA: 12s - loss: 0.2611 - accuracy: 0.917 - ETA: 12s - loss: 0.2675 - accuracy: 0.914 - ETA: 12s - loss: 0.2908 - accuracy: 0.906 - ETA: 12s - loss: 0.2935 - accuracy: 0.905 - ETA: 11s - loss: 0.2931 - accuracy: 0.906 - ETA: 11s - loss: 0.2893 - accuracy: 0.907 - ETA: 11s - loss: 0.2858 - accuracy: 0.907 - ETA: 11s - loss: 0.2842 - accuracy: 0.908 - ETA: 11s - loss: 0.2763 - accuracy: 0.910 - ETA: 10s - loss: 0.2722 - accuracy: 0.912 - ETA: 10s - loss: 0.2747 - accuracy: 0.910 - ETA: 10s - loss: 0.2725 - accuracy: 0.911 - ETA: 10s - loss: 0.2724 - accuracy: 0.912 - ETA: 10s - loss: 0.2724 - accuracy: 0.911 - ETA: 10s - loss: 0.2707 - accuracy: 0.912 - ETA: 9s - loss: 0.2723 - accuracy: 0.911 - ETA: 9s - loss: 0.2710 - accuracy: 0.91 - ETA: 9s - loss: 0.2726 - accuracy: 0.91 - ETA: 9s - loss: 0.2714 - accuracy: 0.91 - ETA: 9s - loss: 0.2718 - accuracy: 0.91 - ETA: 9s - loss: 0.2711 - accuracy: 0.91 - ETA: 9s - loss: 0.2698 - accuracy: 0.91 - ETA: 8s - loss: 0.2709 - accuracy: 0.91 - ETA: 8s - loss: 0.2702 - accuracy: 0.91 - ETA: 8s - loss: 0.2715 - accuracy: 0.91 - ETA: 8s - loss: 0.2701 - accuracy: 0.91 - ETA: 8s - loss: 0.2691 - accuracy: 0.91 - ETA: 8s - loss: 0.2685 - accuracy: 0.91 - ETA: 8s - loss: 0.2692 - accuracy: 0.91 - ETA: 7s - loss: 0.2708 - accuracy: 0.91 - ETA: 7s - loss: 0.2719 - accuracy: 0.91 - ETA: 7s - loss: 0.2737 - accuracy: 0.91 - ETA: 7s - loss: 0.2743 - accuracy: 0.91 - ETA: 7s - loss: 0.2744 - accuracy: 0.91 - ETA: 7s - loss: 0.2744 - accuracy: 0.91 - ETA: 6s - loss: 0.2745 - accuracy: 0.91 - ETA: 6s - loss: 0.2755 - accuracy: 0.91 - ETA: 6s - loss: 0.2764 - accuracy: 0.91 - ETA: 6s - loss: 0.2761 - accuracy: 0.91 - ETA: 6s - loss: 0.2761 - accuracy: 0.91 - ETA: 6s - loss: 0.2759 - accuracy: 0.91 - ETA: 6s - loss: 0.2763 - accuracy: 0.91 - ETA: 5s - loss: 0.2765 - accuracy: 0.91 - ETA: 5s - loss: 0.2775 - accuracy: 0.91 - ETA: 5s - loss: 0.2779 - accuracy: 0.91 - ETA: 5s - loss: 0.2787 - accuracy: 0.91 - ETA: 5s - loss: 0.2786 - accuracy: 0.91 - ETA: 5s - loss: 0.2788 - accuracy: 0.91 - ETA: 5s - loss: 0.2790 - accuracy: 0.91 - ETA: 4s - loss: 0.2784 - accuracy: 0.91 - ETA: 4s - loss: 0.2781 - accuracy: 0.91 - ETA: 4s - loss: 0.2780 - accuracy: 0.91 - ETA: 4s - loss: 0.2781 - accuracy: 0.91 - ETA: 4s - loss: 0.2786 - accuracy: 0.91 - ETA: 4s - loss: 0.2792 - accuracy: 0.91 - ETA: 4s - loss: 0.2798 - accuracy: 0.91 - ETA: 3s - loss: 0.2805 - accuracy: 0.90 - ETA: 3s - loss: 0.2807 - accuracy: 0.90 - ETA: 3s - loss: 0.2800 - accuracy: 0.90 - ETA: 3s - loss: 0.2803 - accuracy: 0.90 - ETA: 3s - loss: 0.2800 - accuracy: 0.90 - ETA: 3s - loss: 0.2798 - accuracy: 0.90 - ETA: 3s - loss: 0.2795 - accuracy: 0.90 - ETA: 2s - loss: 0.2799 - accuracy: 0.90 - ETA: 2s - loss: 0.2803 - accuracy: 0.90 - ETA: 2s - loss: 0.2805 - accuracy: 0.90 - ETA: 2s - loss: 0.2810 - accuracy: 0.90 - ETA: 2s - loss: 0.2816 - accuracy: 0.90 - ETA: 2s - loss: 0.2820 - accuracy: 0.90 - ETA: 2s - loss: 0.2815 - accuracy: 0.90 - ETA: 1s - loss: 0.2812 - accuracy: 0.90 - ETA: 1s - loss: 0.2809 - accuracy: 0.90 - ETA: 1s - loss: 0.2809 - accuracy: 0.90 - ETA: 1s - loss: 0.2810 - accuracy: 0.90 - ETA: 1s - loss: 0.2810 - accuracy: 0.90 - ETA: 1s - loss: 0.2811 - accuracy: 0.90 - ETA: 1s - loss: 0.2819 - accuracy: 0.90 - ETA: 0s - loss: 0.2813 - accuracy: 0.90 - ETA: 0s - loss: 0.2814 - accuracy: 0.90 - ETA: 0s - loss: 0.2815 - accuracy: 0.90 - ETA: 0s - loss: 0.2814 - accuracy: 0.90 - ETA: 0s - loss: 0.2818 - accuracy: 0.90 - ETA: 0s - loss: 0.2823 - accuracy: 0.90 - ETA: 0s - loss: 0.2821 - accuracy: 0.9091\n",
      "Epoch 00035: val_accuracy improved from 0.91290 to 0.91462, saving model to ry_best_model.hdf5\n",
      "84800/84800 [==============================] - 13s 153us/sample - loss: 0.2824 - accuracy: 0.9090 - val_loss: 0.3071 - val_accuracy: 0.9146\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2690 - accuracy: 0.917 - ETA: 11s - loss: 0.2871 - accuracy: 0.910 - ETA: 11s - loss: 0.2997 - accuracy: 0.906 - ETA: 11s - loss: 0.2978 - accuracy: 0.907 - ETA: 11s - loss: 0.2915 - accuracy: 0.909 - ETA: 11s - loss: 0.2946 - accuracy: 0.907 - ETA: 11s - loss: 0.2851 - accuracy: 0.910 - ETA: 11s - loss: 0.2844 - accuracy: 0.910 - ETA: 10s - loss: 0.2823 - accuracy: 0.911 - ETA: 10s - loss: 0.2825 - accuracy: 0.911 - ETA: 10s - loss: 0.2830 - accuracy: 0.910 - ETA: 10s - loss: 0.2838 - accuracy: 0.909 - ETA: 10s - loss: 0.2871 - accuracy: 0.908 - ETA: 10s - loss: 0.2841 - accuracy: 0.908 - ETA: 10s - loss: 0.2815 - accuracy: 0.909 - ETA: 9s - loss: 0.2814 - accuracy: 0.909 - ETA: 9s - loss: 0.2804 - accuracy: 0.90 - ETA: 9s - loss: 0.2786 - accuracy: 0.90 - ETA: 9s - loss: 0.2798 - accuracy: 0.90 - ETA: 9s - loss: 0.2795 - accuracy: 0.90 - ETA: 9s - loss: 0.2793 - accuracy: 0.90 - ETA: 9s - loss: 0.2801 - accuracy: 0.90 - ETA: 8s - loss: 0.2818 - accuracy: 0.90 - ETA: 8s - loss: 0.2802 - accuracy: 0.90 - ETA: 8s - loss: 0.2805 - accuracy: 0.90 - ETA: 8s - loss: 0.2811 - accuracy: 0.90 - ETA: 8s - loss: 0.2804 - accuracy: 0.90 - ETA: 8s - loss: 0.2816 - accuracy: 0.90 - ETA: 8s - loss: 0.2823 - accuracy: 0.90 - ETA: 7s - loss: 0.2837 - accuracy: 0.90 - ETA: 7s - loss: 0.2839 - accuracy: 0.90 - ETA: 7s - loss: 0.2836 - accuracy: 0.90 - ETA: 7s - loss: 0.2834 - accuracy: 0.90 - ETA: 7s - loss: 0.2835 - accuracy: 0.90 - ETA: 7s - loss: 0.2828 - accuracy: 0.90 - ETA: 7s - loss: 0.2841 - accuracy: 0.90 - ETA: 6s - loss: 0.2843 - accuracy: 0.90 - ETA: 6s - loss: 0.2837 - accuracy: 0.90 - ETA: 6s - loss: 0.2830 - accuracy: 0.90 - ETA: 6s - loss: 0.2823 - accuracy: 0.90 - ETA: 6s - loss: 0.2820 - accuracy: 0.90 - ETA: 6s - loss: 0.2824 - accuracy: 0.90 - ETA: 6s - loss: 0.2821 - accuracy: 0.90 - ETA: 5s - loss: 0.2819 - accuracy: 0.90 - ETA: 5s - loss: 0.2818 - accuracy: 0.90 - ETA: 5s - loss: 0.2825 - accuracy: 0.90 - ETA: 5s - loss: 0.2828 - accuracy: 0.90 - ETA: 5s - loss: 0.2824 - accuracy: 0.90 - ETA: 5s - loss: 0.2819 - accuracy: 0.90 - ETA: 5s - loss: 0.2820 - accuracy: 0.90 - ETA: 4s - loss: 0.2822 - accuracy: 0.90 - ETA: 4s - loss: 0.2824 - accuracy: 0.90 - ETA: 4s - loss: 0.2827 - accuracy: 0.90 - ETA: 4s - loss: 0.2820 - accuracy: 0.90 - ETA: 4s - loss: 0.2813 - accuracy: 0.90 - ETA: 4s - loss: 0.2809 - accuracy: 0.90 - ETA: 4s - loss: 0.2807 - accuracy: 0.90 - ETA: 3s - loss: 0.2802 - accuracy: 0.91 - ETA: 3s - loss: 0.2806 - accuracy: 0.90 - ETA: 3s - loss: 0.2807 - accuracy: 0.91 - ETA: 3s - loss: 0.2806 - accuracy: 0.91 - ETA: 3s - loss: 0.2801 - accuracy: 0.91 - ETA: 3s - loss: 0.2802 - accuracy: 0.91 - ETA: 3s - loss: 0.2806 - accuracy: 0.91 - ETA: 2s - loss: 0.2810 - accuracy: 0.90 - ETA: 2s - loss: 0.2807 - accuracy: 0.90 - ETA: 2s - loss: 0.2815 - accuracy: 0.90 - ETA: 2s - loss: 0.2817 - accuracy: 0.90 - ETA: 2s - loss: 0.2814 - accuracy: 0.90 - ETA: 2s - loss: 0.2813 - accuracy: 0.90 - ETA: 2s - loss: 0.2813 - accuracy: 0.90 - ETA: 1s - loss: 0.2820 - accuracy: 0.90 - ETA: 1s - loss: 0.2821 - accuracy: 0.90 - ETA: 1s - loss: 0.2824 - accuracy: 0.90 - ETA: 1s - loss: 0.2831 - accuracy: 0.90 - ETA: 1s - loss: 0.2827 - accuracy: 0.90 - ETA: 1s - loss: 0.2827 - accuracy: 0.90 - ETA: 0s - loss: 0.2827 - accuracy: 0.90 - ETA: 0s - loss: 0.2824 - accuracy: 0.90 - ETA: 0s - loss: 0.2826 - accuracy: 0.90 - ETA: 0s - loss: 0.2827 - accuracy: 0.90 - ETA: 0s - loss: 0.2829 - accuracy: 0.90 - ETA: 0s - loss: 0.2830 - accuracy: 0.90 - ETA: 0s - loss: 0.2831 - accuracy: 0.9090\n",
      "Epoch 00036: val_accuracy did not improve from 0.91462\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.2835 - accuracy: 0.9089 - val_loss: 0.3013 - val_accuracy: 0.9120\n",
      "Epoch 37/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2470 - accuracy: 0.914 - ETA: 11s - loss: 0.2646 - accuracy: 0.912 - ETA: 11s - loss: 0.2845 - accuracy: 0.907 - ETA: 11s - loss: 0.2914 - accuracy: 0.906 - ETA: 11s - loss: 0.2872 - accuracy: 0.905 - ETA: 11s - loss: 0.2790 - accuracy: 0.907 - ETA: 11s - loss: 0.2784 - accuracy: 0.908 - ETA: 10s - loss: 0.2748 - accuracy: 0.909 - ETA: 10s - loss: 0.2744 - accuracy: 0.910 - ETA: 10s - loss: 0.2699 - accuracy: 0.911 - ETA: 10s - loss: 0.2707 - accuracy: 0.910 - ETA: 10s - loss: 0.2684 - accuracy: 0.911 - ETA: 10s - loss: 0.2671 - accuracy: 0.912 - ETA: 10s - loss: 0.2696 - accuracy: 0.911 - ETA: 10s - loss: 0.2687 - accuracy: 0.911 - ETA: 9s - loss: 0.2693 - accuracy: 0.911 - ETA: 9s - loss: 0.2717 - accuracy: 0.91 - ETA: 9s - loss: 0.2778 - accuracy: 0.90 - ETA: 9s - loss: 0.2790 - accuracy: 0.90 - ETA: 9s - loss: 0.2806 - accuracy: 0.90 - ETA: 9s - loss: 0.2779 - accuracy: 0.91 - ETA: 8s - loss: 0.2781 - accuracy: 0.91 - ETA: 8s - loss: 0.2777 - accuracy: 0.91 - ETA: 8s - loss: 0.2786 - accuracy: 0.91 - ETA: 8s - loss: 0.2769 - accuracy: 0.91 - ETA: 8s - loss: 0.2775 - accuracy: 0.91 - ETA: 8s - loss: 0.2790 - accuracy: 0.90 - ETA: 8s - loss: 0.2792 - accuracy: 0.90 - ETA: 7s - loss: 0.2818 - accuracy: 0.90 - ETA: 7s - loss: 0.2825 - accuracy: 0.90 - ETA: 7s - loss: 0.2845 - accuracy: 0.90 - ETA: 7s - loss: 0.2846 - accuracy: 0.90 - ETA: 7s - loss: 0.2846 - accuracy: 0.90 - ETA: 7s - loss: 0.2827 - accuracy: 0.90 - ETA: 7s - loss: 0.2832 - accuracy: 0.90 - ETA: 7s - loss: 0.2840 - accuracy: 0.90 - ETA: 6s - loss: 0.2831 - accuracy: 0.90 - ETA: 6s - loss: 0.2833 - accuracy: 0.90 - ETA: 6s - loss: 0.2821 - accuracy: 0.90 - ETA: 6s - loss: 0.2819 - accuracy: 0.90 - ETA: 6s - loss: 0.2811 - accuracy: 0.90 - ETA: 6s - loss: 0.2810 - accuracy: 0.90 - ETA: 6s - loss: 0.2797 - accuracy: 0.91 - ETA: 5s - loss: 0.2791 - accuracy: 0.91 - ETA: 5s - loss: 0.2785 - accuracy: 0.91 - ETA: 5s - loss: 0.2793 - accuracy: 0.91 - ETA: 5s - loss: 0.2791 - accuracy: 0.91 - ETA: 5s - loss: 0.2790 - accuracy: 0.91 - ETA: 5s - loss: 0.2782 - accuracy: 0.91 - ETA: 5s - loss: 0.2778 - accuracy: 0.91 - ETA: 4s - loss: 0.2784 - accuracy: 0.91 - ETA: 4s - loss: 0.2785 - accuracy: 0.91 - ETA: 4s - loss: 0.2782 - accuracy: 0.91 - ETA: 4s - loss: 0.2788 - accuracy: 0.91 - ETA: 4s - loss: 0.2786 - accuracy: 0.91 - ETA: 4s - loss: 0.2789 - accuracy: 0.91 - ETA: 4s - loss: 0.2790 - accuracy: 0.91 - ETA: 3s - loss: 0.2789 - accuracy: 0.91 - ETA: 3s - loss: 0.2797 - accuracy: 0.91 - ETA: 3s - loss: 0.2792 - accuracy: 0.91 - ETA: 3s - loss: 0.2785 - accuracy: 0.91 - ETA: 3s - loss: 0.2787 - accuracy: 0.91 - ETA: 3s - loss: 0.2788 - accuracy: 0.91 - ETA: 3s - loss: 0.2783 - accuracy: 0.91 - ETA: 2s - loss: 0.2783 - accuracy: 0.91 - ETA: 2s - loss: 0.2786 - accuracy: 0.91 - ETA: 2s - loss: 0.2787 - accuracy: 0.91 - ETA: 2s - loss: 0.2784 - accuracy: 0.91 - ETA: 2s - loss: 0.2786 - accuracy: 0.91 - ETA: 2s - loss: 0.2782 - accuracy: 0.91 - ETA: 2s - loss: 0.2785 - accuracy: 0.91 - ETA: 1s - loss: 0.2788 - accuracy: 0.91 - ETA: 1s - loss: 0.2789 - accuracy: 0.91 - ETA: 1s - loss: 0.2792 - accuracy: 0.91 - ETA: 1s - loss: 0.2795 - accuracy: 0.91 - ETA: 1s - loss: 0.2799 - accuracy: 0.91 - ETA: 1s - loss: 0.2810 - accuracy: 0.90 - ETA: 0s - loss: 0.2808 - accuracy: 0.91 - ETA: 0s - loss: 0.2815 - accuracy: 0.90 - ETA: 0s - loss: 0.2811 - accuracy: 0.91 - ETA: 0s - loss: 0.2809 - accuracy: 0.91 - ETA: 0s - loss: 0.2810 - accuracy: 0.91 - ETA: 0s - loss: 0.2808 - accuracy: 0.91 - ETA: 0s - loss: 0.2800 - accuracy: 0.9105\n",
      "Epoch 00037: val_accuracy did not improve from 0.91462\n",
      "84800/84800 [==============================] - 13s 156us/sample - loss: 0.2800 - accuracy: 0.9105 - val_loss: 0.3143 - val_accuracy: 0.9113\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2525 - accuracy: 0.918 - ETA: 11s - loss: 0.2703 - accuracy: 0.915 - ETA: 11s - loss: 0.2715 - accuracy: 0.915 - ETA: 11s - loss: 0.2762 - accuracy: 0.908 - ETA: 11s - loss: 0.2866 - accuracy: 0.905 - ETA: 11s - loss: 0.2891 - accuracy: 0.905 - ETA: 10s - loss: 0.2924 - accuracy: 0.905 - ETA: 10s - loss: 0.2849 - accuracy: 0.908 - ETA: 10s - loss: 0.2859 - accuracy: 0.908 - ETA: 10s - loss: 0.2816 - accuracy: 0.909 - ETA: 10s - loss: 0.2777 - accuracy: 0.911 - ETA: 10s - loss: 0.2772 - accuracy: 0.912 - ETA: 10s - loss: 0.2766 - accuracy: 0.912 - ETA: 10s - loss: 0.2773 - accuracy: 0.912 - ETA: 9s - loss: 0.2775 - accuracy: 0.911 - ETA: 9s - loss: 0.2764 - accuracy: 0.91 - ETA: 9s - loss: 0.2723 - accuracy: 0.91 - ETA: 9s - loss: 0.2724 - accuracy: 0.91 - ETA: 9s - loss: 0.2740 - accuracy: 0.91 - ETA: 9s - loss: 0.2727 - accuracy: 0.91 - ETA: 9s - loss: 0.2727 - accuracy: 0.91 - ETA: 8s - loss: 0.2721 - accuracy: 0.91 - ETA: 8s - loss: 0.2734 - accuracy: 0.91 - ETA: 8s - loss: 0.2732 - accuracy: 0.91 - ETA: 8s - loss: 0.2725 - accuracy: 0.91 - ETA: 8s - loss: 0.2713 - accuracy: 0.91 - ETA: 8s - loss: 0.2721 - accuracy: 0.91 - ETA: 8s - loss: 0.2733 - accuracy: 0.91 - ETA: 7s - loss: 0.2733 - accuracy: 0.91 - ETA: 7s - loss: 0.2727 - accuracy: 0.91 - ETA: 7s - loss: 0.2742 - accuracy: 0.91 - ETA: 7s - loss: 0.2741 - accuracy: 0.91 - ETA: 7s - loss: 0.2736 - accuracy: 0.91 - ETA: 7s - loss: 0.2747 - accuracy: 0.91 - ETA: 7s - loss: 0.2741 - accuracy: 0.91 - ETA: 7s - loss: 0.2742 - accuracy: 0.91 - ETA: 6s - loss: 0.2736 - accuracy: 0.91 - ETA: 6s - loss: 0.2732 - accuracy: 0.91 - ETA: 6s - loss: 0.2730 - accuracy: 0.91 - ETA: 6s - loss: 0.2727 - accuracy: 0.91 - ETA: 6s - loss: 0.2728 - accuracy: 0.91 - ETA: 6s - loss: 0.2735 - accuracy: 0.91 - ETA: 6s - loss: 0.2732 - accuracy: 0.91 - ETA: 5s - loss: 0.2726 - accuracy: 0.91 - ETA: 5s - loss: 0.2722 - accuracy: 0.91 - ETA: 5s - loss: 0.2737 - accuracy: 0.91 - ETA: 5s - loss: 0.2730 - accuracy: 0.91 - ETA: 5s - loss: 0.2728 - accuracy: 0.91 - ETA: 5s - loss: 0.2733 - accuracy: 0.91 - ETA: 5s - loss: 0.2735 - accuracy: 0.91 - ETA: 4s - loss: 0.2743 - accuracy: 0.91 - ETA: 4s - loss: 0.2750 - accuracy: 0.91 - ETA: 4s - loss: 0.2757 - accuracy: 0.91 - ETA: 4s - loss: 0.2755 - accuracy: 0.91 - ETA: 4s - loss: 0.2758 - accuracy: 0.91 - ETA: 4s - loss: 0.2763 - accuracy: 0.91 - ETA: 4s - loss: 0.2760 - accuracy: 0.91 - ETA: 3s - loss: 0.2754 - accuracy: 0.91 - ETA: 3s - loss: 0.2758 - accuracy: 0.91 - ETA: 3s - loss: 0.2762 - accuracy: 0.91 - ETA: 3s - loss: 0.2762 - accuracy: 0.91 - ETA: 3s - loss: 0.2771 - accuracy: 0.91 - ETA: 3s - loss: 0.2775 - accuracy: 0.91 - ETA: 3s - loss: 0.2780 - accuracy: 0.91 - ETA: 2s - loss: 0.2776 - accuracy: 0.91 - ETA: 2s - loss: 0.2779 - accuracy: 0.91 - ETA: 2s - loss: 0.2773 - accuracy: 0.91 - ETA: 2s - loss: 0.2775 - accuracy: 0.91 - ETA: 2s - loss: 0.2774 - accuracy: 0.91 - ETA: 2s - loss: 0.2782 - accuracy: 0.91 - ETA: 2s - loss: 0.2793 - accuracy: 0.91 - ETA: 1s - loss: 0.2791 - accuracy: 0.91 - ETA: 1s - loss: 0.2790 - accuracy: 0.91 - ETA: 1s - loss: 0.2793 - accuracy: 0.91 - ETA: 1s - loss: 0.2799 - accuracy: 0.91 - ETA: 1s - loss: 0.2802 - accuracy: 0.91 - ETA: 1s - loss: 0.2802 - accuracy: 0.91 - ETA: 0s - loss: 0.2798 - accuracy: 0.91 - ETA: 0s - loss: 0.2804 - accuracy: 0.91 - ETA: 0s - loss: 0.2807 - accuracy: 0.91 - ETA: 0s - loss: 0.2813 - accuracy: 0.91 - ETA: 0s - loss: 0.2820 - accuracy: 0.91 - ETA: 0s - loss: 0.2812 - accuracy: 0.91 - ETA: 0s - loss: 0.2807 - accuracy: 0.9108\n",
      "Epoch 00038: val_accuracy did not improve from 0.91462\n",
      "84800/84800 [==============================] - 13s 154us/sample - loss: 0.2805 - accuracy: 0.9108 - val_loss: 0.3191 - val_accuracy: 0.9110\n",
      "Epoch 39/100\n",
      "84000/84800 [============================>.] - ETA: 11s - loss: 0.2510 - accuracy: 0.915 - ETA: 11s - loss: 0.2824 - accuracy: 0.910 - ETA: 11s - loss: 0.2874 - accuracy: 0.907 - ETA: 11s - loss: 0.2828 - accuracy: 0.907 - ETA: 11s - loss: 0.2700 - accuracy: 0.912 - ETA: 11s - loss: 0.2658 - accuracy: 0.915 - ETA: 10s - loss: 0.2637 - accuracy: 0.915 - ETA: 10s - loss: 0.2692 - accuracy: 0.914 - ETA: 10s - loss: 0.2704 - accuracy: 0.913 - ETA: 10s - loss: 0.2643 - accuracy: 0.915 - ETA: 10s - loss: 0.2631 - accuracy: 0.915 - ETA: 10s - loss: 0.2624 - accuracy: 0.916 - ETA: 10s - loss: 0.2649 - accuracy: 0.915 - ETA: 9s - loss: 0.2637 - accuracy: 0.916 - ETA: 9s - loss: 0.2627 - accuracy: 0.91 - ETA: 9s - loss: 0.2657 - accuracy: 0.91 - ETA: 9s - loss: 0.2654 - accuracy: 0.91 - ETA: 9s - loss: 0.2645 - accuracy: 0.91 - ETA: 9s - loss: 0.2652 - accuracy: 0.91 - ETA: 9s - loss: 0.2653 - accuracy: 0.91 - ETA: 8s - loss: 0.2636 - accuracy: 0.91 - ETA: 8s - loss: 0.2636 - accuracy: 0.91 - ETA: 8s - loss: 0.2625 - accuracy: 0.91 - ETA: 8s - loss: 0.2624 - accuracy: 0.91 - ETA: 8s - loss: 0.2641 - accuracy: 0.91 - ETA: 8s - loss: 0.2639 - accuracy: 0.91 - ETA: 8s - loss: 0.2655 - accuracy: 0.91 - ETA: 7s - loss: 0.2649 - accuracy: 0.91 - ETA: 7s - loss: 0.2643 - accuracy: 0.91 - ETA: 7s - loss: 0.2647 - accuracy: 0.91 - ETA: 7s - loss: 0.2636 - accuracy: 0.91 - ETA: 7s - loss: 0.2644 - accuracy: 0.91 - ETA: 7s - loss: 0.2644 - accuracy: 0.91 - ETA: 7s - loss: 0.2646 - accuracy: 0.91 - ETA: 7s - loss: 0.2650 - accuracy: 0.91 - ETA: 6s - loss: 0.2652 - accuracy: 0.91 - ETA: 6s - loss: 0.2670 - accuracy: 0.91 - ETA: 6s - loss: 0.2681 - accuracy: 0.91 - ETA: 6s - loss: 0.2680 - accuracy: 0.91 - ETA: 6s - loss: 0.2673 - accuracy: 0.91 - ETA: 6s - loss: 0.2665 - accuracy: 0.91 - ETA: 6s - loss: 0.2664 - accuracy: 0.91 - ETA: 5s - loss: 0.2656 - accuracy: 0.91 - ETA: 5s - loss: 0.2644 - accuracy: 0.91 - ETA: 5s - loss: 0.2648 - accuracy: 0.91 - ETA: 5s - loss: 0.2651 - accuracy: 0.91 - ETA: 5s - loss: 0.2658 - accuracy: 0.91 - ETA: 5s - loss: 0.2662 - accuracy: 0.91 - ETA: 5s - loss: 0.2659 - accuracy: 0.91 - ETA: 4s - loss: 0.2663 - accuracy: 0.91 - ETA: 4s - loss: 0.2666 - accuracy: 0.91 - ETA: 4s - loss: 0.2678 - accuracy: 0.91 - ETA: 4s - loss: 0.2685 - accuracy: 0.91 - ETA: 4s - loss: 0.2692 - accuracy: 0.91 - ETA: 4s - loss: 0.2699 - accuracy: 0.91 - ETA: 4s - loss: 0.2702 - accuracy: 0.91 - ETA: 3s - loss: 0.2698 - accuracy: 0.91 - ETA: 3s - loss: 0.2694 - accuracy: 0.91 - ETA: 3s - loss: 0.2683 - accuracy: 0.91 - ETA: 3s - loss: 0.2682 - accuracy: 0.91 - ETA: 3s - loss: 0.2678 - accuracy: 0.91 - ETA: 3s - loss: 0.2676 - accuracy: 0.91 - ETA: 3s - loss: 0.2675 - accuracy: 0.91 - ETA: 2s - loss: 0.2676 - accuracy: 0.91 - ETA: 2s - loss: 0.2673 - accuracy: 0.91 - ETA: 2s - loss: 0.2671 - accuracy: 0.91 - ETA: 2s - loss: 0.2678 - accuracy: 0.91 - ETA: 2s - loss: 0.2687 - accuracy: 0.91 - ETA: 2s - loss: 0.2692 - accuracy: 0.91 - ETA: 2s - loss: 0.2695 - accuracy: 0.91 - ETA: 1s - loss: 0.2697 - accuracy: 0.91 - ETA: 1s - loss: 0.2698 - accuracy: 0.91 - ETA: 1s - loss: 0.2697 - accuracy: 0.91 - ETA: 1s - loss: 0.2698 - accuracy: 0.91 - ETA: 1s - loss: 0.2698 - accuracy: 0.91 - ETA: 1s - loss: 0.2700 - accuracy: 0.91 - ETA: 1s - loss: 0.2700 - accuracy: 0.91 - ETA: 0s - loss: 0.2696 - accuracy: 0.91 - ETA: 0s - loss: 0.2694 - accuracy: 0.91 - ETA: 0s - loss: 0.2695 - accuracy: 0.91 - ETA: 0s - loss: 0.2701 - accuracy: 0.91 - ETA: 0s - loss: 0.2699 - accuracy: 0.91 - ETA: 0s - loss: 0.2701 - accuracy: 0.91 - ETA: 0s - loss: 0.2701 - accuracy: 0.9135\n",
      "Epoch 00039: val_accuracy did not improve from 0.91462\n",
      "84800/84800 [==============================] - 13s 150us/sample - loss: 0.2700 - accuracy: 0.9135 - val_loss: 0.3159 - val_accuracy: 0.9118\n",
      "Epoch 00039: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11005/1 - 1s - loss: 0.4498 - accuracy: 0.9042\n",
      "4890/1 - 1s - loss: 56.7301 - accuracy: 0.8061\n",
      " ~~~ simulation session ended ~~~\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【yes】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【no】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【zero】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【one】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【two】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【three】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【four】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【five】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【six】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【seven】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【eight】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【nine】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【up】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【down】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【left】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【right】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【forward】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【backward】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【yes】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【no】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【on】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【off】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【no】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【stop】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【bed】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【three】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【house】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【happy】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【dog】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【cat】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【marvin】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【sheila】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【follow】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【learn】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【tree】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【seven】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【six】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【seven】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【stop】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "\n",
      "y= 【six】\n",
      "press \"q\" to quit\n",
      "or another key to record 1 sec speech...\n",
      "q\n",
      "~~~the end~~~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【yes】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【no】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【zero】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【one】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【two】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【three】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【four】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【five】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【six】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【seven】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【eight】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【nine】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【up】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【down】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【left】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【right】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【forward】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【backward】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【yes】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【no】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【on】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【off】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【no】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【stop】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【bed】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【three】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【house】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【happy】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【dog】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【cat】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【marvin】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【sheila】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【follow】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【learn】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【tree】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【seven】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【six】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【seven】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【stop】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= 【six】\n",
      "... ry: Good Luck ...\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Dec 21 23:44:17 2019\n",
    "@author: renyu\n",
    "\n",
    "functionalKeras005_spchCmdNmelSpec.py\n",
    "ryLab001.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# In[]\n",
    "\n",
    "# In[]\n",
    "\n",
    "import compress_pickle as cpk\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "'''        \n",
    "def get_data_v2(fn= 'spCmdV002_train.gz'):\n",
    "    \n",
    "    print('get {} .... '.format(fn))\n",
    "    \n",
    "    aL= cpk.load(fn)\n",
    "    \n",
    "    xL= []\n",
    "    yL= []\n",
    "    for a in aL:\n",
    "        x, y= a\n",
    "        xL += [x]\n",
    "        yL += [y]\n",
    "    xL= np.concatenate(xL)\n",
    "    yL= np.concatenate(yL)\n",
    "    \n",
    "    cL=  np.array(sorted(list(set(list(yL)))))\n",
    "    yDist= [list(yL.flatten()).count(c) for c in cL]\n",
    "    \n",
    "    \n",
    "    print('xL.shape= {}\\nyL.shape= {}\\ncL= {}\\nyDist={}'.format(\n",
    "            xL.shape, yL.shape, cL, yDist))\n",
    "    \n",
    "    return xL, yL, cL\n",
    "\n",
    "\n",
    "data_path= 'spCmdV002_train.gz'\n",
    "x_train, y_train, c_train= get_data_v2(fn= data_path)\n",
    "\n",
    "data_path= 'spCmdV002_val.gz'\n",
    "x_val, y_val, c_val=       get_data_v2(fn= data_path)\n",
    "\n",
    "data_path= 'spCmdV002_test.gz'\n",
    "x_test, y_test, c_test=       get_data_v2(fn= data_path)\n",
    "\n",
    "data_path= 'spCmdV002_testREAL.gz'\n",
    "x_testREAL, y_testREAL, c_testREAL=       get_data_v2(fn= data_path)\n",
    "'''\n",
    "\n",
    "# In[]\n",
    "\n",
    "# In[]\n",
    "\n",
    "from ryLab000_PrepareDataset import load_data\n",
    "\n",
    "fn= 'google_spcmd_test.gz'\n",
    "[(x_test, y_test, c_test),    (x_testREAL, y_testREAL, c_testREAL)]= load_data(fn)\n",
    "\n",
    "fn= 'google_spcmd_train.gz'\n",
    "[(x_train, y_train, c_train), (x_val, y_val, c_val)]= load_data(fn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[]\n",
    "import tensorflow as tf\n",
    "\n",
    "def ryFeature(x, \n",
    "           sample_rate= 16000, \n",
    "           \n",
    "           frame_length= 1024,\n",
    "           frame_step=    128,  # frame_length//2\n",
    "           \n",
    "           num_mel_bins=     128,\n",
    "           lower_edge_hertz= 20,     # 0\n",
    "           upper_edge_hertz= 16000/2, # sample_rate/2   \n",
    "           \n",
    "           mfcc_dim= 13\n",
    "           ):\n",
    "    \n",
    "    stfts= tf.signal.stft(x, \n",
    "                          frame_length, #=  256, #1024, \n",
    "                          frame_step, #=    128,\n",
    "                          #fft_length= 1024\n",
    "                          pad_end=True\n",
    "                          )\n",
    "    \n",
    "    spectrograms=     tf.abs(stfts)\n",
    "    log_spectrograms= tf.math.log(spectrograms + 1e-10)\n",
    "    \n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins= stfts.shape[-1]  #.value\n",
    "    \n",
    "    linear_to_mel_weight_matrix= tf.signal.linear_to_mel_weight_matrix(\n",
    "          num_mel_bins, \n",
    "          num_spectrogram_bins, \n",
    "          sample_rate, \n",
    "          lower_edge_hertz,\n",
    "          upper_edge_hertz)\n",
    "    \n",
    "    mel_spectrograms= tf.tensordot(\n",
    "          spectrograms, \n",
    "          linear_to_mel_weight_matrix, 1)\n",
    "    \n",
    "    mel_spectrograms.set_shape(\n",
    "          spectrograms.shape[:-1].concatenate(\n",
    "              linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    \n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms= tf.math.log(mel_spectrograms + 1e-10)\n",
    "    \n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    mfccs= tf.signal.mfccs_from_log_mel_spectrograms(\n",
    "          log_mel_spectrograms)[..., :mfcc_dim]\n",
    "    \n",
    "    feature= {'mfcc':               mfccs, \n",
    "              'log_mel_spectrogram':log_mel_spectrograms, \n",
    "              'log_spectrogram':    log_spectrograms, \n",
    "              'spectrogram':        spectrograms}\n",
    "    \n",
    "    return  feature\n",
    "\n",
    "\n",
    "batch_size= 1000  # 預防 gpu memory 不夠， 分批作業 \n",
    "x= x_train[0:batch_size].astype(np.float32)\n",
    "X= ryFeature(x)['log_mel_spectrogram']\n",
    "X= X.numpy()\n",
    "\n",
    "zzz= '''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# In[]\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_all_fearure(all_x, batch_size= 1000):\n",
    "    t0= time.time()\n",
    "    \n",
    "    x= all_x.astype(np.float32)\n",
    "    \n",
    "    #batch_size= 1000  # 預防 gpu memory 不夠， 分批作業 \n",
    "    \n",
    "    i=0\n",
    "    XL=[]\n",
    "    while i < x.shape[0]:\n",
    "        \n",
    "        if i+batch_size<=x.shape[0]:\n",
    "            xx= x[i:i+batch_size]\n",
    "        else:\n",
    "            xx= x[i:]\n",
    "        \n",
    "        XX= ryFeature(xx)\n",
    "        X= XX['log_mel_spectrogram'] \n",
    "        #'log_spectrogram'] #'mfcc'] #'log_mel_spectrogram']\n",
    "        \n",
    "        X= X.numpy().astype(np.float32)\n",
    "        \n",
    "        i  += batch_size\n",
    "        XL += [X]\n",
    "    \n",
    "    XL= np.concatenate(XL)\n",
    "    print('XL.shape={}'.format(XL.shape))\n",
    "    \n",
    "    dt= time.time()-t0\n",
    "    print('tf.signal.stft, 執行時間 dt= {}'.format(dt))\n",
    "    \n",
    "    '''\n",
    "    XL.shape=(64721, 125, 129) # nTime= 16000/128, nFreq=256/2+1\n",
    "    tf.signal.stft, dt= 8.066392660140991\n",
    "    '''\n",
    "    return XL\n",
    "\n",
    "X_testREAL= get_all_fearure(x_testREAL)\n",
    "X_test=     get_all_fearure(x_test)\n",
    "X_val=      get_all_fearure(x_val)\n",
    "X_train=    get_all_fearure(x_train)\n",
    "\n",
    "\n",
    "# In[]\n",
    "\n",
    "nTime, nFreq= X_train[0].shape\n",
    "\n",
    "zzz='''\n",
    "nTime, nFreq= (125, 128)\n",
    "'''\n",
    "\n",
    "# In[]\n",
    "def normalize(x):   \n",
    "    x= (x-x.mean())/x.std()\n",
    "    return x\n",
    "\n",
    "\n",
    "X_train= X_train.reshape(-1, nTime, nFreq, 1).astype('float32') \n",
    "X_val=   X_val.reshape(-1, nTime, nFreq, 1).astype('float32') \n",
    "X_test=  X_test.reshape( -1, nTime, nFreq, 1).astype('float32') \n",
    "X_testREAL=  X_testREAL.reshape( -1, nTime, nFreq, 1).astype('float32') \n",
    "\n",
    "X_train=     normalize(X_train)\n",
    "X_val=       normalize(X_val)\n",
    "X_test=      normalize(X_test)\n",
    "X_testREAL=  normalize(X_testREAL)\n",
    "\n",
    "\n",
    "# In[]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  \n",
    "# For easy reset of notebook state.\n",
    "\n",
    "from tensorflow              import keras\n",
    "from tensorflow.keras        import layers, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# In[]\n",
    "\n",
    "nCategs= c_train.size #36\n",
    "\n",
    "\n",
    "x= Input(shape= (nTime, nFreq, 1))\n",
    "\n",
    "h= x\n",
    "\n",
    "\n",
    "#'''\n",
    "h= Conv2D(8,   (16,16), activation='relu', padding='same')(h)\n",
    "h= MaxPooling2D((4,4), padding='same')(h)\n",
    "h= Dropout(0.2)(h)\n",
    "\n",
    "h= Conv2D(16,   (8,8), activation='relu', padding='same')(h)\n",
    "h= MaxPooling2D((4,4), padding='same')(h)\n",
    "h= Dropout(0.2)(h)\n",
    "\n",
    "h= Flatten()(h)\n",
    "\n",
    "h= Dense(256,  activation='relu')(h)\n",
    "h= Dropout(0.2)(h)\n",
    "\n",
    "\n",
    "h= Dense(nCategs,  activation='softmax')(h)\n",
    "\n",
    "y= h\n",
    "\n",
    "m= Model(inputs=  x, \n",
    "         outputs= y)\n",
    "\n",
    "m.summary()\n",
    "\n",
    "\n",
    "\n",
    "# In[]\n",
    "#keras.utils.plot_model(m, 'm.png', show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "# In[]\n",
    "m.compile(  \n",
    "        loss=    'sparse_categorical_crossentropy',\n",
    "        metrics= ['accuracy'])\n",
    "\n",
    "\n",
    "es= EarlyStopping(\n",
    "        monitor=   'val_loss', \n",
    "        min_delta= 1e-10,\n",
    "        patience=  10, \n",
    "        mode=      'min', \n",
    "        verbose=   1) \n",
    "\n",
    "mc= ModelCheckpoint('ry_best_model.hdf5', \n",
    "        monitor=    'val_accuracy', \n",
    "        verbose=    1, \n",
    "        save_best_only= True, \n",
    "        mode=      'max')\n",
    "\n",
    "h= m.fit(X_train, y_train,\n",
    "         \n",
    "        batch_size=1000,\n",
    "        epochs=    100,\n",
    "        \n",
    "        callbacks=[es, mc],\n",
    "        \n",
    "        #validation_split= 0.1\n",
    "        validation_data= (X_val, y_val)\n",
    "        )\n",
    "\n",
    "# In[]\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pl\n",
    "v0= h.history['accuracy']\n",
    "v1= h.history['val_accuracy']\n",
    "pl.plot(v0, label='accuracy')\n",
    "pl.plot(v1, label='val_accuracy')\n",
    "pl.legend()\n",
    "pl.grid('on')\n",
    "pl.show()\n",
    "#keras.utils.plot_model(m, 'm.png', show_shapes=True)\n",
    "\n",
    "# In[]\n",
    "\n",
    "m.evaluate(X_test, y_test, verbose=2)\n",
    "m.evaluate(X_testREAL, y_testREAL, verbose=2)\n",
    "\n",
    "zzz='''\n",
    "Epoch 49/100\n",
    "83968/84736 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8813\n",
    "Epoch 00049: val_accuracy did not improve from 【0.90919】\n",
    "84736/84736 [==============================] - 8s 93us/sample - loss: 0.3739 - accuracy: 0.8812 - val_loss: 0.3101 - val_accuracy: 0.9068\n",
    "Epoch 00049: early stopping\n",
    "\n",
    "11005/1 - 1s - loss: 0.4222 - accuracy: 【0.8964】\n",
    "4890/1 - 1s - loss: 46.2492 - accuracy: 【0.7348】 !!!!\n",
    " ~~~ simulation session ended ~~~\n",
    "'''\n",
    "\n",
    "zzz='''\n",
    "Epoch 46/100\n",
    "84000/84736 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.9196 \n",
    "Epoch 00046: val_accuracy improved from 0.91508 to 0.91589, saving model to 【ry_best_model.hdf5】\n",
    "84736/84736 [==============================] - 11s 133us/sample - loss: 0.2507 - accuracy: 0.9196 - val_loss: 0.3050 - val_accuracy: 0.9159\n",
    "Epoch 47/100\n",
    "84000/84736 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9187 \n",
    "Epoch 00047: val_accuracy did not improve from 【0.91589】\n",
    "84736/84736 [==============================] - 11s 133us/sample - loss: 0.2528 - accuracy: 0.9185 - val_loss: 0.3070 - val_accuracy: 0.9128\n",
    "Epoch 00047: early stopping\n",
    "\n",
    "11005/1 - 2s - loss: 0.4620 - accuracy: 【0.9070】\n",
    "4890/1 - 1s - loss: 53.1950 - accuracy: 【0.8084】\n",
    "'''\n",
    "\n",
    "\n",
    "# In[]\n",
    "\n",
    "## for version.002\n",
    "'''\n",
    "labels= np.array([\n",
    "        '_silence_', \n",
    "        'nine', \n",
    "        'yes', \n",
    "        'no', \n",
    "        'up', \n",
    "        'down', \n",
    "        'left', \n",
    "        'right',\n",
    "        'on', \n",
    "        'off', \n",
    "        'stop', \n",
    "        'go', \n",
    "        'zero', \n",
    "        'one', \n",
    "        'two', \n",
    "        'three', \n",
    "        'four',\n",
    "        'five', \n",
    "        'six', \n",
    "        'seven', \n",
    "        'eight', \n",
    "        'backward', \n",
    "        'bed', \n",
    "        'bird', \n",
    "        'cat',\n",
    "        'dog', \n",
    "        'follow', \n",
    "        'forward', \n",
    "        'happy', \n",
    "        'house', \n",
    "        'learn', \n",
    "        'marvin',\n",
    "        'sheila', \n",
    "        'tree', \n",
    "        'visual', \n",
    "        'wow'], \n",
    "        dtype='<U11')\n",
    "\n",
    "'''\n",
    "\n",
    "print(' ~~~ simulation session ended ~~~')\n",
    "\n",
    "# In[]\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import sounddevice as sd\n",
    "\n",
    "labels= np.array([\n",
    "        '_silence_', \n",
    "        'nine', \n",
    "        'yes', \n",
    "        'no', \n",
    "        'up', \n",
    "        'down', \n",
    "        'left', \n",
    "        'right',\n",
    "        'on', \n",
    "        'off', \n",
    "        'stop', \n",
    "        'go', \n",
    "        'zero', \n",
    "        'one', \n",
    "        'two', \n",
    "        'three', \n",
    "        'four',\n",
    "        'five', \n",
    "        'six', \n",
    "        'seven', \n",
    "        'eight', \n",
    "        'backward', \n",
    "        'bed', \n",
    "        'bird', \n",
    "        'cat',\n",
    "        'dog', \n",
    "        'follow', \n",
    "        'forward', \n",
    "        'happy', \n",
    "        'house', \n",
    "        'learn', \n",
    "        'marvin',\n",
    "        'sheila', \n",
    "        'tree', \n",
    "        'visual', \n",
    "        'wow'], \n",
    "        dtype='<U11')\n",
    "\n",
    "\n",
    "model= load_model('ry_best_model.hdf5')\n",
    "\n",
    "\n",
    "def predict(audio, fs=16000):\n",
    "    prob= model.predict(audio)#.reshape(1,fs,1))\n",
    "    index= np.argmax(prob[0])\n",
    "    return labels[index]\n",
    "    \n",
    "T=  1     # Duration of recording\n",
    "fs= 16000  # Sample rate\n",
    "\n",
    "xL= []\n",
    "for i in range(100):\n",
    "    \n",
    "    aKey= input('{}\\n{}\\n'.format(\n",
    "                'press \"q\" to quit', \n",
    "                'or another key to record 1 sec speech...'))\n",
    "    if aKey=='q':\n",
    "        print('~~~the end~~~')\n",
    "        break\n",
    "    \n",
    "    x= sd.rec(int(T*fs), \n",
    "            samplerate= fs, \n",
    "            channels= 1, \n",
    "            dtype='float32')\n",
    "        \n",
    "    sd.wait()  # Wait until recording is finished\n",
    "\n",
    "    x= x.flatten()\n",
    "    \n",
    "    X= ryFeature(x)['log_mel_spectrogram']\n",
    "    \n",
    "    X= X.numpy().astype(np.float32)\n",
    "    \n",
    "    X= normalize(X)\n",
    "\n",
    "    X= X.reshape(1,X.shape[0],X.shape[1], 1)\n",
    "    y= predict(X)\n",
    "    \n",
    "    print('y= 【{}】'.format(y))\n",
    "    xL += [x]\n",
    "# In[]\n",
    "import pickle\n",
    "\n",
    "fn='rySp_v2.gz'\n",
    "cpk.dump(xL, fn)\n",
    "xL= cpk.load(fn)\n",
    "\n",
    "\n",
    "# In[]\n",
    "    \n",
    "#import numpy as np\n",
    "#from tensorflow.keras.models import load_model\n",
    "import sounddevice as sd\n",
    "\n",
    "import pylab as pl    \n",
    "for x in xL:\n",
    "        \n",
    "    sd.play(x, samplerate= 16000)\n",
    "    pl.plot(x)\n",
    "    pl.show()\n",
    "    \n",
    "    X= ryFeature(x)['log_mel_spectrogram']\n",
    "    \n",
    "    X= X.numpy().astype(np.float32)\n",
    "    \n",
    "    \n",
    "    X= normalize(X)\n",
    "\n",
    "    Xspec= X.reshape(X.shape[0],X.shape[1])\n",
    "    pl.imshow(Xspec.transpose(), origin='low')\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "    Xin= X.reshape(1,X.shape[0],X.shape[1], 1)\n",
    "    y= predict(Xin)\n",
    "    print('y= 【{}】'.format(y))\n",
    "        \n",
    "    sd.wait()\n",
    "    \n",
    "# In[]\n",
    "print('... ry: Good Luck ...')\n",
    "\n",
    "_='''\n",
    "\n",
    "Done preparing Google Speech commands dataset version 2\n",
    "SpeechDownloader.PrepareGoogleSpeechCmd(), 【dt= 1696.8315176963806】\n",
    "\n",
    "gscInfo.keys()= dict_keys(['train', 'test', 'val', 'testREAL']), nCategs= 36\n",
    "2019-12-27 03:36:30.308800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\n",
    "\n",
    "dump google_spcmd_test.gz ....\n",
    "fn= google_spcmd_test.gz, 【dt(sec)= 1649.519】\n",
    "dump google_spcmd_train.gz ....\n",
    "fn= google_spcmd_train.gz, 【dt(sec)= 9807.944】\n",
    "\n",
    "load google_spcmd_test.gz ....\n",
    "xL.shape= (11005, 16000)\n",
    "yL.shape= (11005,)\n",
    "cL= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
    " 25 26 27 28 29 30 31 32 33 34 35]\n",
    "yDist=[408, 419, 405, 425, 406, 412, 396, 396, 402, 411, 402, 418, 399, 424, 405, 400, 445, 394, 406, 408, 165, 207, 185, 194, 220, 172, 155, 203, 191, 161, 195, 212, 193, 165, 206]\n",
    "xL.shape= (4890, 16000)\n",
    "yL.shape= (4890,)\n",
    "cL= [ 0  2  3  4  5  6  7  8  9 10 11]\n",
    "yDist=[816, 419, 405, 425, 406, 412, 396, 396, 402, 411, 402]\n",
    "fn= google_spcmd_test.gz, dt(sec)= 7.771\n",
    "load google_spcmd_train.gz ....\n",
    "xL.shape= (84800, 16000)\n",
    "yL.shape= (84800,)\n",
    "cL= [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
    " 24 25 26 27 28 29 30 31 32 33 34 35]\n",
    "yDist=[6, 3168, 3228, 3129, 2944, 3132, 3036, 3016, 3083, 2969, 3109, 3104, 3248, 3140, 3108, 2964, 2954, 3237, 3088, 3200, 3029, 1346, 1594, 1697, 1657, 1711, 1275, 1254, 1632, 1725, 1286, 1710, 1603, 1407, 1287, 1724]\n",
    "xL.shape= (9920, 16000)\n",
    "yL.shape= (9920,)\n",
    "cL= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
    " 25 26 27 28 29 30 31 32 33 34 35]\n",
    "yDist=[353, 394, 404, 350, 375, 347, 362, 359, 369, 349, 370, 384, 350, 343, 353, 367, 364, 372, 387, 345, 150, 212, 182, 180, 197, 131, 145, 217, 194, 127, 194, 203, 159, 139, 193]\n",
    "fn= google_spcmd_train.gz, dt(sec)= 50.388\n",
    "load google_spcmd_test.gz ....\n",
    "xL.shape= (11005, 16000)\n",
    "yL.shape= (11005,)\n",
    "cL= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
    " 25 26 27 28 29 30 31 32 33 34 35]\n",
    "yDist=[408, 419, 405, 425, 406, 412, 396, 396, 402, 411, 402, 418, 399, 424, 405, 400, 445, 394, 406, 408, 165, 207, 185, 194, 220, 172, 155, 203, 191, 161, 195, 212, 193, 165, 206]\n",
    "xL.shape= (4890, 16000)\n",
    "yL.shape= (4890,)\n",
    "cL= [ 0  2  3  4  5  6  7  8  9 10 11]\n",
    "yDist=[816, 419, 405, 425, 406, 412, 396, 396, 402, 411, 402]\n",
    "fn= google_spcmd_test.gz, 【dt(sec)= 7.698】\n",
    "\n",
    "load google_spcmd_train.gz ....\n",
    "xL.shape= (84800, 16000)\n",
    "yL.shape= (84800,)\n",
    "cL= [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
    " 24 25 26 27 28 29 30 31 32 33 34 35]\n",
    "yDist=[6, 3168, 3228, 3129, 2944, 3132, 3036, 3016, 3083, 2969, 3109, 3104, 3248, 3140, 3108, 2964, 2954, 3237, 3088, 3200, 3029, 1346, 1594, 1697, 1657, 1711, 1275, 1254, 1632, 1725, 1286, 1710, 1603, 1407, 1287, 1724]\n",
    "xL.shape= (9920, 16000)\n",
    "yL.shape= (9920,)\n",
    "cL= [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
    " 25 26 27 28 29 30 31 32 33 34 35]\n",
    "yDist=[353, 394, 404, 350, 375, 347, 362, 359, 369, 349, 370, 384, 350, 343, 353, 367, 364, 372, 387, 345, 150, 212, 182, 180, 197, 131, 145, 217, 194, 127, 194, 203, 159, 139, 193]\n",
    "fn= google_spcmd_train.gz, dt(sec)= 75.980\n",
    "2019-12-27 06:49:53.806890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n",
    "2019-12-27 06:49:53.877804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\n",
    "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\n",
    "pciBusID: 0000:01:00.0\n",
    "2019-12-27 06:49:53.883085: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\n",
    "2019-12-27 06:49:53.887344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
    "2019-12-27 06:49:53.909947: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
    "2019-12-27 06:49:53.947343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\n",
    "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\n",
    "pciBusID: 0000:01:00.0\n",
    "2019-12-27 06:49:53.952293: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\n",
    "2019-12-27 06:49:53.955934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
    "2019-12-27 06:49:55.494812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
    "2019-12-27 06:49:55.498653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\n",
    "2019-12-27 06:49:55.500325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\n",
    "2019-12-27 06:49:55.505390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8784 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
    "2019-12-27 06:49:56.086975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll\n",
    "2019-12-27 06:49:56.414985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\n",
    "XL.shape=(4890, 125, 128)\n",
    "tf.signal.stft, 執行時間 dt= 1.3792753219604492\n",
    "XL.shape=(11005, 125, 128)\n",
    "tf.signal.stft, 執行時間 dt= 3.0988919734954834\n",
    "XL.shape=(9920, 125, 128)\n",
    "tf.signal.stft, 執行時間 dt= 1.4850282669067383\n",
    "XL.shape=(84800, 125, 128)\n",
    "tf.signal.stft, 執行時間 dt= 21.542152643203735\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, 125, 128, 1)]     0\n",
    "_________________________________________________________________\n",
    "conv2d (Conv2D)              (None, 125, 128, 8)       2056\n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (None, 32, 32, 8)         0\n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 32, 32, 8)         0\n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 32, 32, 16)        8208\n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 16)          0\n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 8, 8, 16)          0\n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 1024)              0\n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 256)               262400\n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 256)               0\n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 36)                9252\n",
    "=================================================================\n",
    "Total params: 281,916\n",
    "Trainable params: 281,916\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Train on 84800 samples, validate on 9920 samples\n",
    "Epoch 1/100\n",
    "2019-12-27 06:50:37.617241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\n",
    "2019-12-27 06:50:39.046347: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows\n",
    "Relying on driver to perform ptx compilation. This message will be only logged once.\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 2.6291 - accuracy: 0.2705\n",
    "Epoch 00001: val_accuracy improved from -inf to 0.53921, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 16s 193us/sample - loss: 2.6224 - accuracy: 0.2722 - val_loss: 1.6802 - val_accuracy: 0.5392\n",
    "Epoch 2/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 1.6271 - accuracy: 0.5300\n",
    "Epoch 00002: val_accuracy improved from 0.53921 to 0.63790, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 13s 148us/sample - loss: 1.6259 - accuracy: 0.5304 - val_loss: 1.2720 - val_accuracy: 0.6379\n",
    "Epoch 3/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 1.2436 - accuracy: 0.6393\n",
    "Epoch 00003: val_accuracy improved from 0.63790 to 0.77984, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 13s 149us/sample - loss: 1.2417 - accuracy: 0.6397 - val_loss: 0.7749 - val_accuracy: 0.7798\n",
    "Epoch 4/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 1.0186 - accuracy: 0.6996\n",
    "Epoch 00004: val_accuracy improved from 0.77984 to 0.80554, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 13s 148us/sample - loss: 1.0186 - accuracy: 0.6997 - val_loss: 0.6859 - val_accuracy: 0.8055\n",
    "Epoch 5/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.8838 - accuracy: 0.7368\n",
    "Epoch 00005: val_accuracy improved from 0.80554 to 0.82853, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 146us/sample - loss: 0.8829 - accuracy: 0.7370 - val_loss: 0.6044 - val_accuracy: 0.8285\n",
    "Epoch 6/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.7787 - accuracy: 0.7674\n",
    "Epoch 00006: val_accuracy improved from 0.82853 to 0.85595, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.7784 - accuracy: 0.7675 - val_loss: 0.4955 - val_accuracy: 0.8559\n",
    "Epoch 7/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.7069 - accuracy: 0.7866\n",
    "Epoch 00007: val_accuracy did not improve from 0.85595\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.7073 - accuracy: 0.7866 - val_loss: 0.4975 - val_accuracy: 0.8541\n",
    "Epoch 8/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.6541 - accuracy: 0.8028\n",
    "Epoch 00008: val_accuracy improved from 0.85595 to 0.86603, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.6533 - accuracy: 0.8032 - val_loss: 0.4467 - val_accuracy: 0.8660\n",
    "Epoch 9/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.8166\n",
    "Epoch 00009: val_accuracy improved from 0.86603 to 0.87450, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.6041 - accuracy: 0.8169 - val_loss: 0.4260 - val_accuracy: 0.8745\n",
    "Epoch 10/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.5666 - accuracy: 0.8262\n",
    "Epoch 00010: val_accuracy improved from 0.87450 to 0.88216, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.5664 - accuracy: 0.8264 - val_loss: 0.4059 - val_accuracy: 0.8822\n",
    "Epoch 11/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.8365\n",
    "Epoch 00011: val_accuracy improved from 0.88216 to 0.88720, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.5352 - accuracy: 0.8366 - val_loss: 0.3782 - val_accuracy: 0.8872\n",
    "Epoch 12/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.5110 - accuracy: 0.8447\n",
    "Epoch 00012: val_accuracy did not improve from 0.88720\n",
    "84800/84800 [==============================] - 12s 146us/sample - loss: 0.5107 - accuracy: 0.8447 - val_loss: 0.3914 - val_accuracy: 0.8833\n",
    "Epoch 13/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.4877 - accuracy: 0.8493\n",
    "Epoch 00013: val_accuracy did not improve from 0.88720\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.4878 - accuracy: 0.8493 - val_loss: 0.3979 - val_accuracy: 0.8828\n",
    "Epoch 14/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.4671 - accuracy: 0.8558\n",
    "Epoch 00014: val_accuracy improved from 0.88720 to 0.89798, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.4675 - accuracy: 0.8557 - val_loss: 0.3516 - val_accuracy: 0.8980\n",
    "Epoch 15/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.4503 - accuracy: 0.8617\n",
    "Epoch 00015: val_accuracy improved from 0.89798 to 0.89829, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.4493 - accuracy: 0.8620 - val_loss: 0.3471 - val_accuracy: 0.8983\n",
    "Epoch 16/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.4358 - accuracy: 0.8644\n",
    "Epoch 00016: val_accuracy did not improve from 0.89829\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.4359 - accuracy: 0.8644 - val_loss: 0.3547 - val_accuracy: 0.8957\n",
    "Epoch 17/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8686\n",
    "Epoch 00017: val_accuracy improved from 0.89829 to 0.89849, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.4201 - accuracy: 0.8687 - val_loss: 0.3418 - val_accuracy: 0.8985\n",
    "Epoch 18/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.8732\n",
    "Epoch 00018: val_accuracy improved from 0.89849 to 0.90081, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.4086 - accuracy: 0.8733 - val_loss: 0.3303 - val_accuracy: 0.9008\n",
    "Epoch 19/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3931 - accuracy: 0.8768\n",
    "Epoch 00019: val_accuracy improved from 0.90081 to 0.90383, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 13s 147us/sample - loss: 0.3929 - accuracy: 0.8769 - val_loss: 0.3280 - val_accuracy: 0.9038\n",
    "Epoch 20/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3844 - accuracy: 0.8792\n",
    "Epoch 00020: val_accuracy did not improve from 0.90383\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3844 - accuracy: 0.8793 - val_loss: 0.3302 - val_accuracy: 0.9001\n",
    "Epoch 21/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8829\n",
    "Epoch 00021: val_accuracy improved from 0.90383 to 0.90746, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3745 - accuracy: 0.8827 - val_loss: 0.3187 - val_accuracy: 0.9075\n",
    "Epoch 22/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3645 - accuracy: 0.8849\n",
    "Epoch 00022: val_accuracy did not improve from 0.90746\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3645 - accuracy: 0.8849 - val_loss: 0.3273 - val_accuracy: 0.9056\n",
    "Epoch 23/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3583 - accuracy: 0.8865\n",
    "Epoch 00023: val_accuracy did not improve from 0.90746\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3581 - accuracy: 0.8866 - val_loss: 0.3319 - val_accuracy: 0.9017\n",
    "Epoch 24/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8904\n",
    "Epoch 00024: val_accuracy did not improve from 0.90746\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3515 - accuracy: 0.8905 - val_loss: 0.3257 - val_accuracy: 0.9048\n",
    "Epoch 25/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8923\n",
    "Epoch 00025: val_accuracy did not improve from 0.90746\n",
    "84800/84800 [==============================] - 12s 146us/sample - loss: 0.3457 - accuracy: 0.8923 - val_loss: 0.3275 - val_accuracy: 0.9020\n",
    "Epoch 26/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3415 - accuracy: 0.8922\n",
    "Epoch 00026: val_accuracy improved from 0.90746 to 0.90766, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3422 - accuracy: 0.8919 - val_loss: 0.3205 - val_accuracy: 0.9077\n",
    "Epoch 27/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8959\n",
    "Epoch 00027: val_accuracy did not improve from 0.90766\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3283 - accuracy: 0.8959 - val_loss: 0.3313 - val_accuracy: 0.9059\n",
    "Epoch 28/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3301 - accuracy: 0.8956\n",
    "Epoch 00028: val_accuracy improved from 0.90766 to 0.91210, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3294 - accuracy: 0.8958 - val_loss: 0.3097 - val_accuracy: 0.9121\n",
    "Epoch 29/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8980\n",
    "Epoch 00029: val_accuracy did not improve from 0.91210\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3219 - accuracy: 0.8980 - val_loss: 0.3244 - val_accuracy: 0.9072\n",
    "Epoch 30/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3162 - accuracy: 0.8996\n",
    "Epoch 00030: val_accuracy did not improve from 0.91210\n",
    "84800/84800 [==============================] - 12s 146us/sample - loss: 0.3162 - accuracy: 0.8996 - val_loss: 0.3294 - val_accuracy: 0.9077\n",
    "Epoch 31/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.9014\n",
    "Epoch 00031: val_accuracy did not improve from 0.91210\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3111 - accuracy: 0.9013 - val_loss: 0.3159 - val_accuracy: 0.9108\n",
    "Epoch 32/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3066 - accuracy: 0.9026\n",
    "Epoch 00032: val_accuracy did not improve from 0.91210\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3069 - accuracy: 0.9024 - val_loss: 0.3268 - val_accuracy: 0.9053\n",
    "Epoch 33/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.9044\n",
    "Epoch 00033: val_accuracy improved from 0.91210 to 0.91391, saving model to ry_best_model.hdf5\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.3032 - accuracy: 0.9044 - val_loss: 0.3079 - val_accuracy: 0.9139\n",
    "Epoch 34/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.9055\n",
    "Epoch 00034: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2992 - accuracy: 0.9055 - val_loss: 0.3205 - val_accuracy: 0.9078\n",
    "Epoch 35/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2966 - accuracy: 0.9056\n",
    "Epoch 00035: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2967 - accuracy: 0.9055 - val_loss: 0.3157 - val_accuracy: 0.9102\n",
    "Epoch 36/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9085\n",
    "Epoch 00036: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2881 - accuracy: 0.9084 - val_loss: 0.3087 - val_accuracy: 0.9131\n",
    "Epoch 37/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9068\n",
    "Epoch 00037: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2903 - accuracy: 0.9067 - val_loss: 0.3180 - val_accuracy: 0.9091\n",
    "Epoch 38/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2809 - accuracy: 0.9107\n",
    "Epoch 00038: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2810 - accuracy: 0.9106 - val_loss: 0.3330 - val_accuracy: 0.9060\n",
    "Epoch 39/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.9105\n",
    "Epoch 00039: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2796 - accuracy: 0.9106 - val_loss: 0.3108 - val_accuracy: 0.9110\n",
    "Epoch 40/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.9105\n",
    "Epoch 00040: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2793 - accuracy: 0.9104 - val_loss: 0.3282 - val_accuracy: 0.9072\n",
    "Epoch 41/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2780 - accuracy: 0.9111\n",
    "Epoch 00041: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2779 - accuracy: 0.9111 - val_loss: 0.3211 - val_accuracy: 0.9099\n",
    "Epoch 42/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.9128\n",
    "Epoch 00042: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 146us/sample - loss: 0.2736 - accuracy: 0.9129 - val_loss: 0.3100 - val_accuracy: 0.9126\n",
    "Epoch 43/100\n",
    "84000/84800 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.9135\n",
    "Epoch 00043: val_accuracy did not improve from 0.91391\n",
    "84800/84800 [==============================] - 12s 147us/sample - loss: 0.2670 - accuracy: 0.9137 - val_loss: 0.3279 - val_accuracy: 0.9088\n",
    "Epoch 00043: early stopping\n",
    "\n",
    "11005/1 - 1s - loss: 0.4285 - accuracy: 【【0.9016】】\n",
    "4890/1 - 1s - loss: 59.3410 - accuracy: 【【0.7957】】\n",
    "\n",
    " ~~~ simulation session ended ~~~\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【zero】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【one】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【two】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【three】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【four】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【five】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【six】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【seven】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【eight】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【nine】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【up】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【down】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【left】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【right】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【forward】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【backward】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【yes】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【no】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【stop】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【go】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【on】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【off】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【bird】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【bed】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【house】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【happy】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【dog】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【cat】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【marvin】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "\n",
    "y= 【sheila】\n",
    "press \"q\" to quit\n",
    "or another key to record 1 sec speech...\n",
    "q\n",
    "~~~the end~~~\n",
    "y= 【zero】\n",
    "y= 【one】\n",
    "y= 【two】\n",
    "y= 【three】\n",
    "y= 【four】\n",
    "y= 【five】\n",
    "y= 【six】\n",
    "y= 【seven】\n",
    "y= 【eight】\n",
    "y= 【nine】\n",
    "y= 【up】\n",
    "y= 【down】\n",
    "y= 【left】\n",
    "y= 【right】\n",
    "y= 【forward】\n",
    "y= 【backward】\n",
    "y= 【yes】\n",
    "y= 【no】\n",
    "y= 【stop】\n",
    "y= 【go】\n",
    "y= 【on】\n",
    "y= 【off】\n",
    "y= 【bird】\n",
    "y= 【bed】\n",
    "y= 【house】\n",
    "y= 【happy】\n",
    "y= 【dog】\n",
    "y= 【cat】\n",
    "y= 【marvin】\n",
    "y= 【sheila】\n",
    "... ry: Good Luck ...\n",
    "\n",
    "E:\\OneDrive\\__ryTeach\\_2019\\SpeechRecognition\\__exp1__>\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "_='''\n",
    "11005/1 - 1s - loss: 0.4498 - accuracy: 【【0.9042】】\n",
    "4890/1 - 1s - loss: 56.7301 - accuracy: 【【0.8061】】\n",
    " ~~~ simulation session ended ~~~\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
